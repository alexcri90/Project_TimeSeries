{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Definire i concetti di stazionarietà e integrazione e fornire le condizioni per le quali un processo AR(2) è integrato di ordine 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity and Integration in Time Series\n",
    "\n",
    "## Stationarity\n",
    "\n",
    "A stochastic process $\\{X_t\\}$ is said to be **weakly stationary** (or covariance stationary) if it satisfies the following conditions:\n",
    "\n",
    "1. The expected value is constant and independent of time:\n",
    "   $$E[X_t] = \\mu < \\infty, \\quad \\forall t$$\n",
    "\n",
    "2. The variance is finite and independent of time:\n",
    "   $$Var(X_t) = \\sigma^2 < \\infty, \\quad \\forall t$$\n",
    "\n",
    "3. The autocovariance function depends only on the time lag h and not on time t:\n",
    "   $$Cov(X_t, X_{t+h}) = \\gamma(h), \\quad \\forall t, h$$\n",
    "\n",
    "## Integration\n",
    "\n",
    "A time series is said to be **integrated of order d**, denoted as $I(d)$, if it needs to be differenced d times to become stationary. More formally:\n",
    "\n",
    "- If $Y_t \\sim I(d)$, then $\\Delta^d Y_t$ is stationary\n",
    "- Where $\\Delta$ is the difference operator: $\\Delta Y_t = Y_t - Y_{t-1}$\n",
    "- And $\\Delta^d$ represents applying the difference operator d times\n",
    "\n",
    "## AR(2) Process and Integration\n",
    "\n",
    "An AR(2) process is defined as:\n",
    "\n",
    "$$Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\varepsilon_t$$\n",
    "\n",
    "where $\\varepsilon_t$ is white noise.\n",
    "\n",
    "For an AR(2) process to be integrated of order 1, $I(1)$, it must satisfy two conditions:\n",
    "\n",
    "1. The characteristic equation $1 - \\phi_1z - \\phi_2z^2 = 0$ must have exactly one unit root $(z = 1)$\n",
    "2. The other root must lie outside the unit circle\n",
    "\n",
    "This translates to the following conditions on the parameters:\n",
    "\n",
    "1. $\\phi_1 + \\phi_2 = 1$ (ensures one unit root)\n",
    "2. $|\\phi_2| < 1$ (ensures the other root is outside the unit circle)\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the AR(2) process:\n",
    "$$Y_t = 1.5Y_{t-1} - 0.5Y_{t-2} + \\varepsilon_t$$\n",
    "\n",
    "Here, $\\phi_1 = 1.5$ and $\\phi_2 = -0.5$\n",
    "\n",
    "1. Check if $\\phi_1 + \\phi_2 = 1$:\n",
    "   $1.5 + (-0.5) = 1$ ✓\n",
    "\n",
    "2. Check if $|\\phi_2| < 1$:\n",
    "   $|-0.5| = 0.5 < 1$ ✓\n",
    "\n",
    "Therefore, this AR(2) process is integrated of order 1. This means that while $Y_t$ is non-stationary, its first difference $\\Delta Y_t$ will be stationary.\n",
    "\n",
    "The characteristic equation is:\n",
    "$$1 - 1.5z + 0.5z^2 = 0.5(z - 1)(z - 2) = 0$$\n",
    "\n",
    "As we can see, one root is $z = 1$ (the unit root) and the other is $z = 2$ (outside the unit circle), confirming our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The Unit Root Concept\n",
    "\n",
    "A \"unit root\" is a characteristic of a time series process where a root of the characteristic equation equals 1 (unity). The characteristic equation is obtained by:\n",
    "\n",
    "1. Writing the AR process in lag operator form: $(1 - \\phi_1L - \\phi_2L^2)Y_t = \\varepsilon_t$\n",
    "2. Replacing L with z: $1 - \\phi_1z - \\phi_2z^2 = 0$\n",
    "\n",
    "The \"unit circle\" in the complex plane is the circle with radius 1 centered at the origin. A root lying:\n",
    "- On the unit circle $(|z| = 1)$ → Process is non-stationary\n",
    "- Outside the unit circle $(|z| > 1)$ → Process is stationary\n",
    "- Inside the unit circle $(|z| < 1)$ → Process is explosive\n",
    "\n",
    "## Visual Representation\n",
    "\n",
    "For an AR(2) process, we can visualize the roots in the complex plane:\n",
    "```\n",
    "                    Im\n",
    "                     ↑\n",
    "            Unit Circle → |z| = 1\n",
    "                     |\n",
    "          -1 ←------+-----→ 1   Re\n",
    "                     |\n",
    "                     ↓\n",
    "```\n",
    "\n",
    "## Implications for Different AR Processes\n",
    "\n",
    "### AR(1) Process\n",
    "For an AR(1) process $Y_t = \\phi Y_{t-1} + \\varepsilon_t$:\n",
    "- Characteristic equation: $1 - \\phi z = 0$\n",
    "- Single root: $z = \\frac{1}{\\phi}$\n",
    "- To be I(1): must have exactly $\\phi = 1$\n",
    "\n",
    "### AR(2) Process\n",
    "For an AR(2) process $Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\varepsilon_t$:\n",
    "- Characteristic equation: $1 - \\phi_1z - \\phi_2z^2 = 0$\n",
    "- Two roots: both can be real or complex conjugates\n",
    "- To be I(1): one root must be 1, other outside unit circle\n",
    "\n",
    "### AR(3) Process\n",
    "For an AR(3) process $Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\phi_3Y_{t-3} + \\varepsilon_t$:\n",
    "- Characteristic equation: $1 - \\phi_1z - \\phi_2z^2 - \\phi_3z^3 = 0$\n",
    "- Three roots: can be all real or one real and two complex conjugates\n",
    "- To be I(1): one root must be 1, other two outside unit circle\n",
    "- Parameter conditions: $\\phi_1 + \\phi_2 + \\phi_3 = 1$ and other stability conditions\n",
    "\n",
    "## Behavior Examples\n",
    "\n",
    "1. **All roots outside unit circle** (Stationary):\n",
    "   - Series fluctuates around mean\n",
    "   - Shocks have temporary effects\n",
    "   - Example: AR(1) with $\\phi = 0.5$\n",
    "\n",
    "2. **One unit root** (I(1)):\n",
    "   - Series wanders without fixed mean\n",
    "   - Shocks have permanent effects\n",
    "   - Example: Random Walk $Y_t = Y_{t-1} + \\varepsilon_t$\n",
    "\n",
    "3. **Root inside unit circle** (Explosive):\n",
    "   - Series diverges exponentially\n",
    "   - Shocks have amplifying effects\n",
    "   - Example: AR(1) with $\\phi = 1.2$\n",
    "\n",
    "## Implications for Time Series Analysis\n",
    "\n",
    "1. **Stationarity Testing**:\n",
    "   - Unit root tests (like ADF, KPSS) check for presence of unit roots\n",
    "   - Critical for choosing appropriate modeling strategy\n",
    "\n",
    "2. **Cointegration**:\n",
    "   - When two I(1) series share a common unit root\n",
    "   - Their linear combination might be stationary\n",
    "\n",
    "3. **Forecasting**:\n",
    "   - Unit roots affect forecast uncertainty\n",
    "   - Confidence intervals grow wider for I(1) processes\n",
    "\n",
    "4. **Model Selection**:\n",
    "   - I(1) series need differencing or ARIMA modeling\n",
    "   - Stationary series can use ARMA modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Quale processo della famiglia ARMA ha il seguente correlogramma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying ARMA Processes from Correlograms\n",
    "\n",
    "## Theoretical Patterns in ACF and PACF\n",
    "\n",
    "The identification of ARMA processes relies on the analysis of two key functions:\n",
    "\n",
    "1. **Autocorrelation Function (ACF)** $\\rho(k)$:\n",
    "   $$\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)} = \\frac{Cov(Y_t, Y_{t-k})}{Var(Y_t)}$$\n",
    "\n",
    "2. **Partial Autocorrelation Function (PACF)** $\\alpha(k)$:\n",
    "   Measures correlation between $Y_t$ and $Y_{t-k}$ after removing the linear effects of $Y_{t-1}, ..., Y_{t-k+1}$\n",
    "\n",
    "## Identifying Patterns\n",
    "\n",
    "### 1. AR(p) Processes\n",
    "\n",
    "For an AR(p) process:\n",
    "- ACF: Tails off gradually (exponential decay or damped sinusoidal)\n",
    "- PACF: Cuts off after lag p\n",
    "- Example AR(1): $Y_t = 0.7Y_{t-1} + \\varepsilon_t$\n",
    "  * ACF: $\\rho(k) = 0.7^k$\n",
    "  * PACF: $\\alpha(1) = 0.7$, $\\alpha(k) = 0$ for $k > 1$\n",
    "\n",
    "### 2. MA(q) Processes\n",
    "\n",
    "For an MA(q) process:\n",
    "- ACF: Cuts off after lag q\n",
    "- PACF: Tails off gradually\n",
    "- Example MA(1): $Y_t = \\varepsilon_t + 0.7\\varepsilon_{t-1}$\n",
    "  * ACF: $\\rho(1) = \\frac{0.7}{1+0.7^2}$, $\\rho(k) = 0$ for $k > 1$\n",
    "  * PACF: Decays exponentially\n",
    "\n",
    "### 3. ARMA(p,q) Processes\n",
    "\n",
    "For an ARMA(p,q) process:\n",
    "- ACF: Tails off after lag q\n",
    "- PACF: Tails off after lag p\n",
    "- More complex patterns that combine AR and MA characteristics\n",
    "\n",
    "## Common Correlogram Patterns\n",
    "\n",
    "1. **White Noise**\n",
    "   - ACF: All zero except at lag 0\n",
    "   - PACF: All zero except at lag 0\n",
    "\n",
    "2. **AR(1)**\n",
    "   - ACF: Exponential decay\n",
    "   - PACF: Single spike at lag 1\n",
    "\n",
    "3. **AR(2)**\n",
    "   - ACF: Damped exponential or sinusoidal decay\n",
    "   - PACF: Two spikes, zero afterward\n",
    "\n",
    "4. **MA(1)**\n",
    "   - ACF: Single spike at lag 1\n",
    "   - PACF: Exponential decay\n",
    "\n",
    "5. **MA(2)**\n",
    "   - ACF: Two spikes, zero afterward\n",
    "   - PACF: Damped exponential decay\n",
    "\n",
    "## Identification Steps\n",
    "\n",
    "1. **Examine ACF**:\n",
    "   - If cuts off: Suggests MA component\n",
    "   - If decays: Suggests AR component\n",
    "   - Count significant lags before cutoff\n",
    "\n",
    "2. **Examine PACF**:\n",
    "   - If cuts off: Suggests AR component\n",
    "   - If decays: Suggests MA component\n",
    "   - Count significant lags before cutoff\n",
    "\n",
    "3. **Combine Information**:\n",
    "   - If both tail off: ARMA process\n",
    "   - If one cuts off: Pure AR or MA\n",
    "   - Note the lags where patterns change\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "1. **Sample Size Effects**:\n",
    "   - Larger samples give clearer patterns\n",
    "   - Use confidence bands (typically ±2/√n)\n",
    "\n",
    "2. **Stationarity**:\n",
    "   - Patterns only valid for stationary series\n",
    "   - May need differencing first\n",
    "\n",
    "3. **Seasonality**:\n",
    "   - Look for spikes at seasonal lags\n",
    "   - May need seasonal differencing\n",
    "\n",
    "4. **Model Validation**:\n",
    "   - Check residual correlograms\n",
    "   - Should resemble white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Le matrici T e Q dei due tipi di stagionalità (ogni sette giorni)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T and Q Matrices for Weekly Seasonality (s=7)\n",
    "\n",
    "## 1. Dummy Variables Seasonality\n",
    "\n",
    "For weekly seasonality using dummy variables, we need 6 state variables (the 7th is determined by the constraint that they sum to zero).\n",
    "\n",
    "### Transition Matrix T\n",
    "For s = 7, the transition matrix T is a 6×6 matrix:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix} \n",
    "-1 & -1 & -1 & -1 & -1 & -1 \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Disturbance Variance Matrix Q\n",
    "The Q matrix is a 6×6 matrix with only one non-zero element:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "\\sigma_{\\omega}^2 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## 2. Trigonometric Seasonality\n",
    "\n",
    "For weekly seasonality using trigonometric form, we need 3 harmonics (since ⌊7/2⌋ = 3).\n",
    "\n",
    "### Transition Matrix T\n",
    "For s = 7, the transition matrix T is a 6×6 block diagonal matrix:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\lambda_1) & \\sin(\\lambda_1) \\\\\n",
    "-\\sin(\\lambda_1) & \\cos(\\lambda_1)\n",
    "\\end{bmatrix} & 0 & 0 \\\\\n",
    "0 & \\begin{bmatrix}\n",
    "\\cos(\\lambda_2) & \\sin(\\lambda_2) \\\\\n",
    "-\\sin(\\lambda_2) & \\cos(\\lambda_2)\n",
    "\\end{bmatrix} & 0 \\\\\n",
    "0 & 0 & \\begin{bmatrix}\n",
    "\\cos(\\lambda_3) & \\sin(\\lambda_3) \\\\\n",
    "-\\sin(\\lambda_3) & \\cos(\\lambda_3)\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\lambda_j = \\frac{2\\pi j}{7}$ for j = 1, 2, 3\n",
    "\n",
    "### Disturbance Variance Matrix Q\n",
    "The Q matrix is a 6×6 diagonal matrix:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "\\sigma_{\\omega}^2 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma_{\\omega}^2 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\sigma_{\\omega}^2 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & \\sigma_{\\omega}^2 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\sigma_{\\omega}^2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\sigma_{\\omega}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Properties and Interpretation\n",
    "\n",
    "### Dummy Variables Form:\n",
    "- First row of T matrix ensures sum-to-zero constraint\n",
    "- Subsequent rows shift the seasonal effects\n",
    "- Single variance parameter in Q controls evolution\n",
    "- State vector directly represents seasonal effects\n",
    "\n",
    "### Trigonometric Form:\n",
    "- Block diagonal structure in T represents harmonics\n",
    "- Each 2×2 block is a rotation matrix\n",
    "- Equal variances in Q for all components\n",
    "- State vector represents amplitudes of harmonics\n",
    "\n",
    "## Key Differences:\n",
    "1. **Size**: Both are 6×6 but structured differently\n",
    "2. **Evolution**: \n",
    "   - Dummy: Direct shifts with one shock\n",
    "   - Trigonometric: Smooth rotation with multiple shocks\n",
    "3. **Interpretation**:\n",
    "   - Dummy: Direct seasonal effects\n",
    "   - Trigonometric: Frequency components\n",
    "4. **Smoothness**:\n",
    "   - Dummy: Can have sharp changes\n",
    "   - Trigonometric: Naturally smoother transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:\n",
    "\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "\n",
    "- random walk,\n",
    "- ciclo stocastico,\n",
    "- regressione su xt,\n",
    "- rumore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Matrices for UCM Model\n",
    "\n",
    "## Model Components\n",
    "\n",
    "The model contains:\n",
    "1. Random walk (level component)\n",
    "2. Stochastic cycle\n",
    "3. Regression on $x_t$\n",
    "4. Observation noise\n",
    "\n",
    "The model can be written as:\n",
    "\n",
    "$$y_t = \\mu_t + \\psi_t + \\beta x_t + \\varepsilon_t$$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the random walk\n",
    "- $\\psi_t$ is the stochastic cycle\n",
    "- $\\beta x_t$ is the regression term\n",
    "- $\\varepsilon_t$ is the observation noise\n",
    "\n",
    "## State Space Representation\n",
    "\n",
    "### State Vector\n",
    "The state vector $\\alpha_t$ contains:\n",
    "- Random walk level ($\\mu_t$)\n",
    "- Cycle ($\\psi_t$) and auxiliary cycle component ($\\psi_t^*$)\n",
    "- Regression coefficient ($\\beta_t$)\n",
    "\n",
    "$$\\alpha_t = \\begin{bmatrix} \n",
    "\\mu_t \\\\\n",
    "\\psi_t \\\\\n",
    "\\psi_t^* \\\\\n",
    "\\beta_t\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Transition Matrix T\n",
    "$$T = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & \\rho\\cos(\\lambda) & \\rho\\sin(\\lambda) & 0 \\\\\n",
    "0 & -\\rho\\sin(\\lambda) & \\rho\\cos(\\lambda) & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\rho$ is the damping factor of the cycle $(0 < \\rho < 1)$\n",
    "- $\\lambda$ is the cycle frequency $(0 < \\lambda < \\pi)$\n",
    "\n",
    "### Observation Matrix Z\n",
    "$$Z = \\begin{bmatrix} 1 & 1 & 0 & x_t \\end{bmatrix}$$\n",
    "\n",
    "Note that $x_t$ enters in the observation matrix as it multiplies $\\beta_t$\n",
    "\n",
    "### System Disturbance Matrix R\n",
    "$$R = I_4$$ \n",
    "(4×4 identity matrix)\n",
    "\n",
    "### System Disturbance Covariance Matrix Q\n",
    "$$Q = \\begin{bmatrix}\n",
    "\\sigma_\\eta^2 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma_\\kappa^2 & 0 & 0 \\\\\n",
    "0 & 0 & \\sigma_\\kappa^2 & 0 \\\\\n",
    "0 & 0 & 0 & \\sigma_\\beta^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\sigma_\\eta^2$ is the variance of random walk innovations\n",
    "- $\\sigma_\\kappa^2$ is the variance of cycle disturbances\n",
    "- $\\sigma_\\beta^2$ is the variance of regression coefficient innovations\n",
    "\n",
    "### Observation Disturbance Variance H\n",
    "$$H = \\sigma_\\varepsilon^2$$\n",
    "\n",
    "## Properties and Interpretation\n",
    "\n",
    "1. **Random Walk Component**:\n",
    "   - Single state element ($\\mu_t$)\n",
    "   - Unit coefficient in T matrix\n",
    "   - Innovation variance $\\sigma_\\eta^2$\n",
    "\n",
    "2. **Cycle Component**:\n",
    "   - Two state elements ($\\psi_t, \\psi_t^*$)\n",
    "   - 2×2 rotation matrix in T\n",
    "   - Equal variances $\\sigma_\\kappa^2$ for both components\n",
    "\n",
    "3. **Regression Component**:\n",
    "   - Time-varying coefficient $\\beta_t$\n",
    "   - Random walk evolution\n",
    "   - Innovation variance $\\sigma_\\beta^2$\n",
    "\n",
    "4. **Complete System**:\n",
    "   - State dimension: 4\n",
    "   - $x_t$ enters via Z matrix\n",
    "   - All disturbances are uncorrelated\n",
    "\n",
    "## State Evolution Equations\n",
    "\n",
    "1. Random walk:\n",
    "   $$\\mu_t = \\mu_{t-1} + \\eta_t$$\n",
    "\n",
    "2. Stochastic cycle:\n",
    "   $$\\begin{bmatrix} \\psi_t \\\\ \\psi_t^* \\end{bmatrix} = \\rho\\begin{bmatrix} \\cos(\\lambda) & \\sin(\\lambda) \\\\ -\\sin(\\lambda) & \\cos(\\lambda) \\end{bmatrix} \\begin{bmatrix} \\psi_{t-1} \\\\ \\psi_{t-1}^* \\end{bmatrix} + \\begin{bmatrix} \\kappa_t \\\\ \\kappa_t^* \\end{bmatrix}$$\n",
    "\n",
    "3. Regression coefficient:\n",
    "   $$\\beta_t = \\beta_{t-1} + \\zeta_t$$\n",
    "\n",
    "## Observation Equation\n",
    "$$y_t = \\begin{bmatrix} 1 & 1 & 0 & x_t \\end{bmatrix} \\begin{bmatrix} \\mu_t \\\\ \\psi_t \\\\ \\psi_t^* \\\\ \\beta_t \\end{bmatrix} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Stochastic Cycle\n",
    "\n",
    "## 1. Single Form vs Seasonality\n",
    "\n",
    "Unlike seasonality, the stochastic cycle comes in only one form. This is because the cycle is inherently defined using trigonometric functions (sine and cosine). The reason is fundamental:\n",
    "\n",
    "- **Seasonality** models a pattern that repeats at fixed, known intervals (like days of the week). This can be done either by directly modeling each period's effect (dummy approach) or by using trigonometric functions.\n",
    "\n",
    "- **Cycle** models a smooth, wave-like pattern where the period itself might vary over time. It can only be effectively modeled using trigonometric functions.\n",
    "\n",
    "## 2. The Role of ψ and ψ*\n",
    "\n",
    "The stochastic cycle uses two components (ψ_t and ψ*_t) to create a flexible cyclical pattern. Here's why:\n",
    "\n",
    "### Basic Cycle Evolution\n",
    "```\n",
    "[ψ_t   ]  =  ρ[cos(λ)  sin(λ) ] [ψ_{t-1}  ]  +  [κ_t  ]\n",
    "[ψ*_t  ]     [-sin(λ)  cos(λ) ] [ψ*_{t-1} ]     [κ*_t ]\n",
    "```\n",
    "\n",
    "where:\n",
    "- λ is the frequency (determines cycle length)\n",
    "- ρ is the damping factor (0 < ρ ≤ 1)\n",
    "- κ_t and κ*_t are independent disturbances\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "1. **ψ_t (Primary Component)**:\n",
    "   - This is the actual cycle component that enters the observation equation\n",
    "   - Represents the current position in the cycle\n",
    "\n",
    "2. **ψ*_t (Auxiliary Component)**:\n",
    "   - Doesn't enter the observation equation directly\n",
    "   - Helps create the circular motion of the cycle\n",
    "   - Acts like a \"memory\" of where the cycle is heading\n",
    "\n",
    "Together, they create a flexible rotating movement in a 2-dimensional space where:\n",
    "- ψ_t represents the x-coordinate\n",
    "- ψ*_t represents the y-coordinate\n",
    "\n",
    "## Why Two Components are Necessary\n",
    "\n",
    "The two components are needed because:\n",
    "\n",
    "1. **Single Dimension Limitation**:\n",
    "   - With just one component, you could only move back and forth along a line\n",
    "   - You couldn't capture the smooth, circular nature of cycles\n",
    "\n",
    "2. **Phase Information**:\n",
    "   - ψ*_t stores information about the phase of the cycle\n",
    "   - Helps determine whether the cycle is increasing or decreasing\n",
    "\n",
    "3. **Smooth Transitions**:\n",
    "   - The interaction between ψ_t and ψ*_t creates smooth transitions\n",
    "   - Prevents sudden jumps that would occur with a single component\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "1. **Frequency (λ)**:\n",
    "   - Controls how fast the cycle completes one rotation\n",
    "   - Period = 2π/λ\n",
    "   - Fixed parameter (estimated from data)\n",
    "\n",
    "2. **Damping Factor (ρ)**:\n",
    "   - Controls how quickly the cycle dies out\n",
    "   - ρ = 1: persistent cycle\n",
    "   - ρ < 1: dying cycle\n",
    "   - Also fixed parameter\n",
    "\n",
    "3. **Disturbances (κ_t, κ*_t)**:\n",
    "   - Allow the cycle to evolve stochastically\n",
    "   - Make each cycle different from the last\n",
    "   - Usually assumed to have equal variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5:\n",
    "Come si costruisce la funzione di verosimiglianza di un modello Gaussiano in forma state-space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Function Construction for Gaussian State Space Models\n",
    "\n",
    "## 1. State Space Model Structure\n",
    "\n",
    "Consider a state space model in its general form:\n",
    "\n",
    "**Observation equation:**\n",
    "$$y_t = Z_t\\alpha_t + d_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, H_t)$$\n",
    "\n",
    "**State equation:**\n",
    "$$\\alpha_t = T_t\\alpha_{t-1} + c_t + R_t\\eta_t, \\quad \\eta_t \\sim N(0, Q_t)$$\n",
    "\n",
    "## 2. Likelihood Function Components\n",
    "\n",
    "The log-likelihood function is built from the prediction errors (innovations):\n",
    "\n",
    "$$\\ell(\\theta) = -\\frac{1}{2}\\sum_{t=1}^n \\left[ k\\log(2\\pi) + \\log|F_t| + v_t'F_t^{-1}v_t \\right]$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ is the vector of parameters to be estimated\n",
    "- $k$ is the dimension of the observation vector $y_t$\n",
    "- $v_t$ is the innovation vector\n",
    "- $F_t$ is the variance matrix of the innovations\n",
    "- $n$ is the sample size\n",
    "\n",
    "## 3. Construction Steps\n",
    "\n",
    "### Step 1: Initialize\n",
    "- Set initial state: $a_0 = E(\\alpha_0)$\n",
    "- Set initial variance: $P_0 = Var(\\alpha_0)$\n",
    "\n",
    "### Step 2: Kalman Filter Recursions\n",
    "For t = 1 to n:\n",
    "\n",
    "1. **Prediction step:**\n",
    "   $$a_{t|t-1} = T_ta_{t-1} + c_t$$\n",
    "   $$P_{t|t-1} = T_tP_{t-1}T_t' + R_tQ_tR_t'$$\n",
    "\n",
    "2. **Innovation calculations:**\n",
    "   $$v_t = y_t - Z_ta_{t|t-1} - d_t$$\n",
    "   $$F_t = Z_tP_{t|t-1}Z_t' + H_t$$\n",
    "\n",
    "3. **Update step:**\n",
    "   $$a_t = a_{t|t-1} + P_{t|t-1}Z_t'F_t^{-1}v_t$$\n",
    "   $$P_t = P_{t|t-1} - P_{t|t-1}Z_t'F_t^{-1}Z_tP_{t|t-1}$$\n",
    "\n",
    "### Step 3: Accumulate Log-Likelihood\n",
    "For each t, add to the log-likelihood:\n",
    "$$\\ell_t = -\\frac{1}{2}[k\\log(2\\pi) + \\log|F_t| + v_t'F_t^{-1}v_t]$$#\n",
    "\n",
    "## 4. Practical Implementation\n",
    "\n",
    "1. **Initialization Approaches:**\n",
    "   - For stationary components: use unconditional distribution\n",
    "   - For non-stationary components: use diffuse initialization\n",
    "\n",
    "2. **Numerical Considerations:**\n",
    "   - Use log-sum to prevent numerical overflow\n",
    "   - Handle missing values by skipping their contribution\n",
    "   - Check for positive definiteness of $F_t$\n",
    "\n",
    "3. **Parameter Constraints:**\n",
    "   - Ensure variance matrices remain positive definite\n",
    "   - Maintain stationarity conditions where required\n",
    "   - Handle boundary conditions appropriately\n",
    "\n",
    "## 5. Special Cases\n",
    "\n",
    "### Diffuse Initialization\n",
    "When some states have infinite variance:\n",
    "1. Skip likelihood contribution for first d observations\n",
    "2. Use modified likelihood for subsequent observations\n",
    "\n",
    "### Missing Observations\n",
    "When $y_t$ is partially missing:\n",
    "1. Remove missing elements from observation equation\n",
    "2. Adjust dimensions of $Z_t$ and $H_t$ accordingly\n",
    "\n",
    "### Time-Invariant Systems\n",
    "When matrices are constant:\n",
    "1. Simplified storage requirements\n",
    "2. Potential for computational optimizations\n",
    "\n",
    "## 6. Maximum Likelihood Estimation\n",
    "\n",
    "The likelihood function is maximized numerically:\n",
    "\n",
    "1. **Optimization Methods:**\n",
    "   - Quasi-Newton methods (BFGS)\n",
    "   - Simplex algorithm (Nelder-Mead)\n",
    "   - Grid search for initial values\n",
    "\n",
    "2. **Parameter Transformations:**\n",
    "   - Log transform for variances\n",
    "   - Logit transform for correlations\n",
    "   - Ensure parameter constraints\n",
    "\n",
    "3. **Standard Errors:**\n",
    "   Obtained from numerical second derivatives:\n",
    "   $$Var(\\hat{\\theta}) \\approx \\left[-\\frac{\\partial^2\\ell(\\theta)}{\\partial\\theta\\partial\\theta'}\\right]^{-1}_{\\theta=\\hat{\\theta}}$$\n",
    "\n",
    "## 7. Diagnostic Checks\n",
    "\n",
    "After maximizing the likelihood:\n",
    "\n",
    "1. Check standardized innovations for:\n",
    "   - Serial correlation\n",
    "   - Normality\n",
    "   - Homoscedasticity\n",
    "\n",
    "2. Check parameter significance using:\n",
    "   - t-statistics\n",
    "   - Likelihood ratio tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Likelihood in State Space Models: A Simple Guide\n",
    "\n",
    "## The Basic Idea\n",
    "\n",
    "Imagine you're tracking the position of a moving object, but you can only see it through a foggy window (noisy observations). You want to:\n",
    "1. Know where the object really is (state estimation)\n",
    "2. Know how good your tracking system is (likelihood)\n",
    "\n",
    "## What is the Kalman Filter?\n",
    "\n",
    "The Kalman filter is like a smart prediction system that:\n",
    "1. Makes a guess about where the object will be (prediction)\n",
    "2. Looks at the actual observation\n",
    "3. Updates its guess based on how wrong it was (updating)\n",
    "4. Learns how much to trust its predictions vs observations\n",
    "\n",
    "Think of it like GPS navigation:\n",
    "- Your phone predicts where you'll be based on your speed and direction\n",
    "- It gets actual GPS readings\n",
    "- It combines both pieces of information to give you your best estimated position\n",
    "\n",
    "## How Does Likelihood Come Into Play?\n",
    "\n",
    "The likelihood tells us \"how likely\" our model is to produce the data we see. It's built by:\n",
    "\n",
    "1. **Making Predictions**\n",
    "   - Using our model to predict the next observation\n",
    "   - Like guessing where a ball will land based on its trajectory\n",
    "\n",
    "2. **Comparing to Reality**\n",
    "   - Seeing how far off our predictions were\n",
    "   - The smaller the errors, the better our model\n",
    "\n",
    "3. **Building the Score (Likelihood)**\n",
    "   - Good predictions (small errors) → Higher likelihood\n",
    "   - Bad predictions (large errors) → Lower likelihood\n",
    "\n",
    "## Simple Example\n",
    "\n",
    "Let's say we're tracking temperature:\n",
    "\n",
    "1. **State Space Model Components:**\n",
    "   - True temperature (state we can't directly observe)\n",
    "   - Thermometer reading (noisy observation)\n",
    "   - How temperature typically evolves\n",
    "   - How noisy our thermometer is\n",
    "\n",
    "2. **For Each New Reading:**\n",
    "   - Predict temperature based on previous information\n",
    "   - Take new thermometer reading\n",
    "   - Compare prediction to reading\n",
    "   - Update our understanding\n",
    "   - Add to our likelihood score\n",
    "\n",
    "3. **Final Likelihood:**\n",
    "   - Combines all these prediction errors\n",
    "   - Tells us how well our model fits the data\n",
    "   - Helps us choose the best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Kalman Filter Estimation\n",
    "\n",
    "## Basic Concept\n",
    "\n",
    "The Kalman filter is like a \"smart averaging\" system that combines:\n",
    "1. What we expect based on our model\n",
    "2. What we actually observe\n",
    "3. How much we trust each piece of information\n",
    "\n",
    "## Simple Example: Tracking a Car's Position\n",
    "\n",
    "Imagine tracking a car's position with GPS. At each moment:\n",
    "\n",
    "### 1. Prediction Step\n",
    "We predict where the car should be based on:\n",
    "- Last known position\n",
    "- Speed\n",
    "- Direction\n",
    "\n",
    "$$\\underbrace{\\hat{x}_{t|t-1}}_{\\text{prediction}} = \\underbrace{\\hat{x}_{t-1}}_{\\text{last position}} + \\underbrace{v\\Delta t}_{\\text{speed × time}}$$\n",
    "\n",
    "### 2. Measurement Step\n",
    "We get a GPS reading (with some error):\n",
    "\n",
    "$$\\underbrace{z_t}_{\\text{GPS reading}} = \\underbrace{x_t}_{\\text{true position}} + \\underbrace{\\varepsilon_t}_{\\text{measurement noise}}$$\n",
    "\n",
    "### 3. Update Step\n",
    "We combine our prediction with the GPS reading:\n",
    "\n",
    "$$\\underbrace{\\hat{x}_t}_{\\text{final estimate}} = \\underbrace{\\hat{x}_{t|t-1}}_{\\text{prediction}} + \\underbrace{K_t}_{\\text{Kalman gain}} (\\underbrace{z_t - \\hat{x}_{t|t-1}}_{\\text{measurement error}})$$\n",
    "\n",
    "The Kalman gain $K_t$ is like a \"trust factor\" that decides how much to trust:\n",
    "- Our prediction vs. GPS reading\n",
    "- Higher $K_t$ → Trust GPS more\n",
    "- Lower $K_t$ → Trust prediction more\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Adaptive Trust**:\n",
    "   - If GPS is usually accurate → Trust it more\n",
    "   - If car moves predictably → Trust predictions more\n",
    "   - Automatically adjusts based on performance\n",
    "\n",
    "2. **Error Handling**:\n",
    "   - Accounts for both prediction and measurement errors\n",
    "   - More uncertain → Less trust\n",
    "   - More precise → More trust\n",
    "\n",
    "3. **Memory**:\n",
    "   - Maintains running estimates\n",
    "   - Uses all past information efficiently\n",
    "   - Updates beliefs smoothly\n",
    "\n",
    "## Why It Works\n",
    "\n",
    "The Kalman filter is optimal because it:\n",
    "1. Minimizes estimation errors\n",
    "2. Accounts for all known uncertainties\n",
    "3. Updates estimates efficiently\n",
    "4. Adapts to changing conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6:\n",
    "\n",
    "Condizioni di stazionarietà di un processo ARMA. Il processo AR(2) yt = 1.5yt−1 − 0.5yt−2 + εt è stazionario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMA Process Stationarity\n",
    "\n",
    "## Understanding Stationarity\n",
    "\n",
    "Let's start with what stationarity means in practical terms. A process is stationary if its statistical properties don't change over time. This means:\n",
    "\n",
    "1. Constant mean: $E[Y_t] = \\mu$ (same for all t)\n",
    "2. Constant variance: $Var(Y_t) = \\sigma^2$ (same for all t)\n",
    "3. Covariance depends only on time difference: $Cov(Y_t, Y_{t+h}) = \\gamma(h)$\n",
    "\n",
    "## Stationarity Conditions for AR Processes\n",
    "\n",
    "For an AR(p) process:\n",
    "$$Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + ... + \\phi_pY_{t-p} + \\varepsilon_t$$\n",
    "\n",
    "The stationarity condition involves the characteristic equation:\n",
    "$$1 - \\phi_1z - \\phi_2z^2 - ... - \\phi_pz^p = 0$$\n",
    "\n",
    "The process is stationary if and only if all roots of this equation lie outside the unit circle (have modulus greater than 1).\n",
    "\n",
    "## For Our Specific AR(2) Process\n",
    "\n",
    "Let's analyze: $y_t = 1.5y_{t-1} - 0.5y_{t-2} + \\varepsilon_t$\n",
    "\n",
    "Step 1: Write the characteristic equation\n",
    "- Original equation: $y_t = 1.5y_{t-1} - 0.5y_{t-2} + \\varepsilon_t$\n",
    "- Rearrange: $y_t - 1.5y_{t-1} + 0.5y_{t-2} = \\varepsilon_t$\n",
    "- Characteristic equation: $1 - 1.5z + 0.5z^2 = 0$\n",
    "\n",
    "Step 2: Find the roots\n",
    "- This is a quadratic equation: $0.5z^2 - 1.5z + 1 = 0$\n",
    "- Using the quadratic formula: $z = \\frac{1.5 \\pm \\sqrt{2.25 - 2}}{1}$\n",
    "- $z = \\frac{1.5 \\pm \\sqrt{0.25}}{1}$\n",
    "- $z = \\frac{1.5 \\pm 0.5}{1}$\n",
    "- Roots are: $z_1 = 2$ and $z_2 = 1$\n",
    "\n",
    "Step 3: Check stationarity\n",
    "- One root is $z_1 = 2$ (outside unit circle)\n",
    "- Other root is $z_2 = 1$ (exactly on unit circle)\n",
    "- Since we have a root on the unit circle, this process is NOT stationary\n",
    "\n",
    "## Visual Explanation\n",
    "\n",
    "Consider what this means:\n",
    "1. Having a root on the unit circle means the process has \"infinite memory\"\n",
    "2. The process won't \"forget\" past shocks\n",
    "3. This creates persistent effects that prevent mean reversion\n",
    "4. Therefore, the process can wander without returning to any fixed mean\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "A non-stationary process like this one:\n",
    "1. Won't have a constant mean\n",
    "2. Won't have a constant variance\n",
    "3. Will show persistent effects from shocks\n",
    "4. May need differencing to become stationary\n",
    "\n",
    "## Alternative Form: Factored Representation\n",
    "\n",
    "We can write our characteristic equation in factored form:\n",
    "$$(1 - \\frac{1}{2}z)(1 - z) = 0$$\n",
    "\n",
    "This clearly shows:\n",
    "1. One root at $z = 2$ (stationary component)\n",
    "2. One root at $z = 1$ (unit root, non-stationary component)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This AR(2) process is not stationary because:\n",
    "1. It has a unit root $(z = 1)$\n",
    "2. It would need first differencing to become stationary\n",
    "3. It is actually an integrated process of order 1, or I(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8:\n",
    "\n",
    "Genesi e proprietà del ciclo stocastico stazionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genesis and Properties of the Stationary Stochastic Cycle\n",
    "\n",
    "The stochastic cycle emerges from the deterministic cycle through a process of \"stochasticization\". Let's understand this step by step:\n",
    "\n",
    "## 1. Starting from the Deterministic Cycle\n",
    "\n",
    "A deterministic cycle can be represented as a sinusoidal function:\n",
    "\n",
    "$$f(t) = R \\cos(\\phi + \\lambda t)$$\n",
    "\n",
    "where:\n",
    "- $R$ is the amplitude (the cycle oscillates between $+R$ and $-R$)\n",
    "- $\\lambda$ is the frequency (number of cycles per unit time)\n",
    "- $\\phi$ is the phase (which shifts the cosine left or right)\n",
    "\n",
    "This can be rewritten equivalently as:\n",
    "\n",
    "$$f(t) = A\\cos(\\lambda t) + B\\sin(\\lambda t)$$\n",
    "\n",
    "where:\n",
    "- $A = R\\cos(\\phi)$\n",
    "- $B = -R\\sin(\\phi)$\n",
    "\n",
    "## 2. Markov Representation\n",
    "\n",
    "For discrete time $t$, we can write this in a recursive form:\n",
    "\n",
    "$$\\begin{pmatrix} f_t \\\\ f^*_t \\end{pmatrix} = \\begin{pmatrix} \\cos\\lambda & \\sin\\lambda \\\\ -\\sin\\lambda & \\cos\\lambda \\end{pmatrix} \\begin{pmatrix} f_{t-1} \\\\ f^*_{t-1} \\end{pmatrix}$$\n",
    "\n",
    "where $f^*_t$ is an auxiliary variable that helps generate the cycle.\n",
    "\n",
    "## 3. Making it Stochastic\n",
    "\n",
    "To create a stochastic cycle, we add random innovations and a damping factor:\n",
    "\n",
    "$$\\begin{pmatrix} \\psi_t \\\\ \\psi^*_t \\end{pmatrix} = \\rho \\begin{pmatrix} \\cos\\lambda & \\sin\\lambda \\\\ -\\sin\\lambda & \\cos\\lambda \\end{pmatrix} \\begin{pmatrix} \\psi_{t-1} \\\\ \\psi^*_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\kappa_t \\\\ \\kappa^*_t \\end{pmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\rho$ is the damping factor $(0 \\leq \\rho < 1)$\n",
    "- $\\kappa_t, \\kappa^*_t$ are white noise disturbances with variance $\\sigma^2_\\kappa$\n",
    "- $\\psi_t$ is the stochastic cycle\n",
    "- $\\psi^*_t$ is its auxiliary component\n",
    "\n",
    "## 4. Key Properties\n",
    "\n",
    "1. **Stationarity**: The cycle is stationary when $0 \\leq \\rho < 1$. The damping factor $\\rho$ ensures that shocks have a diminishing effect over time.\n",
    "\n",
    "2. **Period**: The period of the cycle is $2\\pi/\\lambda$. For example, if we want a cycle of 20 time units, we set $\\lambda = 2\\pi/20$.\n",
    "\n",
    "3. **Persistence**: $\\rho$ controls how long cycles persist. Values close to 1 create long-lasting cycles, while smaller values create more rapidly dampening cycles.\n",
    "\n",
    "4. **Innovation Variance**: $\\sigma^2_\\kappa$ determines how much random variation enters the cycle at each time point.\n",
    "\n",
    "5. **Complex Roots**: The transition matrix has complex eigenvalues $\\rho(\\cos\\lambda \\pm i\\sin\\lambda)$, which create the cyclical behavior.\n",
    "\n",
    "## 5. Interpretation\n",
    "\n",
    "The stochastic cycle combines:\n",
    "- Regular cyclical movement (from the rotation matrix)\n",
    "- Persistence (through $\\rho$)\n",
    "- Random innovations (via $\\kappa_t$)\n",
    "\n",
    "This makes it ideal for modeling economic cycles, where we observe:\n",
    "- Regular but not perfectly periodic fluctuations\n",
    "- Gradual changes in amplitude and phase\n",
    "- Random shocks that affect the cycle\n",
    "\n",
    "The stochastic cycle is a key component in structural time series models, often combined with trend and seasonal components to create comprehensive models of economic time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9:\n",
    "\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- random walk integrato,\n",
    "- stagionalità trimestrale a dummy stocastiche,\n",
    "- rumore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Matrices for UCM with Integrated Random Walk and Quarterly Seasonality\n",
    "\n",
    "Let's construct the state space representation for this Unobserved Components Model. The model combines:\n",
    "1. An integrated random walk for the trend\n",
    "2. Quarterly stochastic dummy seasonality\n",
    "3. Observation noise\n",
    "\n",
    "## State Space Form\n",
    "\n",
    "A state space model is generally written as:\n",
    "\n",
    "Observation equation:\n",
    "$$y_t = Z\\alpha_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma^2_\\varepsilon)$$\n",
    "\n",
    "State equation:\n",
    "$$\\alpha_t = T\\alpha_t + R\\eta_t, \\quad \\eta_t \\sim N(0, Q)$$\n",
    "\n",
    "## Component Breakdown\n",
    "\n",
    "### 1. Integrated Random Walk\n",
    "The integrated random walk is a double-integrated process where:\n",
    "$$\\mu_t = \\mu_{t-1} + \\beta_{t-1}$$\n",
    "$$\\beta_t = \\beta_{t-1} + \\zeta_t$$\n",
    "\n",
    "### 2. Quarterly Seasonal Component\n",
    "For quarterly data (s=4), the seasonal component requires 3 states due to the zero-sum constraint:\n",
    "$$\\gamma_t = -\\gamma_{t-1} - \\gamma_{t-2} - \\gamma_{t-3} + \\omega_t$$\n",
    "\n",
    "### 3. Complete State Vector\n",
    "The state vector combines all components:\n",
    "\n",
    "$$\\alpha_t = \\begin{pmatrix} \\mu_t \\\\ \\beta_t \\\\ \\gamma_t \\\\ \\gamma_{t-1} \\\\ \\gamma_{t-2} \\end{pmatrix}$$\n",
    "\n",
    "## The State Space Matrices\n",
    "\n",
    "### Transition Matrix T:\n",
    "$$T = \\begin{pmatrix} \n",
    "1 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & -1 & -1 & -1 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "### Observation Matrix Z:\n",
    "$$Z = \\begin{pmatrix} 1 & 0 & 1 & 0 & 0 \\end{pmatrix}$$\n",
    "\n",
    "### Selection Matrix R:\n",
    "$$R = \\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "### Disturbance Covariance Matrix Q:\n",
    "$$Q = \\begin{pmatrix}\n",
    "\\sigma^2_\\zeta & 0 & 0 \\\\\n",
    "0 & \\sigma^2_\\omega & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "## Understanding the Matrices\n",
    "\n",
    "1. **T matrix**: \n",
    "   - The upper 2×2 block handles the integrated random walk\n",
    "   - The lower 3×3 block manages the seasonal component\n",
    "   - Zeros elsewhere ensure components evolve independently\n",
    "\n",
    "2. **Z matrix**:\n",
    "   - The 1 in first position extracts the trend level\n",
    "   - The 1 in third position adds the current seasonal\n",
    "   - Zeros elsewhere exclude other state elements from observation\n",
    "\n",
    "3. **R and Q matrices**:\n",
    "   - Together they determine how innovations enter the system\n",
    "   - Innovations affect the slope ($\\zeta_t$) and seasonal ($\\omega_t$)\n",
    "   - The zeros in R and Q ensure some states only transfer values\n",
    "\n",
    "The observation equation includes additional noise:\n",
    "$$y_t = Z\\alpha_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma^2_\\varepsilon)$$\n",
    "\n",
    "This complete state space formulation allows us to:\n",
    "- Track the evolution of trend and seasonal components\n",
    "- Apply the Kalman filter for estimation\n",
    "- Generate predictions\n",
    "- Decompose the series into its components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Trend Models: Local Linear Trend vs. Integrated Random Walk\n",
    "\n",
    "Let's break down these concepts carefully to understand their differences and relationships.\n",
    "\n",
    "## 1. Local Linear Trend (LLT)\n",
    "\n",
    "The Local Linear Trend is defined by two equations:\n",
    "\n",
    "$$\\mu_t = \\mu_{t-1} + \\beta_{t-1} + \\eta_t \\quad \\text{(level equation)}$$\n",
    "$$\\beta_t = \\beta_{t-1} + \\zeta_t \\quad \\text{(slope equation)}$$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the level\n",
    "- $\\beta_t$ is the slope\n",
    "- $\\eta_t \\sim N(0, \\sigma^2_\\eta)$ is the level disturbance\n",
    "- $\\zeta_t \\sim N(0, \\sigma^2_\\zeta)$ is the slope disturbance\n",
    "\n",
    "Key characteristics:\n",
    "1. Has TWO sources of randomness ($\\eta_t$ and $\\zeta_t$)\n",
    "2. The level equation includes both the previous slope and a random shock\n",
    "3. The slope follows a random walk\n",
    "\n",
    "## 2. Integrated Random Walk (IRW)\n",
    "\n",
    "The Integrated Random Walk is a special case of the LLT where $\\sigma^2_\\eta = 0$:\n",
    "\n",
    "$$\\mu_t = \\mu_{t-1} + \\beta_{t-1} \\quad \\text{(level equation)}$$\n",
    "$$\\beta_t = \\beta_{t-1} + \\zeta_t \\quad \\text{(slope equation)}$$\n",
    "\n",
    "Key characteristics:\n",
    "1. Has only ONE source of randomness ($\\zeta_t$)\n",
    "2. The level equation is deterministic given the previous state\n",
    "3. The slope follows a random walk\n",
    "4. It's \"integrated\" because the level $\\mu_t$ is the integral (cumulative sum) of the random walk $\\beta_t$\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "1. **Number of Disturbances:**\n",
    "   - LLT: Has both level ($\\eta_t$) and slope ($\\zeta_t$) disturbances\n",
    "   - IRW: Has only slope disturbances ($\\zeta_t$)\n",
    "\n",
    "2. **Smoothness:**\n",
    "   - LLT: Generally less smooth due to level shocks\n",
    "   - IRW: Smoother because changes only come through the slope\n",
    "\n",
    "3. **Flexibility:**\n",
    "   - LLT: More flexible in adapting to sudden changes\n",
    "   - IRW: Changes must occur gradually through slope adjustments\n",
    "\n",
    "4. **Path Dependencies:**\n",
    "   - LLT: Can have immediate jumps in level\n",
    "   - IRW: All changes must accumulate through the slope\n",
    "\n",
    "## Why \"Integrated\"?\n",
    "\n",
    "The term \"integrated\" in IRW comes from the fact that:\n",
    "1. The slope $\\beta_t$ is a random walk\n",
    "2. The level $\\mu_t$ is the cumulative sum (integral) of this random walk\n",
    "3. This makes $\\mu_t$ an I(2) process (needs differencing twice to become stationary)\n",
    "\n",
    "## Choosing Between Models\n",
    "\n",
    "1. Use LLT when:\n",
    "   - The series can have sudden level shifts\n",
    "   - You need more flexibility in the trend\n",
    "   - The underlying process might have abrupt changes\n",
    "\n",
    "2. Use IRW when:\n",
    "   - The trend should be smooth\n",
    "   - Changes should occur gradually\n",
    "   - You want to enforce continuity in the trend\n",
    "\n",
    "The choice often depends on the nature of your data and the type of trend behavior you expect to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    " Understanding Random Walks vs. Integrated Random Walks\n",
    "\n",
    "## Simple Random Walk\n",
    "\n",
    "A Random Walk (RW) is the simplest non-stationary process. It's defined by:\n",
    "\n",
    "$$y_t = y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "Key properties:\n",
    "1. Each step is independent of previous steps\n",
    "2. The change in position (first difference) is just white noise\n",
    "3. It's an I(1) process - needs one difference to become stationary\n",
    "4. The variance grows linearly with time: $Var(y_t) = t\\sigma^2$\n",
    "\n",
    "Think of it like a drunk person walking: each step is random and independent of previous steps.\n",
    "\n",
    "## Integrated Random Walk\n",
    "\n",
    "An Integrated Random Walk (IRW) has two layers:\n",
    "\n",
    "$$\\mu_t = \\mu_{t-1} + \\beta_{t-1} \\quad \\text{(level)}$$\n",
    "$$\\beta_t = \\beta_{t-1} + \\zeta_t \\quad \\text{(slope)}$$\n",
    "\n",
    "Key properties:\n",
    "1. The slope follows a Random Walk\n",
    "2. The level accumulates (integrates) the Random Walk\n",
    "3. It's an I(2) process - needs two differences to become stationary\n",
    "4. The variance grows much faster: approximately $t^5/20$ for large $t$\n",
    "5. Much smoother paths than a Random Walk\n",
    "\n",
    "Think of it like a car with random accelerations: the speed follows a random walk, and the position accumulates these speed changes.\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "1. **Order of Integration:**\n",
    "   - RW: I(1) - difference once to get white noise\n",
    "   - IRW: I(2) - need to difference twice\n",
    "\n",
    "2. **Path Smoothness:**\n",
    "   - RW: Jagged, can change direction suddenly\n",
    "   - IRW: Smooth, changes direction gradually\n",
    "\n",
    "3. **Memory:**\n",
    "   - RW: No memory of past changes\n",
    "   - IRW: Changes accumulate through the slope\n",
    "\n",
    "4. **Variance Growth:**\n",
    "   - RW: Grows linearly with time\n",
    "   - IRW: Grows much faster (∝ t⁵)\n",
    "\n",
    "## Visual Interpretation\n",
    "\n",
    "In the graphs above:\n",
    "- The Random Walk shows more jagged movements\n",
    "- The Integrated Random Walk shows smoother, more persistent trends\n",
    "- The IRW slope (bottom graph) shows how the rate of change evolves\n",
    "\n",
    "This is why IRWs are often used for trend modeling - they produce smoother, more natural-looking trends than simple Random Walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10:\n",
    "\n",
    "L’inizializzazione del vettore di stato in un modello in forma state space: si considerino i casi di variabili di stato stazionarie e non stazionarie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the State Vector in State Space Models\n",
    "\n",
    "The initialization of state space models is a crucial step that determines how we begin our filtering and smoothing procedures. Let's understand how we handle both stationary and non-stationary components.\n",
    "\n",
    "## General Form of State Space Models\n",
    "\n",
    "Remember that a state space model has the form:\n",
    "\n",
    "Observation equation:\n",
    "$$y_t = Z_t\\alpha_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, H_t)$$\n",
    "\n",
    "State equation:\n",
    "$$\\alpha_t = T_t\\alpha_{t-1} + R_t\\eta_t, \\quad \\eta_t \\sim N(0, Q_t)$$\n",
    "\n",
    "To start the Kalman filter, we need initial values:\n",
    "$$\\alpha_0 \\sim N(a_0, P_0)$$\n",
    "\n",
    "## Initialization for Stationary Components\n",
    "\n",
    "When components are stationary, we can use their unconditional distribution for initialization:\n",
    "\n",
    "1. **Mean Initialization ($a_0$):**\n",
    "   The unconditional mean satisfies:\n",
    "   $$E(\\alpha_t) = TE(\\alpha_{t-1})$$\n",
    "   Therefore:\n",
    "   $$a_0 = (I - T)^{-1}c$$\n",
    "   where $c$ is the constant term in the state equation.\n",
    "\n",
    "2. **Variance Initialization ($P_0$):**\n",
    "   The unconditional variance satisfies:\n",
    "   $$P_0 = TP_0T' + RQR'$$\n",
    "   This can be solved as:\n",
    "   $$vec(P_0) = (I - T \\otimes T)^{-1}vec(RQR')$$\n",
    "   where $\\otimes$ denotes the Kronecker product.\n",
    "\n",
    "For example, for a stationary AR(1) component:\n",
    "$$\\alpha_t = \\phi\\alpha_{t-1} + \\eta_t, \\quad |\\phi| < 1$$\n",
    "The initialization would be:\n",
    "$$a_0 = 0, \\quad P_0 = \\frac{\\sigma^2_\\eta}{1-\\phi^2}$$\n",
    "\n",
    "## Initialization for Non-stationary Components\n",
    "\n",
    "Non-stationary components require different treatment because their unconditional distribution doesn't exist. We have two main approaches:\n",
    "\n",
    "1. **Diffuse Initialization:**\n",
    "   For non-stationary components, we use:\n",
    "   $$P_0 = \\kappa I, \\quad \\kappa \\to \\infty$$\n",
    "   This represents complete uncertainty about the initial state.\n",
    "\n",
    "2. **Exact Diffuse Initialization:**\n",
    "   We decompose the initial state into:\n",
    "   $$\\alpha_0 = a_0 + A_0\\delta$$\n",
    "   where:\n",
    "   - $a_0$ is a known vector\n",
    "   - $A_0$ is a selection matrix\n",
    "   - $\\delta$ is a vector of diffuse elements\n",
    "\n",
    "For example, for a random walk:\n",
    "$$\\alpha_t = \\alpha_{t-1} + \\eta_t$$\n",
    "We would use:\n",
    "$$a_0 = 0, \\quad P_0 = \\kappa \\quad (\\kappa \\to \\infty)$$\n",
    "\n",
    "## Mixed Cases\n",
    "\n",
    "Many models contain both stationary and non-stationary components. In these cases:\n",
    "\n",
    "1. **Split the State Vector:**\n",
    "   $$\\alpha_t = \\begin{pmatrix} \\alpha^s_t \\\\ \\alpha^n_t \\end{pmatrix}$$\n",
    "   where:\n",
    "   - $\\alpha^s_t$ contains stationary components\n",
    "   - $\\alpha^n_t$ contains non-stationary components\n",
    "\n",
    "2. **Initialize Separately:**\n",
    "   - Use unconditional distribution for $\\alpha^s_t$\n",
    "   - Use diffuse initialization for $\\alpha^n_t$\n",
    "\n",
    "For example, in a local level model with AR(1) seasonal:\n",
    "- Initialize the level component with diffuse prior\n",
    "- Initialize the seasonal component using its stationary distribution\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "1. **Numerical Implementation:**\n",
    "   - For diffuse initialization, use a large but finite value for $\\kappa$ (e.g., $10^7$)\n",
    "   - Handle potential numerical instability in matrix inversions\n",
    "\n",
    "2. **Software Implementation:**\n",
    "   - Most software packages handle initialization automatically\n",
    "   - May need to specify if components are diffuse or stationary\n",
    "\n",
    "3. **Diagnostics:**\n",
    "   - Check sensitivity to initialization choices\n",
    "   - Monitor convergence of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11:\n",
    "\n",
    "Condizioni di stazionarietà di un processo AR(p). Il processo Yt = 1.5Yt−1 − 0.6Yt−1 + εt, εt ∼ WN è stazionario? Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity Conditions for AR(p) Processes\n",
    "\n",
    "## General Theory\n",
    "\n",
    "An AR(p) process is defined as:\n",
    "\n",
    "$$Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + ... + \\phi_pY_{t-p} + \\varepsilon_t$$\n",
    "\n",
    "where $\\varepsilon_t$ is white noise.\n",
    "\n",
    "### Stationarity Condition\n",
    "\n",
    "The process is stationary if and only if all roots of the characteristic equation:\n",
    "\n",
    "$$1 - \\phi_1z - \\phi_2z^2 - ... - \\phi_pz^p = 0$$\n",
    "\n",
    "lie outside the unit circle (i.e., have modulus greater than 1).\n",
    "\n",
    "Equivalently, if we define the lag operator polynomial:\n",
    "$$\\phi(L) = 1 - \\phi_1L - \\phi_2L^2 - ... - \\phi_pL^p$$\n",
    "then all roots of $\\phi(z) = 0$ must lie outside the unit circle.\n",
    "\n",
    "### Special Cases:\n",
    "\n",
    "1. **AR(1)**: $Y_t = \\phi_1Y_{t-1} + \\varepsilon_t$\n",
    "   - Stationary if $|\\phi_1| < 1$\n",
    "\n",
    "2. **AR(2)**: $Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\varepsilon_t$\n",
    "   - Must satisfy three conditions:\n",
    "     1. $\\phi_2 + \\phi_1 < 1$\n",
    "     2. $\\phi_2 - \\phi_1 < 1$\n",
    "     3. $|\\phi_2| < 1$\n",
    "\n",
    "## Analysis of the Given Process\n",
    "\n",
    "Let's analyze: $Y_t = 1.5Y_{t-1} - 0.6Y_{t-1} + \\varepsilon_t$\n",
    "\n",
    "1. First, note there seems to be a typo in the equation as $Y_{t-1}$ appears twice.\n",
    "   We'll assume it should be: $Y_t = 1.5Y_{t-1} - 0.6Y_{t-2} + \\varepsilon_t$\n",
    "\n",
    "2. This is an AR(2) process with:\n",
    "   - $\\phi_1 = 1.5$\n",
    "   - $\\phi_2 = -0.6$\n",
    "\n",
    "3. Let's check the stationarity conditions:\n",
    "   - Condition 1: $\\phi_2 + \\phi_1 < 1$\n",
    "     - $-0.6 + 1.5 = 0.9 < 1$ ✓\n",
    "   - Condition 2: $\\phi_2 - \\phi_1 < 1$\n",
    "     - $-0.6 - 1.5 = -2.1 < 1$ ✓\n",
    "   - Condition 3: $|\\phi_2| < 1$\n",
    "     - $|-0.6| = 0.6 < 1$ ✓\n",
    "\n",
    "4. Alternative method: Find roots of characteristic equation:\n",
    "   $$1 - 1.5z + 0.6z^2 = 0$$\n",
    "   \n",
    "   Using the quadratic formula:\n",
    "   $$z = \\frac{1.5 \\pm \\sqrt{2.25 - 2.4}}{1.2} = \\frac{1.5 \\pm \\sqrt{-0.15}}{1.2}$$\n",
    "\n",
    "   The roots are complex with modulus:\n",
    "   $$|z| = \\sqrt{\\frac{2.25 + 0.15}{1.44}} = \\sqrt{\\frac{2.4}{1.44}} \\approx 1.29 > 1$$\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Yes, the process is stationary because:\n",
    "1. All three conditions for AR(2) stationarity are satisfied\n",
    "2. The roots of the characteristic equation have modulus greater than 1\n",
    "\n",
    "The complex roots indicate that the process will show cyclical behavior, but since their modulus is greater than 1, these cycles will be stationary rather than explosive.\n",
    "\n",
    "### Practical Interpretation\n",
    "\n",
    "A stationary AR(2) process with these coefficients will:\n",
    "1. Show oscillatory behavior (due to complex roots)\n",
    "2. Have a tendency to return to its mean value\n",
    "3. Have finite and constant variance\n",
    "4. Have autocorrelation function that decays over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Original Process (no typo)\n",
    "\n",
    "## Step 1: Simplify the Equation\n",
    "\n",
    "The original equation:\n",
    "$$Y_t = 1.5Y_{t-1} - 0.6Y_{t-1} + \\varepsilon_t$$\n",
    "\n",
    "Can be simplified by combining like terms:\n",
    "$$Y_t = (1.5 - 0.6)Y_{t-1} + \\varepsilon_t$$\n",
    "$$Y_t = 0.9Y_{t-1} + \\varepsilon_t$$\n",
    "\n",
    "## Step 2: Identify the Process\n",
    "\n",
    "After simplification, we can see this is actually an AR(1) process with:\n",
    "- $\\phi_1 = 0.9$\n",
    "- $\\varepsilon_t$ is white noise\n",
    "\n",
    "## Step 3: Check Stationarity Condition\n",
    "\n",
    "For an AR(1) process, the stationarity condition is simply $|\\phi_1| < 1$\n",
    "\n",
    "In this case:\n",
    "$$|\\phi_1| = |0.9| = 0.9 < 1$$\n",
    "\n",
    "## Step 4: Alternative Method - Characteristic Equation\n",
    "\n",
    "The characteristic equation is:\n",
    "$$1 - 0.9z = 0$$\n",
    "\n",
    "Solving for z:\n",
    "$$z = \\frac{1}{0.9} \\approx 1.11$$\n",
    "\n",
    "The root lies outside the unit circle (as $1.11 > 1$).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Yes, the process is stationary because:\n",
    "1. $|\\phi_1| = 0.9 < 1$\n",
    "2. Equivalently, the root of the characteristic equation lies outside the unit circle\n",
    "\n",
    "## Properties of this Process\n",
    "\n",
    "Since this is a stationary AR(1) with positive coefficient close to 1:\n",
    "1. It will show high persistence\n",
    "2. The autocorrelation function will decay slowly\n",
    "3. The process will tend to stay on the same side of its mean for several periods\n",
    "4. The unconditional variance will be:\n",
    "   $$\\sigma^2_Y = \\frac{\\sigma^2_\\varepsilon}{1-0.9^2} \\approx 5.26\\sigma^2_\\varepsilon$$\n",
    "\n",
    "This process is notably different from the previous interpretation:\n",
    "- It's an AR(1) rather than AR(2)\n",
    "- It shows no oscillatory behavior\n",
    "- It has simpler dynamics\n",
    "- It's more persistent but less volatile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12:\n",
    "\n",
    "Descrivere le funzioni di autocorrelaione e autocorrelazione parziale di un processo AR(p)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial Autocorrelation Functions of AR(p) Processes\n",
    "\n",
    "## General AR(p) Process\n",
    "\n",
    "An AR(p) process is defined as:\n",
    "$$Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + ... + \\phi_pY_{t-p} + \\varepsilon_t$$\n",
    "\n",
    "## Autocorrelation Function (ACF)\n",
    "\n",
    "### Key Properties:\n",
    "\n",
    "1. **Infinite Decay:**\n",
    "   - The ACF of an AR(p) process decays infinitely\n",
    "   - The decay pattern depends on the roots of the characteristic equation\n",
    "\n",
    "2. **Pattern Types:**\n",
    "   - For real roots: exponential decay\n",
    "   - For complex roots: damped sinusoidal pattern\n",
    "   - For multiple roots: mixture of patterns\n",
    "\n",
    "3. **Yule-Walker Equations:**\n",
    "   For lag k > 0:\n",
    "   $$\\rho_k = \\phi_1\\rho_{k-1} + \\phi_2\\rho_{k-2} + ... + \\phi_p\\rho_{k-p}$$\n",
    "\n",
    "### Special Cases:\n",
    "\n",
    "1. **AR(1):**\n",
    "   $$\\rho_k = \\phi_1^k$$\n",
    "   - Exponential decay\n",
    "   - Sign depends on φ₁'s sign\n",
    "   - Rate depends on |φ₁|\n",
    "\n",
    "2. **AR(2):**\n",
    "   - With real roots: sum of two exponentials\n",
    "   - With complex roots: damped sinusoid\n",
    "\n",
    "## Partial Autocorrelation Function (PACF)\n",
    "\n",
    "### Key Properties:\n",
    "\n",
    "1. **Cut-off Pattern:**\n",
    "   - PACF \"cuts off\" after lag p\n",
    "   - All values after lag p are theoretically zero\n",
    "\n",
    "2. **Values at Initial Lags:**\n",
    "   - For k ≤ p: PACF at lag k equals the last coefficient in AR(k) representation\n",
    "   - These values are the φₖₖ in the Yule-Walker equations\n",
    "\n",
    "3. **Interpretation:**\n",
    "   - PACF measures direct relationship between Yₜ and Yₜ₋ₖ\n",
    "   - Removes intermediate effects through Yₜ₋₁, ..., Yₜ₋ₖ₊₁\n",
    "\n",
    "### Special Cases:\n",
    "\n",
    "1. **AR(1):**\n",
    "   - PACF = φ₁ at lag 1\n",
    "   - PACF = 0 for all lags > 1\n",
    "\n",
    "2. **AR(2):**\n",
    "   - PACF = φ₁ at lag 1\n",
    "   - PACF = φ₂ at lag 2\n",
    "   - PACF = 0 for all lags > 2\n",
    "\n",
    "## Practical Identification\n",
    "\n",
    "These patterns are crucial for model identification:\n",
    "\n",
    "1. **ACF Properties:**\n",
    "   - Infinite extent\n",
    "   - Shape indicates type of roots\n",
    "   - Decay rate indicates parameter values\n",
    "\n",
    "2. **PACF Properties:**\n",
    "   - Sharp cut-off after lag p\n",
    "   - Values before cut-off indicate parameter values\n",
    "   - Used to determine order of AR process\n",
    "\n",
    "3. **Combined Use:**\n",
    "   - ACF: confirms process is AR\n",
    "   - PACF: determines order p\n",
    "   - Together: suggest parameter values\n",
    "\n",
    "## Examples in Visualizations\n",
    "\n",
    "The plots above show:\n",
    "\n",
    "1. **AR(1) with φ = 0.7:**\n",
    "   - ACF: exponential decay\n",
    "   - PACF: single spike at lag 1\n",
    "\n",
    "2. **AR(2) with complex roots:**\n",
    "   - ACF: damped sinusoidal pattern\n",
    "   - PACF: two spikes at lags 1 and 2\n",
    "\n",
    "These patterns are diagnostic tools for identifying AR processes in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial Autocorrelation Functions of MA(q) Processes\n",
    "\n",
    "## General MA(q) Process\n",
    "\n",
    "An MA(q) process is defined as:\n",
    "$$Y_t = \\varepsilon_t + \\theta_1\\varepsilon_{t-1} + \\theta_2\\varepsilon_{t-2} + ... + \\theta_q\\varepsilon_{t-q}$$\n",
    "\n",
    "## Autocorrelation Function (ACF)\n",
    "\n",
    "### Key Properties:\n",
    "\n",
    "1. **Finite Cut-off:**\n",
    "   - The ACF of an MA(q) process cuts off after lag q\n",
    "   - All autocorrelations at lags > q are theoretically zero\n",
    "\n",
    "2. **Explicit Formulas:**\n",
    "   For lag k:\n",
    "   $$\\rho_k = \\begin{cases}\n",
    "   \\frac{\\theta_k + \\theta_1\\theta_{k+1} + ... + \\theta_{q-k}\\theta_q}{1 + \\theta_1^2 + ... + \\theta_q^2} & \\text{for } k = 1,2,...,q \\\\\n",
    "   0 & \\text{for } k > q\n",
    "   \\end{cases}$$\n",
    "\n",
    "### Special Cases:\n",
    "\n",
    "1. **MA(1):**\n",
    "   $$\\rho_1 = \\frac{\\theta_1}{1 + \\theta_1^2}, \\quad \\rho_k = 0 \\text{ for } k > 1$$\n",
    "   - Single non-zero value at lag 1\n",
    "   - Maximum absolute value is 0.5\n",
    "\n",
    "2. **MA(2):**\n",
    "   $$\\rho_1 = \\frac{\\theta_1 + \\theta_1\\theta_2}{1 + \\theta_1^2 + \\theta_2^2}, \\quad \\rho_2 = \\frac{\\theta_2}{1 + \\theta_1^2 + \\theta_2^2}$$\n",
    "   - Two non-zero values at lags 1 and 2\n",
    "   - Pattern depends on signs of θ₁ and θ₂\n",
    "\n",
    "## Partial Autocorrelation Function (PACF)\n",
    "\n",
    "### Key Properties:\n",
    "\n",
    "1. **Infinite Decay:**\n",
    "   - The PACF decays infinitely\n",
    "   - Pattern depends on MA parameters\n",
    "\n",
    "2. **Decay Pattern:**\n",
    "   - For MA(1): dominated by geometric decay with ratio -θ\n",
    "   - For MA(q): mixture of geometric decays\n",
    "\n",
    "3. **Theoretical Values:**\n",
    "   For MA(1):\n",
    "   $$\\phi_{kk} = -\\theta_1(-\\theta_1)^{k-1} \\text{ for } k ≥ 1$$\n",
    "\n",
    "### Special Cases:\n",
    "\n",
    "1. **MA(1):**\n",
    "   - Geometric decay with alternating signs if θ > 0\n",
    "   - Geometric decay without alternation if θ < 0\n",
    "   - Rate of decay depends on |θ|\n",
    "\n",
    "2. **MA(2):**\n",
    "   - More complex pattern\n",
    "   - Still shows infinite decay\n",
    "   - May show damped oscillations\n",
    "\n",
    "## Practical Identification\n",
    "\n",
    "These patterns are crucial for model identification:\n",
    "\n",
    "1. **ACF Properties:**\n",
    "   - Sharp cut-off after lag q\n",
    "   - Values at lags ≤ q indicate parameter values\n",
    "   - Used to determine order of MA process\n",
    "\n",
    "2. **PACF Properties:**\n",
    "   - Infinite extent\n",
    "   - Pattern indicates parameter values\n",
    "   - More complex than AR PACF patterns\n",
    "\n",
    "3. **Combined Use:**\n",
    "   - ACF: determines order q\n",
    "   - PACF: confirms process is MA\n",
    "   - Together: suggest parameter values\n",
    "\n",
    "## Comparison with AR Processes\n",
    "\n",
    "The key distinction is that MA processes show:\n",
    "- Finite ACF (cuts off after lag q)\n",
    "- Infinite PACF (decaying pattern)\n",
    "\n",
    "This is exactly opposite to AR processes which have:\n",
    "- Infinite ACF (decaying pattern)\n",
    "- Finite PACF (cuts off after lag p)\n",
    "\n",
    "## Examples in Visualizations\n",
    "\n",
    "The plots above show:\n",
    "\n",
    "1. **MA(1) with θ = 0.7:**\n",
    "   - ACF: single spike at lag 1\n",
    "   - PACF: geometric decay with alternating signs\n",
    "\n",
    "2. **MA(2) with θ₁ = 0.7, θ₂ = 0.3:**\n",
    "   - ACF: two spikes at lags 1 and 2\n",
    "   - PACF: more complex decay pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 13:\n",
    "\n",
    "Le matrici T , Q, a1|0, P1|0 del ciclo stocastico stazionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Matrices for the Stationary Stochastic Cycle\n",
    "\n",
    "## State Space Form\n",
    "\n",
    "Recall that a stochastic cycle is defined as:\n",
    "\n",
    "$$\\begin{pmatrix} \\psi_t \\\\ \\psi^*_t \\end{pmatrix} = \\rho \\begin{pmatrix} \\cos\\lambda & \\sin\\lambda \\\\ -\\sin\\lambda & \\cos\\lambda \\end{pmatrix} \\begin{pmatrix} \\psi_{t-1} \\\\ \\psi^*_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\kappa_t \\\\ \\kappa^*_t \\end{pmatrix}$$\n",
    "\n",
    "## The Matrices\n",
    "\n",
    "### 1. Transition Matrix T\n",
    "\n",
    "$$T = \\rho \\begin{pmatrix} \\cos\\lambda & \\sin\\lambda \\\\ -\\sin\\lambda & \\cos\\lambda \\end{pmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\rho$ is the damping factor (0 ≤ ρ < 1)\n",
    "- $\\lambda$ is the cycle frequency (0 < λ < π)\n",
    "\n",
    "### 2. Disturbance Covariance Matrix Q\n",
    "\n",
    "$$Q = \\begin{pmatrix} \\sigma^2_\\kappa & 0 \\\\ 0 & \\sigma^2_\\kappa \\end{pmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\sigma^2_\\kappa$ is the variance of the cycle disturbances\n",
    "- The same variance is used for both components\n",
    "- Zero covariance between the disturbances\n",
    "\n",
    "### 3. Initial State Vector a₁|₀\n",
    "\n",
    "$$a_{1|0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n",
    "\n",
    "- Zero initial values are used because the cycle is stationary\n",
    "- The cycle will naturally evolve around zero\n",
    "\n",
    "### 4. Initial State Covariance Matrix P₁|₀\n",
    "\n",
    "For a stationary cycle:\n",
    "\n",
    "$$P_{1|0} = \\frac{\\sigma^2_\\kappa}{1-\\rho^2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "This matrix is derived from:\n",
    "- The stationarity condition\n",
    "- The unconditional variance of the cycle components\n",
    "\n",
    "## Properties and Interpretation\n",
    "\n",
    "1. **Transition Matrix T:**\n",
    "   - Combines rotation ($\\cos\\lambda$, $\\sin\\lambda$) with damping ($\\rho$)\n",
    "   - Complex eigenvalues: $\\rho(\\cos\\lambda \\pm i\\sin\\lambda)$\n",
    "   - Creates cyclical behavior\n",
    "\n",
    "2. **Covariance Matrix Q:**\n",
    "   - Diagonal structure implies uncorrelated innovations\n",
    "   - Equal variances maintain symmetry in the cycle\n",
    "\n",
    "3. **Initial State a₁|₀:**\n",
    "   - Zero mean reflects stationarity\n",
    "   - Cycle oscillates around zero\n",
    "\n",
    "4. **Initial Covariance P₁|₀:**\n",
    "   - Increases as $\\rho$ approaches 1\n",
    "   - Reflects uncertainty in initial conditions\n",
    "   - Related to long-run variance of cycle\n",
    "\n",
    "## Relationships\n",
    "\n",
    "The matrices are interconnected through:\n",
    "\n",
    "1. **Stationarity:**\n",
    "   - $\\rho < 1$ ensures T's eigenvalues are inside unit circle\n",
    "   - P₁|₀ is the stationary variance\n",
    "\n",
    "2. **Cycle Properties:**\n",
    "   - Period = $2\\pi/\\lambda$\n",
    "   - Persistence determined by $\\rho$\n",
    "   - Amplitude controlled by $\\sigma^2_\\kappa$\n",
    "\n",
    "3. **Long-run Behavior:**\n",
    "   - Unconditional mean of zero\n",
    "   - Variance stabilizes at $\\sigma^2_\\kappa/(1-\\rho^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 14:\n",
    "\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- random walk integrato,\n",
    "- stagionalità trimestrale a dummy stocastiche,\n",
    "- rumore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Matrices for UCM with Integrated Random Walk and Seasonal Components\n",
    "\n",
    "## Model Components\n",
    "\n",
    "We have three components:\n",
    "1. Integrated random walk (level + slope)\n",
    "2. Quarterly stochastic dummy seasonality (s=4)\n",
    "3. Observation noise\n",
    "\n",
    "## State Space Form\n",
    "\n",
    "### State Vector\n",
    "$$\\alpha_t = \\begin{pmatrix} \\mu_t \\\\ \\beta_t \\\\ \\gamma_t \\\\ \\gamma_{t-1} \\\\ \\gamma_{t-2} \\end{pmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the level\n",
    "- $\\beta_t$ is the slope\n",
    "- $\\gamma_t, \\gamma_{t-1}, \\gamma_{t-2}$ are seasonal components\n",
    "\n",
    "## The Matrices\n",
    "\n",
    "### 1. Transition Matrix T:\n",
    "$$T = \\begin{pmatrix} \n",
    "1 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & -1 & -1 & -1 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "This matrix:\n",
    "- Top 2×2 block handles the integrated random walk\n",
    "- Bottom 3×3 block manages the seasonal component\n",
    "\n",
    "### 2. Selection Matrix R:\n",
    "$$R = \\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "### 3. Disturbance Covariance Matrix Q:\n",
    "$$Q = \\begin{pmatrix}\n",
    "\\sigma^2_\\zeta & 0 \\\\\n",
    "0 & \\sigma^2_\\omega\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\sigma^2_\\zeta$ is the slope disturbance variance\n",
    "- $\\sigma^2_\\omega$ is the seasonal disturbance variance\n",
    "\n",
    "### 4. Observation Matrix Z:\n",
    "$$Z = \\begin{pmatrix} 1 & 0 & 1 & 0 & 0 \\end{pmatrix}$$\n",
    "\n",
    "### 5. Observation Noise Variance H:\n",
    "$$H = \\sigma^2_\\varepsilon$$\n",
    "\n",
    "## Complete Model\n",
    "\n",
    "The observation equation is:\n",
    "$$y_t = Z\\alpha_t + \\varepsilon_t$$\n",
    "\n",
    "The state equation is:\n",
    "$$\\alpha_t = T\\alpha_{t-1} + R\\eta_t$$\n",
    "\n",
    "where $\\varepsilon_t \\sim N(0, \\sigma^2_\\varepsilon)$ and $\\eta_t \\sim N(0, Q)$\n",
    "\n",
    "## Component Breakdown\n",
    "\n",
    "1. **Integrated Random Walk:**\n",
    "   $$\\mu_t = \\mu_{t-1} + \\beta_{t-1}$$\n",
    "   $$\\beta_t = \\beta_{t-1} + \\zeta_t$$\n",
    "\n",
    "2. **Seasonal Component:**\n",
    "   $$\\gamma_t = -\\gamma_{t-1} - \\gamma_{t-2} - \\gamma_{t-3} + \\omega_t$$\n",
    "   With the constraint: $\\gamma_t + \\gamma_{t-1} + \\gamma_{t-2} + \\gamma_{t-3} = \\omega_t$\n",
    "\n",
    "3. **Complete Model:**\n",
    "   $$y_t = \\mu_t + \\gamma_t + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 15:\n",
    "\n",
    "Come modellereste un improvviso cambio di pendenza in un modello UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling a Sudden Slope Change in Unobserved Components Models\n",
    "\n",
    "A sudden change in slope (also known as a slope break) in a structural time series model can be modeled through intervention analysis. Let's examine how to incorporate this into a UCM framework.\n",
    "\n",
    "## Basic Local Linear Trend Model\n",
    "\n",
    "First, recall the standard local linear trend model without interventions:\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_t &= \\mu_t + \\varepsilon_t \\\\\n",
    "\\mu_t &= \\mu_{t-1} + \\beta_{t-1} + \\eta_t \\\\\n",
    "\\beta_t &= \\beta_{t-1} + \\zeta_t\n",
    "\\end{align*}$$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the level component\n",
    "- $\\beta_t$ is the slope component\n",
    "- $\\varepsilon_t$, $\\eta_t$, and $\\zeta_t$ are white noise disturbances\n",
    "\n",
    "## Modeling a Slope Break\n",
    "\n",
    "To model a sudden change in slope at time $\\tau$, we modify the slope equation by adding an intervention variable:\n",
    "\n",
    "$$\\beta_t = \\beta_{t-1} + \\delta S_\\tau(t) + \\zeta_t$$\n",
    "\n",
    "where:\n",
    "- $\\delta$ is the magnitude of the slope change\n",
    "- $S_\\tau(t)$ is a step dummy variable defined as:\n",
    "\n",
    "$$S_\\tau(t) = \\begin{cases} \n",
    "1 & \\text{if } t = \\tau \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "## State Space Representation\n",
    "\n",
    "The state space form becomes:\n",
    "\n",
    "$$\\begin{pmatrix} \n",
    "\\mu_t \\\\\n",
    "\\beta_t\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\mu_{t-1} \\\\\n",
    "\\beta_{t-1}\n",
    "\\end{pmatrix} +\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "\\delta\n",
    "\\end{pmatrix}\n",
    "S_\\tau(t) +\n",
    "\\begin{pmatrix}\n",
    "\\eta_t \\\\\n",
    "\\zeta_t\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "## Effects of the Intervention\n",
    "\n",
    "The intervention at time $\\tau$ has the following effects:\n",
    "\n",
    "1. **Immediate Effect**: At time $\\tau$, the slope changes by $\\delta$\n",
    "2. **Permanent Effect**: The change in slope persists for all future periods\n",
    "3. **Cumulative Effect**: The level component accumulates the slope change over time\n",
    "\n",
    "The cumulative effect on the level after $k$ periods is:\n",
    "$$\\text{Effect on } \\mu_{\\tau+k} = \\delta k$$\n",
    "\n",
    "## Testing for Slope Breaks\n",
    "\n",
    "To identify potential slope breaks:\n",
    "\n",
    "1. Use auxiliary residuals (particularly the slope residuals)\n",
    "2. Check for significant outliers in the smoothed slope disturbances\n",
    "3. Look for persistent patterns in the one-step-ahead prediction errors\n",
    "\n",
    "## Example in R Code\n",
    "\n",
    "```r\n",
    "# Simulating data with a slope break\n",
    "set.seed(123)\n",
    "n <- 100\n",
    "tau <- 50  # break point\n",
    "delta <- 0.5  # magnitude of slope change\n",
    "\n",
    "# Generate components\n",
    "epsilon <- rnorm(n, 0, 0.1)  # observation noise\n",
    "eta <- rnorm(n, 0, 0.1)      # level disturbance\n",
    "zeta <- rnorm(n, 0, 0.01)    # slope disturbance\n",
    "\n",
    "# Initialize series\n",
    "mu <- beta <- numeric(n)\n",
    "beta[1] <- 0.1  # initial slope\n",
    "\n",
    "# Add slope break\n",
    "for(t in 2:n) {\n",
    "    if(t == tau) beta[t] <- beta[t-1] + delta + zeta[t]\n",
    "    else beta[t] <- beta[t-1] + zeta[t]\n",
    "    \n",
    "    mu[t] <- mu[t-1] + beta[t-1] + eta[t]\n",
    "}\n",
    "\n",
    "y <- mu + epsilon  # observed series\n",
    "```\n",
    "\n",
    "The parameter $\\delta$ can be estimated along with the other model parameters using maximum likelihood estimation. The significance of the slope break can be tested using likelihood ratio tests or by examining the t-statistic of the intervention parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 16:\n",
    "\n",
    "a) Si enunci la condizione di stazionarietà di un processo AR(p) (causale).\n",
    "\n",
    "b) Il processo Yt = 1.5Yt−1 − 0.5Yt−1 + εt, εt ∼ WN è stazionario? Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity Conditions for Autoregressive Processes\n",
    "\n",
    "Consider an AR(p) process defined as:\n",
    "\n",
    "$$Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + ... + \\phi_pY_{t-p} + \\varepsilon_t$$\n",
    "\n",
    "where $\\varepsilon_t$ is white noise with mean zero and variance $\\sigma^2$.\n",
    "\n",
    "### General Stationarity Condition\n",
    "\n",
    "For an AR(p) process to be (weakly) stationary, all roots of its characteristic equation must lie outside the unit circle. The characteristic equation is:\n",
    "\n",
    "$$1 - \\phi_1z - \\phi_2z^2 - ... - \\phi_pz^p = 0$$\n",
    "\n",
    "Equivalently, if we define the lag operator polynomial $\\phi(L) = 1 - \\phi_1L - \\phi_2L^2 - ... - \\phi_pL^p$, then $\\phi(z) = 0$ must have all its roots outside the unit circle.\n",
    "\n",
    "### Analysis of the Given Process\n",
    "\n",
    "Let's analyze the specific process:\n",
    "\n",
    "$$Y_t = 1.5Y_{t-1} - 0.5Y_{t-1} + \\varepsilon_t$$\n",
    "\n",
    "First, we notice there appears to be a typo in the equation as it has $Y_{t-1}$ twice. We'll assume it should be:\n",
    "\n",
    "$$Y_t = 1.5Y_{t-1} - 0.5Y_{t-2} + \\varepsilon_t$$\n",
    "\n",
    "This is an AR(2) process with $\\phi_1 = 1.5$ and $\\phi_2 = -0.5$.\n",
    "\n",
    "The characteristic equation is:\n",
    "\n",
    "$$1 - 1.5z + 0.5z^2 = 0$$\n",
    "\n",
    "To solve this quadratic equation:\n",
    "1. $0.5z^2 - 1.5z + 1 = 0$\n",
    "2. Using the quadratic formula: $z = \\frac{1.5 \\pm \\sqrt{2.25 - 2}}{1} = \\frac{1.5 \\pm \\sqrt{0.25}}{1}$\n",
    "3. $z = \\frac{1.5 \\pm 0.5}{1}$\n",
    "4. Therefore, $z_1 = 2$ and $z_2 = 1$\n",
    "\n",
    "Since one root ($z_2 = 1$) lies on the unit circle, this process is not stationary. This root on the unit circle indicates that the process has a unit root, making it integrated of order 1.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "The presence of a unit root means that:\n",
    "1. The process has an infinite variance\n",
    "2. Shocks to the system have permanent effects\n",
    "3. The process does not return to any long-run mean level\n",
    "4. The process would need to be differenced once to achieve stationarity\n",
    "\n",
    "This can be visually demonstrated with a simulation:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Simulate the process\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "y = np.zeros(n)\n",
    "eps = np.random.normal(0, 1, n)\n",
    "\n",
    "# Generate the AR(2) process\n",
    "for t in range(2, n):\n",
    "    y[t] = 1.5*y[t-1] - 0.5*y[t-2] + eps[t]\n",
    "\n",
    "# The resulting series y would show non-stationary behavior\n",
    "# with trending patterns and no constant variance\n",
    "```\n",
    "\n",
    "The process fails to be stationary because any shock to the system is not gradually forgotten (as would happen in a stationary process) but instead has a permanent effect on the level of the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Given AR Process\n",
    "\n",
    "Let's examine the process exactly as written:\n",
    "\n",
    "$$Y_t = 1.5Y_{t-1} - 0.5Y_{t-1} + \\varepsilon_t$$\n",
    "\n",
    "We can simplify this equation by combining like terms. Notice that we have two terms involving $Y_{t-1}$:\n",
    "\n",
    "$$Y_t = (1.5 - 0.5)Y_{t-1} + \\varepsilon_t = Y_{t-1} + \\varepsilon_t$$\n",
    "\n",
    "This simplification reveals that the process is actually a first-order process, specifically a random walk, as the coefficient of $Y_{t-1}$ equals 1.\n",
    "\n",
    "### Stationarity Analysis\n",
    "\n",
    "To determine if this process is stationary, we write it in standard AR(1) form:\n",
    "\n",
    "$$(1 - L)Y_t = \\varepsilon_t$$\n",
    "\n",
    "The characteristic equation is:\n",
    "\n",
    "$$1 - z = 0$$\n",
    "\n",
    "This equation has a single root:\n",
    "$$z = 1$$\n",
    "\n",
    "Since the root lies exactly on the unit circle, the process is not stationary. This is a classical example of a random walk process, which is integrated of order 1, or I(1).\n",
    "\n",
    "### Implications\n",
    "\n",
    "This process exhibits several important characteristics:\n",
    "1. The variance increases linearly with time\n",
    "2. There is no tendency to return to any mean level\n",
    "3. Each shock $\\varepsilon_t$ has a permanent effect on all future values of the series\n",
    "4. The process needs to be differenced once to achieve stationarity\n",
    "\n",
    "### Alternative Representation\n",
    "\n",
    "We can also write the process as a sum of all past shocks plus an initial condition:\n",
    "\n",
    "$$Y_t = Y_0 + \\sum_{i=1}^t \\varepsilon_i$$\n",
    "\n",
    "This representation makes it clear why the process is not stationary - each new observation adds another random shock to the sum, causing the variance to grow without bound as t increases.\n",
    "\n",
    "The variance at time t is:\n",
    "\n",
    "$$Var(Y_t) = Var(Y_0) + t\\sigma^2_\\varepsilon$$\n",
    "\n",
    "where $\\sigma^2_\\varepsilon$ is the variance of the white noise process. This clearly shows that the variance is not constant but grows linearly with time, violating one of the key requirements for stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 17:\n",
    "\n",
    "Descrivere le funzioni di autocorrelaione e autocorrelazione parziale di un processo MA(q)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Functions of Moving Average Processes\n",
    "\n",
    "Let's explore how autocorrelation and partial autocorrelation functions behave for Moving Average processes of order q. We'll build our understanding step by step, starting with the basic structure.\n",
    "\n",
    "### Structure of MA(q) Process\n",
    "\n",
    "A Moving Average process of order q is defined as:\n",
    "\n",
    "$$Y_t = \\varepsilon_t + \\theta_1\\varepsilon_{t-1} + \\theta_2\\varepsilon_{t-2} + ... + \\theta_q\\varepsilon_{t-q}$$\n",
    "\n",
    "where $\\varepsilon_t$ is white noise with variance $\\sigma^2_\\varepsilon$.\n",
    "\n",
    "### Autocorrelation Function (ACF)\n",
    "\n",
    "For an MA(q) process, the autocorrelation function has a very distinctive pattern. Let's derive it:\n",
    "\n",
    "1. For lag k = 0, we have the variance:\n",
    "   $$\\gamma(0) = \\sigma^2_\\varepsilon(1 + \\theta^2_1 + \\theta^2_2 + ... + \\theta^2_q)$$\n",
    "\n",
    "2. For lag k = 1,2,...,q:\n",
    "   $$\\gamma(k) = \\sigma^2_\\varepsilon(\\theta_k + \\theta_{k+1}\\theta_1 + ... + \\theta_q\\theta_{q-k})$$\n",
    "\n",
    "3. For lag k > q:\n",
    "   $$\\gamma(k) = 0$$\n",
    "\n",
    "Therefore, the autocorrelation function ρ(k) = γ(k)/γ(0) has these key properties:\n",
    "\n",
    "1. It cuts off after lag q (becomes exactly zero)\n",
    "2. The cutoff point identifies the order of the MA process\n",
    "3. The pattern before the cutoff depends on the values of the θ parameters\n",
    "\n",
    "For example, in an MA(1) process $Y_t = \\varepsilon_t + \\theta\\varepsilon_{t-1}$:\n",
    "$$\\rho(1) = \\frac{\\theta}{1 + \\theta^2}, \\quad \\rho(k) = 0 \\text{ for } k > 1$$\n",
    "\n",
    "### Partial Autocorrelation Function (PACF)\n",
    "\n",
    "The partial autocorrelation function of an MA(q) process has very different characteristics:\n",
    "\n",
    "1. It doesn't cut off at any lag\n",
    "2. It decays gradually to zero\n",
    "3. The decay pattern depends on the roots of the MA polynomial\n",
    "\n",
    "For an MA(1) process, the PACF follows an exponential decay pattern:\n",
    "$$\\alpha(k) = \\frac{\\theta^k}{1 + \\theta^2} \\text{ for } k > 0$$\n",
    "\n",
    "### Pattern Recognition for Model Identification\n",
    "\n",
    "These patterns are crucial for model identification:\n",
    "\n",
    "1. ACF:\n",
    "   - Sharp cutoff after lag q\n",
    "   - Non-zero values only up to lag q\n",
    "   - Pattern helps identify MA order\n",
    "\n",
    "2. PACF:\n",
    "   - No sharp cutoff\n",
    "   - Gradual decay toward zero\n",
    "   - May show damped oscillation or exponential decay\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Let's consider an MA(2) process with θ₁ = 0.6 and θ₂ = 0.3:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Generate MA(2) process\n",
    "n = 1000\n",
    "epsilon = np.random.normal(0, 1, n+2)\n",
    "y = epsilon[2:] + 0.6*epsilon[1:-1] + 0.3*epsilon[:-2]\n",
    "\n",
    "# Theoretical ACF values for first few lags\n",
    "acf_theoretical = np.zeros(4)\n",
    "acf_theoretical[0] = 1  # lag 0\n",
    "acf_theoretical[1] = (0.6 + 0.6*0.3)/(1 + 0.6**2 + 0.3**2)  # lag 1\n",
    "acf_theoretical[2] = 0.3/(1 + 0.6**2 + 0.3**2)  # lag 2\n",
    "acf_theoretical[3] = 0  # lag 3\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- ACF will show significant values at lags 1 and 2, then cut off\n",
    "- PACF will show a gradually decreasing pattern \n",
    "- The ratio of consecutive PACF values will approximately equal θ₂/θ₁ after the first few lags\n",
    "\n",
    "Understanding these patterns is crucial for model identification in time series analysis. When examining real data, we look for these characteristic patterns in sample ACF and PACF plots to identify potential MA processes and their orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 18:\n",
    "\n",
    "Che cosa sono e a che cosa servono i residui ausiliari nei modelli UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Auxiliary Residuals in Unobserved Components Models\n",
    "\n",
    "In Unobserved Components Models, auxiliary residuals are standardized smoothed estimates of the disturbances associated with each component of the model. These special residuals serve as powerful diagnostic tools for detecting and analyzing structural breaks, outliers, and other irregularities in the data that might not be visible in the regular one-step-ahead prediction residuals.\n",
    "\n",
    "### Types of Auxiliary Residuals\n",
    "\n",
    "Let's consider a basic structural model with level, slope, and irregular components:\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_t &= \\mu_t + \\varepsilon_t \\\\\n",
    "\\mu_t &= \\mu_{t-1} + \\beta_{t-1} + \\eta_t \\\\\n",
    "\\beta_t &= \\beta_{t-1} + \\zeta_t\n",
    "\\end{align*}$$\n",
    "\n",
    "For this model, we have three types of auxiliary residuals:\n",
    "\n",
    "1. **Irregular Auxiliary Residuals**: $\\tilde{\\varepsilon}_t$ (related to observation equation)\n",
    "2. **Level Auxiliary Residuals**: $\\tilde{\\eta}_t$ (related to level disturbances)\n",
    "3. **Slope Auxiliary Residuals**: $\\tilde{\\zeta}_t$ (related to slope disturbances)\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "The auxiliary residuals are computed using the output of the Kalman smoother. For any disturbance $u_t$, the standardized auxiliary residual is:\n",
    "\n",
    "$$r_t = \\frac{\\tilde{u}_t}{\\sqrt{Var(\\tilde{u}_t)}}$$\n",
    "\n",
    "where $\\tilde{u}_t$ is the smoothed estimate of the disturbance and $Var(\\tilde{u}_t)$ is its smoothed variance.\n",
    "\n",
    "### Uses of Auxiliary Residuals\n",
    "\n",
    "1. **Detecting Outliers**:\n",
    "   The irregular auxiliary residuals help identify outliers in the observation equation. A large value suggests an unusually large measurement error at that time point.\n",
    "\n",
    "2. **Level Breaks**:\n",
    "   Level auxiliary residuals help detect structural breaks in the level component. A significant value indicates a sudden change in the series' level that the model's regular dynamics cannot explain.\n",
    "\n",
    "3. **Slope Changes**:\n",
    "   Slope auxiliary residuals help identify changes in the trend's slope. Large values suggest points where the growth rate of the series changes unexpectedly.\n",
    "\n",
    "### Statistical Properties\n",
    "\n",
    "Under normal conditions:\n",
    "\n",
    "1. Auxiliary residuals are approximately normally distributed\n",
    "2. They have mean zero\n",
    "3. Their variance is less than one\n",
    "4. They are serially correlated, unlike regular standardized innovations\n",
    "\n",
    "### Diagnostic Framework\n",
    "\n",
    "We can construct statistical tests based on auxiliary residuals:\n",
    "\n",
    "1. For outlier detection:\n",
    "   $$H_0: \\text{no outlier at time } t$$\n",
    "   Test statistic: $|r_{\\varepsilon,t}| > c_\\alpha$\n",
    "\n",
    "2. For level breaks:\n",
    "   $$H_0: \\text{no level break at time } t$$\n",
    "   Test statistic: $|r_{\\eta,t}| > c_\\alpha$\n",
    "\n",
    "where $c_\\alpha$ is the critical value from the standard normal distribution, usually 2.5 or 3.\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "1. Values exceeding ±3 strongly suggest structural breaks or outliers\n",
    "2. Patterns in auxiliary residuals can suggest model misspecification\n",
    "3. Multiple significant residuals close together may indicate a more complex structural change\n",
    "4. Different types of auxiliary residuals should be examined together for a complete diagnosis\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "In practice, auxiliary residuals are particularly useful for:\n",
    "\n",
    "1. Model diagnostics and validation\n",
    "2. Identifying points where interventions might be needed\n",
    "3. Understanding the nature of structural changes in the series\n",
    "4. Improving model specification through iterative analysis\n",
    "\n",
    "When significant auxiliary residuals are found, the model can be modified by:\n",
    "\n",
    "1. Adding intervention variables\n",
    "2. Introducing structural breaks\n",
    "3. Modifying the stochastic specifications of components\n",
    "4. Including additional components where needed\n",
    "\n",
    "This diagnostic process helps ensure that the final model adequately captures the key features and structural changes in the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Understanding Auxiliary Residuals\n",
    "\n",
    "Let's consider a simple local level model for a company's monthly sales data:\n",
    "\n",
    "$$\\underbrace{y_t}_{\\text{observed sales}} = \\underbrace{\\mu_t}_{\\text{underlying level}} + \\underbrace{\\varepsilon_t}_{\\text{irregular component}}$$\n",
    "\n",
    "where the level follows:\n",
    "\n",
    "$$\\mu_t = \\underbrace{\\mu_{t-1}}_{\\text{previous level}} + \\underbrace{\\eta_t}_{\\text{level disturbance}}$$\n",
    "\n",
    "In this context, we have two types of auxiliary residuals:\n",
    "\n",
    "1. Irregular auxiliary residuals: $\\tilde{\\varepsilon}_t = \\underbrace{\\frac{y_t - \\mathbb{E}(y_t|Y_n)}{\\sqrt{Var(y_t|Y_n)}}}_{\\text{standardized smoothed irregular}}$\n",
    "\n",
    "2. Level auxiliary residuals: $\\tilde{\\eta}_t = \\underbrace{\\frac{\\mu_t - \\mu_{t-1} - \\mathbb{E}(\\mu_t - \\mu_{t-1}|Y_n)}{\\sqrt{Var(\\mu_t - \\mu_{t-1}|Y_n)}}}_{\\text{standardized smoothed level changes}}$\n",
    "\n",
    "where $Y_n$ represents all observations up to time n.\n",
    "\n",
    "### Interpretation Example\n",
    "\n",
    "Consider a retail store's monthly sales data where:\n",
    "- A computer error caused one month's sales to be recorded at twice the actual value\n",
    "- A competitor closed down, causing a permanent increase in sales level\n",
    "\n",
    "These events would show up in auxiliary residuals as follows:\n",
    "\n",
    "Irregular Auxiliary Residuals ($\\tilde{\\varepsilon}_t$):\n",
    "- Large spike at the computer error month\n",
    "- Normal values elsewhere \n",
    "- Helps identify temporary outliers\n",
    "\n",
    "Level Auxiliary Residuals ($\\tilde{\\eta}_t$):\n",
    "- Large value when competitor closed\n",
    "- Shows when permanent changes occur\n",
    "- Helps identify structural breaks\n",
    "\n",
    "The standardization in auxiliary residuals means we can interpret values above 3 or below -3 as significant anomalies, much like we would with z-scores in standard normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 19:\n",
    "\n",
    "Disegnare una possibile coppia di funzioni di autocorrelaione e autocorrelazione parziale per un processo MA(2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<svg viewBox=\"0 0 800 400\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "    <!-- First plot (ACF) -->\n",
    "    <g transform=\"translate(0,0)\">\n",
    "        <!-- Axes -->\n",
    "        <line x1=\"50\" y1=\"200\" x2=\"350\" y2=\"200\" stroke=\"black\" stroke-width=\"2\"/>\n",
    "        <line x1=\"50\" y1=\"50\" x2=\"50\" y2=\"350\" stroke=\"black\" stroke-width=\"2\"/>\n",
    "        \n",
    "        <!-- Horizontal grid lines for correlations -->\n",
    "        <line x1=\"50\" y1=\"50\" x2=\"350\" y2=\"50\" stroke=\"gray\" stroke-width=\"0.5\" stroke-dasharray=\"5,5\"/>\n",
    "        <line x1=\"50\" y1=\"350\" x2=\"350\" y2=\"350\" stroke=\"gray\" stroke-width=\"0.5\" stroke-dasharray=\"5,5\"/>\n",
    "        \n",
    "        <!-- Correlation bars -->\n",
    "        <rect x=\"45\" y=\"100\" width=\"10\" height=\"200\" fill=\"blue\"/>\n",
    "        <rect x=\"95\" y=\"150\" width=\"10\" height=\"100\" fill=\"blue\"/>\n",
    "        <rect x=\"145\" y=\"180\" width=\"10\" height=\"40\" fill=\"blue\"/>\n",
    "        <rect x=\"195\" y=\"195\" width=\"10\" height=\"10\" fill=\"blue\"/>\n",
    "        <rect x=\"245\" y=\"198\" width=\"10\" height=\"4\" fill=\"blue\"/>\n",
    "        <rect x=\"295\" y=\"199\" width=\"10\" height=\"2\" fill=\"blue\"/>\n",
    "        \n",
    "        <!-- Labels -->\n",
    "        <text x=\"200\" y=\"380\" font-family=\"Arial\" font-size=\"14\">Lag</text>\n",
    "        <text x=\"20\" y=\"200\" font-family=\"Arial\" font-size=\"14\" transform=\"rotate(-90, 20, 200)\">ACF</text>\n",
    "        <text x=\"170\" y=\"30\" font-family=\"Arial\" font-size=\"16\">Autocorrelation Function</text>\n",
    "        \n",
    "        <!-- Confidence bands -->\n",
    "        <line x1=\"50\" y1=\"170\" x2=\"350\" y2=\"170\" stroke=\"red\" stroke-width=\"1\" stroke-dasharray=\"5,5\"/>\n",
    "        <line x1=\"50\" y1=\"230\" x2=\"350\" y2=\"230\" stroke=\"red\" stroke-width=\"1\" stroke-dasharray=\"5,5\"/>\n",
    "    </g>\n",
    "    \n",
    "    <!-- Second plot (PACF) -->\n",
    "    <g transform=\"translate(400,0)\">\n",
    "        <!-- Axes -->\n",
    "        <line x1=\"50\" y1=\"200\" x2=\"350\" y2=\"200\" stroke=\"black\" stroke-width=\"2\"/>\n",
    "        <line x1=\"50\" y1=\"50\" x2=\"50\" y2=\"350\" stroke=\"black\" stroke-width=\"2\"/>\n",
    "        \n",
    "        <!-- Horizontal grid lines -->\n",
    "        <line x1=\"50\" y1=\"50\" x2=\"350\" y2=\"50\" stroke=\"gray\" stroke-width=\"0.5\" stroke-dasharray=\"5,5\"/>\n",
    "        <line x1=\"50\" y1=\"350\" x2=\"350\" y2=\"350\" stroke=\"gray\" stroke-width=\"0.5\" stroke-dasharray=\"5,5\"/>\n",
    "        \n",
    "        <!-- Exponentially decaying oscillating pattern -->\n",
    "        <path d=\"M50,100 C100,280 150,140 200,220 S250,180 300,200\" fill=\"none\" stroke=\"blue\" stroke-width=\"2\"/>\n",
    "        \n",
    "        <!-- Labels -->\n",
    "        <text x=\"200\" y=\"380\" font-family=\"Arial\" font-size=\"14\">Lag</text>\n",
    "        <text x=\"20\" y=\"200\" font-family=\"Arial\" font-size=\"14\" transform=\"rotate(-90, 20, 200)\">PACF</text>\n",
    "        <text x=\"170\" y=\"30\" font-family=\"Arial\" font-size=\"16\">Partial Autocorrelation Function</text>\n",
    "        \n",
    "        <!-- Confidence bands -->\n",
    "        <line x1=\"50\" y1=\"170\" x2=\"350\" y2=\"170\" stroke=\"red\" stroke-width=\"1\" stroke-dasharray=\"5,5\"/>\n",
    "        <line x1=\"50\" y1=\"230\" x2=\"350\" y2=\"230\" stroke=\"red\" stroke-width=\"1\" stroke-dasharray=\"5,5\"/>\n",
    "    </g>\n",
    "</svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Autocorrelation Function (ACF) on the left, notice how it shows the defining characteristic of an MA(2) process - it cuts off after lag 2. This means:\n",
    "\n",
    "At lag 0, we see a correlation of 1 (the tallest blue bar)\n",
    "At lags 1 and 2, we see non-zero correlations (the next two shorter blue bars)\n",
    "After lag 2, all correlations fall within the red confidence bands around zero\n",
    "\n",
    "This pattern emerges because an MA(2) process only has direct dependencies up to two periods in the past. Any correlation at higher lags must be zero by the definition of the process.\n",
    "For the Partial Autocorrelation Function (PACF) on the right, we see a very different pattern. Instead of cutting off, it shows a gradual decay with oscillations. This is typical for MA processes because:\n",
    "\n",
    "The PACF captures the correlation between observations after removing intermediate effects\n",
    "In an MA process, this creates an infinite series of decreasing correlations\n",
    "The oscillating pattern occurs because we're looking at even and odd lags separately\n",
    "\n",
    "The red dashed lines in both plots represent approximate confidence bands at ±2/√n, where n is the sample size. Values within these bands are not significantly different from zero at the 5% level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 20:\n",
    "\n",
    "Le matrici T , Q, a1|0, P1|0 per la stagionalità a sinusoidi stocastiche a con periodo base s = 7 (per esempio per dati giornalieri).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Matrices for Weekly Trigonometric Seasonality\n",
    "\n",
    "Let's examine the state space matrices for a stochastic trigonometric seasonal component with period s = 7, which is particularly useful for daily data showing weekly patterns.\n",
    "\n",
    "### Number of Required Harmonics\n",
    "\n",
    "For a period s = 7, we need ⌊7/2⌋ = 3 harmonics to fully represent the seasonal pattern. The seasonal frequencies are:\n",
    "\n",
    "$$\\lambda_j = \\frac{2\\pi j}{7}, \\quad j = 1,2,3$$\n",
    "\n",
    "### Transition Matrix T\n",
    "\n",
    "The transition matrix T has a block diagonal structure, with each block corresponding to one harmonic:\n",
    "\n",
    "$$T = \\begin{bmatrix} \n",
    "\\cos(\\lambda_1) & \\sin(\\lambda_1) & 0 & 0 & 0 & 0 \\\\\n",
    "-\\sin(\\lambda_1) & \\cos(\\lambda_1) & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\cos(\\lambda_2) & \\sin(\\lambda_2) & 0 & 0 \\\\\n",
    "0 & 0 & -\\sin(\\lambda_2) & \\cos(\\lambda_2) & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\cos(\\lambda_3) & \\sin(\\lambda_3) \\\\\n",
    "0 & 0 & 0 & 0 & -\\sin(\\lambda_3) & \\cos(\\lambda_3)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Substituting the actual values:\n",
    "$$T = \\begin{bmatrix} \n",
    "\\cos(\\frac{2\\pi}{7}) & \\sin(\\frac{2\\pi}{7}) & 0 & 0 & 0 & 0 \\\\\n",
    "-\\sin(\\frac{2\\pi}{7}) & \\cos(\\frac{2\\pi}{7}) & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\cos(\\frac{4\\pi}{7}) & \\sin(\\frac{4\\pi}{7}) & 0 & 0 \\\\\n",
    "0 & 0 & -\\sin(\\frac{4\\pi}{7}) & \\cos(\\frac{4\\pi}{7}) & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\cos(\\frac{6\\pi}{7}) & \\sin(\\frac{6\\pi}{7}) \\\\\n",
    "0 & 0 & 0 & 0 & -\\sin(\\frac{6\\pi}{7}) & \\cos(\\frac{6\\pi}{7})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### State Disturbance Variance Matrix Q\n",
    "\n",
    "Assuming homogeneous variances across harmonics:\n",
    "\n",
    "$$Q = \\sigma^2_\\omega I_6$$\n",
    "\n",
    "where $\\sigma^2_\\omega$ is the variance of the seasonal disturbances and $I_6$ is the 6×6 identity matrix.\n",
    "\n",
    "### Initial State Vector a₁|₀\n",
    "\n",
    "For seasonal components, we typically initialize with zero values:\n",
    "\n",
    "$$a_{1|0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "### Initial State Variance Matrix P₁|₀\n",
    "\n",
    "For a diffuse initialization of the seasonal component:\n",
    "\n",
    "$$P_{1|0} = \\kappa I_6$$\n",
    "\n",
    "where $\\kappa$ is a large number (e.g., 10⁶) indicating high initial uncertainty.\n",
    "\n",
    "### Understanding the Components\n",
    "\n",
    "The matrices work together to create a flexible seasonal pattern:\n",
    "\n",
    "1. Matrix T creates rotating pairs of trigonometric components for each frequency\n",
    "2. Matrix Q allows the seasonal pattern to evolve over time\n",
    "3. The initial conditions (a₁|₀, P₁|₀) express our uncertainty about the initial seasonal pattern\n",
    "\n",
    "Together, these matrices enable the model to:\n",
    "- Capture weekly patterns in daily data\n",
    "- Allow these patterns to evolve gradually over time\n",
    "- Maintain the constraint that seasonal effects sum to zero over a complete cycle\n",
    "\n",
    "The seasonal component at any time t is then given by the sum of the odd-numbered elements of the state vector:\n",
    "\n",
    "$$\\gamma_t = \\sum_{j=1}^3 \\gamma_{j,t}$$\n",
    "\n",
    "where $\\gamma_{j,t}$ represents the first element of each harmonic pair in the state vector.\n",
    "\n",
    "Think of this system as describing a weekly pattern that can gradually change over time, much like how store sales might show different weekly patterns in summer versus winter. The transition matrix T creates the basic weekly cycle by combining three different rotating patterns (harmonics). Each harmonic captures a different aspect of the weekly cycle - the first might capture a simple weekly up-and-down pattern, while the others capture more complex patterns within the week.\n",
    "The Q matrix allows these patterns to evolve slowly over time by adding small random changes at each step. The initial conditions (a₁|₀ and P₁|₀) tell us that we start without assuming any particular pattern (zeros in a₁|₀) but with high uncertainty (large values in P₁|₀), letting the data inform us about the actual seasonal pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 21:\n",
    "\n",
    "Che cosa significa che un processo stocastico è integrato di ordine d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Order in Stochastic Processes\n",
    "\n",
    "When we say a process is integrated of order d, written as I(d), we mean that we need to difference it d times to achieve stationarity. Let me explain this concept step by step, building from fundamental principles to more complex ideas.\n",
    "\n",
    "### Understanding Through a Simple Example\n",
    "\n",
    "Imagine we have a time series $Y_t$ of company sales. A process integrated of order 1 means that while $Y_t$ itself is not stationary, its first difference $\\Delta Y_t = Y_t - Y_{t-1}$ is stationary. In economic terms, this might mean that while the sales levels keep growing without bound, the changes in sales from one period to the next follow a stable pattern.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For a stochastic process $Y_t$, we say it is integrated of order d, or $Y_t \\sim I(d)$, if:\n",
    "\n",
    "1. $\\Delta^d Y_t$ is stationary\n",
    "2. $\\Delta^{d-1} Y_t$ is not stationary\n",
    "\n",
    "where $\\Delta^d$ represents the difference operator applied d times:\n",
    "\n",
    "$$\\Delta^d Y_t = \\underbrace{\\Delta(\\Delta(...\\Delta}_{d \\text{ times}}(Y_t)...))$$\n",
    "\n",
    "### Common Orders of Integration\n",
    "\n",
    "1. $I(0)$: The process is already stationary\n",
    "   - Example: White noise, AR, MA processes\n",
    "   - No differencing needed\n",
    "\n",
    "2. $I(1)$: First-order integrated\n",
    "   - Example: Random walk\n",
    "   - Need one difference to achieve stationarity\n",
    "   - $\\Delta Y_t = Y_t - Y_{t-1}$ is stationary\n",
    "\n",
    "3. $I(2)$: Second-order integrated\n",
    "   - Example: Accelerating trends\n",
    "   - Need two differences\n",
    "   - $\\Delta^2 Y_t = \\Delta(\\Delta Y_t) = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})$ is stationary\n",
    "\n",
    "### Properties of Integrated Processes\n",
    "\n",
    "A process integrated of order d exhibits several key characteristics:\n",
    "\n",
    "1. Memory Persistence:\n",
    "   - Shocks have increasingly permanent effects as d increases\n",
    "   - For I(1): shocks have permanent effects\n",
    "   - For I(2): shocks have permanent and growing effects\n",
    "\n",
    "2. Variance Behavior:\n",
    "   - I(1): variance grows linearly with time\n",
    "   - I(2): variance grows cubically with time\n",
    "\n",
    "3. Mean Behavior:\n",
    "   - I(1): can drift away from any initial value\n",
    "   - I(2): can show accelerating or decelerating trends\n",
    "\n",
    "### Economic Interpretation\n",
    "\n",
    "Integration order has important economic implications:\n",
    "\n",
    "1. I(0) processes:\n",
    "   - Represent stable, mean-reverting behaviors\n",
    "   - Example: Interest rate spreads\n",
    "\n",
    "2. I(1) processes:\n",
    "   - Represent persistent changes\n",
    "   - Example: Stock prices, GDP levels\n",
    "\n",
    "3. I(2) processes:\n",
    "   - Represent accelerating changes\n",
    "   - Example: Some price indices during high inflation\n",
    "\n",
    "### Practical Identification\n",
    "\n",
    "We can identify the order of integration through:\n",
    "\n",
    "1. Visual inspection:\n",
    "   - I(0): fluctuates around constant mean\n",
    "   - I(1): shows wandering behavior\n",
    "   - I(2): shows clearly accelerating patterns\n",
    "\n",
    "2. Unit root tests:\n",
    "   - ADF test\n",
    "   - KPSS test\n",
    "   - Apply sequentially until stationarity is achieved\n",
    "\n",
    "Understanding the order of integration is crucial for:\n",
    "- Choosing appropriate modeling strategies\n",
    "- Avoiding spurious regression\n",
    "- Making valid statistical inferences\n",
    "- Implementing proper differencing in ARIMA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 22:\n",
    "\n",
    "A che cosa servono il filtro di Kalman e lo smoother (che quantità calcolano)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kalman Filter and Smoother: Core Tools for State Space Models\n",
    "\n",
    "The Kalman filter and smoother serve as fundamental tools for inference in state space models, each performing distinct but complementary calculations. Let's understand their purposes and the quantities they compute.\n",
    "\n",
    "### The Kalman Filter\n",
    "\n",
    "The Kalman filter calculates the conditional distribution of the state vector at time t, given all observations up to that point. Specifically, for each time t, it computes:\n",
    "\n",
    "$$a_{t|t} = E(\\alpha_t|y_1,...,y_t)$$\n",
    "$$P_{t|t} = Var(\\alpha_t|y_1,...,y_t)$$\n",
    "\n",
    "where $\\alpha_t$ represents the state vector and $y_t$ the observations. \n",
    "\n",
    "The filter operates recursively through two steps:\n",
    "\n",
    "Prediction Step:\n",
    "$$a_{t|t-1} = T_t a_{t-1|t-1}$$\n",
    "$$P_{t|t-1} = T_t P_{t-1|t-1} T_t' + R_t Q_t R_t'$$\n",
    "\n",
    "Update Step:\n",
    "$$a_{t|t} = a_{t|t-1} + P_{t|t-1}Z_t'F_t^{-1}(y_t - Z_ta_{t|t-1})$$\n",
    "$$P_{t|t} = P_{t|t-1} - P_{t|t-1}Z_t'F_t^{-1}Z_tP_{t|t-1}$$\n",
    "\n",
    "These quantities prove essential for:\n",
    "1. Real-time monitoring of unobserved components\n",
    "2. One-step-ahead predictions\n",
    "3. Model likelihood evaluation\n",
    "\n",
    "### The Kalman Smoother\n",
    "\n",
    "The smoother calculates state estimates using the entire sample of observations:\n",
    "\n",
    "$$a_{t|n} = E(\\alpha_t|y_1,...,y_n)$$\n",
    "$$P_{t|n} = Var(\\alpha_t|y_1,...,y_n)$$\n",
    "\n",
    "where n is the sample size. The smoother runs backwards through the data, starting from the final filtered estimate and computing:\n",
    "\n",
    "$$a_{t|n} = a_{t|t} + P_{t|t}T_{t+1}'P_{t+1|t}^{-1}(a_{t+1|n} - T_{t+1}a_{t|t})$$\n",
    "$$P_{t|n} = P_{t|t} + P_{t|t}T_{t+1}'P_{t+1|t}^{-1}(P_{t+1|n} - P_{t+1|t})P_{t+1|t}^{-1}T_{t+1}P_{t|t}$$\n",
    "\n",
    "The smoother provides:\n",
    "1. Historical component estimates using all available information\n",
    "2. State estimates with minimum mean square error\n",
    "3. Auxiliary residuals for diagnostic checking\n",
    "\n",
    "### Practical Example in Economics\n",
    "\n",
    "Consider tracking a country's underlying economic growth rate. The observed GDP growth contains both the trend and noise. The Kalman filter would give real-time estimates of growth as data arrives, while the smoother would provide revised historical estimates using all available information.\n",
    "\n",
    "The filter might show a growth estimate of 2.5% for 2022Q3 based on data up to that quarter. Later, the smoother might revise this to 2.3% after incorporating subsequent data that revealed a temporary factor had inflated the initial estimate.\n",
    "\n",
    "### Relationship Between Filter and Smoother\n",
    "\n",
    "The relationship between filtered and smoothed estimates can be understood as:\n",
    "\n",
    "1. The filter provides the best estimate of the state \"right now\" using past and current data\n",
    "2. The smoother improves these estimates by incorporating future information\n",
    "3. For any time t, the smoothed estimate has lower variance than the filtered estimate:\n",
    "\n",
    "$$P_{t|n} \\leq P_{t|t}$$\n",
    "\n",
    "This inequality reflects the additional information used by the smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Kalman Filter and Smoother\n",
    "\n",
    "Let's start with the complete state space model setup:\n",
    "\n",
    "$$\\underbrace{y_t}_{\\text{observation}} = \\underbrace{Z_t}_{\\text{measurement matrix}} \\underbrace{\\alpha_t}_{\\text{state vector}} + \\underbrace{\\varepsilon_t}_{\\text{observation noise}}$$\n",
    "\n",
    "$$\\underbrace{\\alpha_t}_{\\text{state vector}} = \\underbrace{T_t}_{\\text{transition matrix}} \\underbrace{\\alpha_{t-1}}_{\\text{previous state}} + \\underbrace{R_t\\eta_t}_{\\text{state noise}}$$\n",
    "\n",
    "### The Kalman Filter Step by Step\n",
    "\n",
    "1. Prediction Equations:\n",
    "\n",
    "$$\\underbrace{a_{t|t-1}}_{\\text{predicted state}} = \\underbrace{T_t}_{\\text{transition}} \\underbrace{a_{t-1|t-1}}_{\\text{previous update}}$$\n",
    "\n",
    "$$\\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} = \\underbrace{T_t P_{t-1|t-1} T_t'}_{\\text{propagated uncertainty}} + \\underbrace{R_t Q_t R_t'}_{\\text{added noise}}$$\n",
    "\n",
    "2. Innovation Calculations:\n",
    "\n",
    "$$\\underbrace{v_t}_{\\text{innovation}} = \\underbrace{y_t}_{\\text{observation}} - \\underbrace{Z_t a_{t|t-1}}_{\\text{predicted measurement}}$$\n",
    "\n",
    "$$\\underbrace{F_t}_{\\text{innovation variance}} = \\underbrace{Z_t P_{t|t-1} Z_t'}_{\\text{projected uncertainty}} + \\underbrace{H_t}_{\\text{measurement noise}}$$\n",
    "\n",
    "3. Kalman Gain (the key formula we were missing):\n",
    "\n",
    "$$\\underbrace{K_t}_{\\text{Kalman gain}} = \\underbrace{P_{t|t-1} Z_t'}_{\\text{predicted cross-covariance}} \\underbrace{F_t^{-1}}_{\\text{innovation precision}}$$\n",
    "\n",
    "4. Update Equations:\n",
    "\n",
    "$$\\underbrace{a_{t|t}}_{\\text{updated state}} = \\underbrace{a_{t|t-1}}_{\\text{prediction}} + \\underbrace{K_t v_t}_{\\text{weighted correction}}$$\n",
    "\n",
    "$$\\underbrace{P_{t|t}}_{\\text{updated covariance}} = \\underbrace{P_{t|t-1}}_{\\text{predicted}} - \\underbrace{K_t Z_t P_{t|t-1}}_{\\text{uncertainty reduction}}$$\n",
    "\n",
    "### The Kalman Smoother Extended\n",
    "\n",
    "The smoother operates backwards through time using these recursions:\n",
    "\n",
    "1. Forward Pass Storage:\n",
    "Store $\\{a_{t|t}, P_{t|t}, a_{t|t-1}, P_{t|t-1}\\}$ for t = 1,...,n\n",
    "\n",
    "2. Backward Recursion (the complete formulas we were missing):\n",
    "\n",
    "$$\\underbrace{r_{t-1}}_{\\text{smoothing factor}} = \\underbrace{Z_t' F_t^{-1} v_t}_{\\text{scaled innovation}} + \\underbrace{L_t' r_t}_{\\text{propagated information}}$$\n",
    "\n",
    "where $\\underbrace{L_t}_{\\text{state propagator}} = T_t - \\underbrace{K_t Z_t}_{\\text{gain effect}}$\n",
    "\n",
    "$$\\underbrace{N_{t-1}}_{\\text{smoothing variance}} = \\underbrace{Z_t' F_t^{-1} Z_t}_{\\text{measurement information}} + \\underbrace{L_t' N_t L_t}_{\\text{propagated precision}}$$\n",
    "\n",
    "3. Smoothed State and Variance:\n",
    "\n",
    "$$\\underbrace{a_{t|n}}_{\\text{smoothed state}} = \\underbrace{a_{t|t-1}}_{\\text{prediction}} + \\underbrace{P_{t|t-1} r_{t-1}}_{\\text{smoothing correction}}$$\n",
    "\n",
    "$$\\underbrace{P_{t|n}}_{\\text{smoothed variance}} = \\underbrace{P_{t|t-1}}_{\\text{predicted}} - \\underbrace{P_{t|t-1} N_{t-1} P_{t|t-1}}_{\\text{precision adjustment}}$$\n",
    "\n",
    "### Interpretation of Kalman Gain\n",
    "\n",
    "The Kalman gain $K_t$ acts as an adaptive weighting mechanism that balances our trust between the model prediction and the new measurement. When we decompose it:\n",
    "\n",
    "$$K_t = P_{t|t-1} Z_t' (Z_t P_{t|t-1} Z_t' + H_t)^{-1}$$\n",
    "\n",
    "We can see that:\n",
    "- If measurement noise ($H_t$) is large, $K_t$ becomes small, giving more weight to predictions\n",
    "- If prediction uncertainty ($P_{t|t-1}$) is large, $K_t$ becomes large, giving more weight to measurements\n",
    "\n",
    "### Smoothing Factor Interpretation\n",
    "\n",
    "The smoothing factor $r_t$ accumulates information from future observations. It combines:\n",
    "- Current measurement information ($Z_t' F_t^{-1} v_t$)\n",
    "- Future information propagated backward ($L_t' r_t$)\n",
    "\n",
    "This allows the smoother to utilize all available information, both past and future, to provide optimal state estimates.\n",
    "\n",
    "Think of the Kalman filter and smoother as two different ways of reconstructing a story. The filter is like a detective working in real-time, making the best possible assessment based on evidence gathered so far. The Kalman gain acts like the detective's judgment of how much to trust new evidence versus existing theories.\n",
    "The smoother, on the other hand, is like a historian who can look at the entire case file, including what happened after each event. The smoothing factor r_t is like the historian's way of incorporating knowledge of future events into understanding past ones.\n",
    "To really understand how they work together, consider estimating GDP growth. The filter might see a sudden rise in GDP and have to decide (via the Kalman gain) how much of this is real growth versus measurement noise. Later, the smoother might look at the same period and, knowing what happened in subsequent quarters, provide a more accurate estimate of what growth really was at that time.\n",
    "Would you like me to elaborate on any particular aspect of these formulas or their interpretation? For instance, we could dive deeper into how the Kalman gain adapts to different types of uncertainty, or how the smoothing factor propagates information backwards through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 23:\n",
    "Che cosa significa “Xt è un processo debolmente stazionario”? Come posso trasformare Xt in modo che diventi un processo integrato di ordine 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Stationarity and First-Order Integration\n",
    "\n",
    "A process $X_t$ is weakly stationary (or covariance stationary) if it satisfies three key properties:\n",
    "\n",
    "1. The expected value is constant over time:\n",
    "   $$E[X_t] = \\mu \\quad \\text{for all } t$$\n",
    "\n",
    "2. The variance is finite and constant over time:\n",
    "   $$Var(X_t) = \\sigma^2 < \\infty \\quad \\text{for all } t$$\n",
    "\n",
    "3. The autocovariance function depends only on the time lag $h$, not on the specific time $t$:\n",
    "   $$Cov(X_t, X_{t+h}) = \\gamma(h) \\quad \\text{for all } t \\text{ and } h$$\n",
    "\n",
    "Weak stationarity is a fundamental concept in time series analysis because it allows us to make meaningful inferences about the statistical properties of a process using observed data.\n",
    "\n",
    "To transform a weakly stationary process $X_t$ into an integrated process of order 1 (denoted as I(1)), we need to accumulate (or integrate) the values of $X_t$ over time. This can be done by defining a new process $Y_t$ as:\n",
    "\n",
    "$$Y_t = Y_{t-1} + X_t$$\n",
    "\n",
    "with some initial value $Y_0$. This is equivalent to:\n",
    "\n",
    "$$Y_t = Y_0 + \\sum_{i=1}^t X_i$$\n",
    "\n",
    "The resulting process $Y_t$ is said to be integrated of order 1 because taking first differences returns the original stationary process:\n",
    "\n",
    "$$\\Delta Y_t = Y_t - Y_{t-1} = X_t$$\n",
    "\n",
    "A classic example of this relationship is between:\n",
    "- Price returns ($X_t$): often approximately stationary\n",
    "- Price levels ($Y_t$): typically integrated of order 1\n",
    "\n",
    "The log of asset prices is usually I(1), while their returns (first differences of logs) are typically stationary. This is why financial analysts often work with returns rather than price levels when conducting statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 24:\n",
    "Disegnare una possibile coppia di funzioni di autocorrelaione/autocorrelazione parziale per un processo MA(3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial Autocorrelation Functions for MA(3)\n",
    "\n",
    "A Moving Average process of order 3, denoted as MA(3), is defined as:\n",
    "\n",
    "$$X_t = \\varepsilon_t + \\theta_1\\varepsilon_{t-1} + \\theta_2\\varepsilon_{t-2} + \\theta_3\\varepsilon_{t-3}$$\n",
    "\n",
    "where $\\varepsilon_t$ is white noise with variance $\\sigma^2$.\n",
    "\n",
    "The theoretical autocorrelation function (ACF) for an MA(3) process has these key characteristics:\n",
    "\n",
    "1. For lag 1: $$\\rho(1) = \\frac{\\theta_1 + \\theta_1\\theta_2 + \\theta_2\\theta_3}{1 + \\theta_1^2 + \\theta_2^2 + \\theta_3^2}$$\n",
    "\n",
    "2. For lag 2: $$\\rho(2) = \\frac{\\theta_2 + \\theta_1\\theta_3}{1 + \\theta_1^2 + \\theta_2^2 + \\theta_3^2}$$\n",
    "\n",
    "3. For lag 3: $$\\rho(3) = \\frac{\\theta_3}{1 + \\theta_1^2 + \\theta_2^2 + \\theta_3^2}$$\n",
    "\n",
    "4. For lags k > 3: $$\\rho(k) = 0$$\n",
    "\n",
    "The partial autocorrelation function (PACF) for an MA(3) process has these characteristics:\n",
    "\n",
    "1. Unlike the ACF, it does not cut off at any lag\n",
    "2. It decays exponentially with a pattern that depends on the roots of the characteristic equation:\n",
    "   $$(1 + \\theta_1z + \\theta_2z^2 + \\theta_3z^3) = 0$$\n",
    "\n",
    "A typical MA(3) process might have parameters $\\theta_1 = 0.7$, $\\theta_2 = 0.5$, $\\theta_3 = 0.3$, which would produce characteristic patterns in both the ACF and PACF.\n",
    "\n",
    "The visualization shows the characteristic patterns of both functions for an MA(3) process:\n",
    "\n",
    "The ACF shows significant spikes only up to lag 3, then drops to zero (within the significance bounds shown by red dashed lines). This is a key identifying feature of MA processes - the ACF \"cuts off\" after lag q, where q is the order of the process.\n",
    "The PACF shows a gradual decay pattern that never quite reaches zero, instead approaching it asymptotically. This decay pattern is typically exponential or a mixture of exponential and sinusoidal patterns, depending on the values of the MA coefficients.\n",
    "\n",
    "These patterns are crucial for model identification in time series analysis. When examining a real time series, if we observe a pattern where the ACF cuts off after lag 3 and the PACF shows gradual decay, this suggests that an MA(3) model might be appropriate for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 25:\n",
    "Le matrici T , Q, a1|0, P1|0 per la stagionalità a sinusoidi stocastiche con periodo base s = 365 composta solo dalle prime due armoniche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Matrices for Annual Sinusoidal Seasonality (First Two Harmonics)\n",
    "\n",
    "For annual data (s=365) using only the first two harmonics, the frequencies are:\n",
    "- λ₁ = 2π/365 (first harmonic)\n",
    "- λ₂ = 4π/365 (second harmonic)\n",
    "\n",
    "### T Matrix (Transition Matrix)\n",
    "The transition matrix T contains two 2×2 blocks for the two harmonics:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix} \n",
    "\\cos(\\frac{2\\pi}{365}) & \\sin(\\frac{2\\pi}{365}) & 0 & 0 \\\\\n",
    "-\\sin(\\frac{2\\pi}{365}) & \\cos(\\frac{2\\pi}{365}) & 0 & 0 \\\\\n",
    "0 & 0 & \\cos(\\frac{4\\pi}{365}) & \\sin(\\frac{4\\pi}{365}) \\\\\n",
    "0 & 0 & -\\sin(\\frac{4\\pi}{365}) & \\cos(\\frac{4\\pi}{365})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Q Matrix (State Disturbance Covariance Matrix)\n",
    "For stochastic seasonality, Q is typically diagonal with the same variance σ²ω:\n",
    "\n",
    "$$\n",
    "Q = \\sigma^2_ω \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### a₁|₀ Vector (Initial State)\n",
    "The initial state vector is typically set to zero:\n",
    "\n",
    "$$\n",
    "a_{1|0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### P₁|₀ Matrix (Initial State Covariance Matrix)\n",
    "Using a diffuse prior (κ → ∞) for the initial state:\n",
    "\n",
    "$$\n",
    "P_{1|0} = \\kappa \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first harmonic captures yearly patterns (period 365 days), while the second harmonic captures semi-yearly patterns (period 182.5 days). By combining just these two harmonics, we can model the main seasonal patterns in annual data while keeping the model relatively parsimonious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 26:\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- random walk integrato,\n",
    "- stagionalità a sinusoidi stocastiche con periodo s = 7 (dati giornalieri),\n",
    "- AR(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Matrices for Combined UCM Model\n",
    "\n",
    "The state vector α_t combines all components:\n",
    "$$\n",
    "\\alpha_t = \\begin{bmatrix} \\mu_t \\\\ \\beta_t \\\\ \\gamma^{(1)}_t \\\\ \\gamma^{(1)*}_t \\\\ \\gamma^{(2)}_t \\\\ \\gamma^{(2)*}_t \\\\ \\gamma^{(3)}_t \\\\ \\gamma^{(3)*}_t \\\\ \\psi_t \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- (μt, βt) are level and slope of integrated random walk\n",
    "- (γ^(j)_t, γ^(j)*_t) are the j-th seasonal harmonic pairs\n",
    "- ψt is the AR(1) component\n",
    "\n",
    "### T Matrix (Transition Matrix)\n",
    "$$\n",
    "T = \\begin{bmatrix} \n",
    "1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\cos(\\lambda_1) & \\sin(\\lambda_1) & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & -\\sin(\\lambda_1) & \\cos(\\lambda_1) & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\cos(\\lambda_2) & \\sin(\\lambda_2) & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & -\\sin(\\lambda_2) & \\cos(\\lambda_2) & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\cos(\\lambda_3) & \\sin(\\lambda_3) & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & -\\sin(\\lambda_3) & \\cos(\\lambda_3) & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\phi\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- λj = 2πj/7 for j = 1,2,3\n",
    "- φ is the AR(1) coefficient\n",
    "\n",
    "### Q Matrix (State Disturbance Covariance Matrix)\n",
    "$$\n",
    "Q = \\begin{bmatrix} \n",
    "\\sigma^2_\\eta & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma^2_\\zeta & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\sigma^2_\\omega & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & \\sigma^2_\\omega & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\sigma^2_\\omega & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\sigma^2_\\omega & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2_\\omega & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2_\\omega & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2_\\xi\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- σ²η is the level disturbance variance\n",
    "- σ²ζ is the slope disturbance variance\n",
    "- σ²ω is the seasonal disturbance variance\n",
    "- σ²ξ is the AR(1) innovation variance\n",
    "\n",
    "### Z Matrix (Observation Matrix)\n",
    "$$\n",
    "Z = \\begin{bmatrix} 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Initial State Vector (a₁|₀)\n",
    "$$\n",
    "a_{1|0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Initial State Covariance Matrix (P₁|₀)\n",
    "$$\n",
    "P_{1|0} = \\begin{bmatrix} \n",
    "\\kappa & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & \\kappa & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\kappa & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & \\kappa & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\kappa & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\kappa & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\kappa & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & \\kappa & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\frac{\\sigma^2_\\xi}{1-\\phi^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- κ is a large number for diffuse initialization of non-stationary components\n",
    "- The AR(1) component is initialized with its unconditional variance σ²ξ/(1-φ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2:\n",
    "## Complete Unobserved Components Model Specification\n",
    "\n",
    "### Model Structure\n",
    "\n",
    "The observed time series yt is modeled as the sum of three components plus observation noise:\n",
    "\n",
    "$$\n",
    "y_t = \\mu_t + \\gamma_t + \\psi_t + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- μt is the integrated random walk (trend)\n",
    "- γt is the stochastic seasonal component with period 7\n",
    "- ψt is the AR(1) component\n",
    "- εt ~ N(0, σ²ε) is the observation noise\n",
    "\n",
    "### Component Specifications\n",
    "\n",
    "1. Integrated Random Walk (Level + Slope):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_t &= \\mu_{t-1} + \\beta_{t-1} + \\eta_t \\\\\n",
    "\\beta_t &= \\beta_{t-1} + \\zeta_t\n",
    "\\end{align*}\n",
    "$$\n",
    "where ηt ~ N(0, σ²η) and ζt ~ N(0, σ²ζ)\n",
    "\n",
    "2. Stochastic Seasonal Component (3 harmonics):\n",
    "$$\n",
    "\\gamma_t = \\sum_{j=1}^3 \\gamma_t^{(j)}\n",
    "$$\n",
    "where each harmonic follows:\n",
    "$$\n",
    "\\begin{bmatrix} \\gamma_t^{(j)} \\\\ \\gamma_t^{(j)*} \\end{bmatrix} = \n",
    "\\begin{bmatrix} \\cos(\\lambda_j) & \\sin(\\lambda_j) \\\\ -\\sin(\\lambda_j) & \\cos(\\lambda_j) \\end{bmatrix}\n",
    "\\begin{bmatrix} \\gamma_{t-1}^{(j)} \\\\ \\gamma_{t-1}^{(j)*} \\end{bmatrix} +\n",
    "\\begin{bmatrix} \\omega_t^{(j)} \\\\ \\omega_t^{(j)*} \\end{bmatrix}\n",
    "$$\n",
    "with λj = 2πj/7 and ωt^(j), ωt^(j)* ~ N(0, σ²ω)\n",
    "\n",
    "3. AR(1) Component:\n",
    "$$\n",
    "\\psi_t = \\phi\\psi_{t-1} + \\xi_t\n",
    "$$\n",
    "where ξt ~ N(0, σ²ξ)\n",
    "\n",
    "### State Space Representation\n",
    "\n",
    "The complete model can be written in state space form:\n",
    "\n",
    "Measurement equation:\n",
    "$$\n",
    "y_t = Z\\alpha_t + \\epsilon_t\n",
    "$$\n",
    "\n",
    "State equation:\n",
    "$$\n",
    "\\alpha_t = T\\alpha_{t-1} + R\\eta_t\n",
    "$$\n",
    "\n",
    "where αt is the state vector containing all components:\n",
    "$$\n",
    "\\alpha_t = \\begin{bmatrix} \n",
    "\\mu_t \\\\ \\beta_t \\\\ \n",
    "\\gamma_t^{(1)} \\\\ \\gamma_t^{(1)*} \\\\\n",
    "\\gamma_t^{(2)} \\\\ \\gamma_t^{(2)*} \\\\\n",
    "\\gamma_t^{(3)} \\\\ \\gamma_t^{(3)*} \\\\\n",
    "\\psi_t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrices T, Q, Z, a₁|₀, and P₁|₀ are as specified in the previous response, with:\n",
    "\n",
    "$$\n",
    "H = \\sigma^2_\\epsilon\n",
    "$$\n",
    "\n",
    "The matrix R is the identity matrix of appropriate dimension, as all state disturbances enter directly into their respective equations.\n",
    "\n",
    "### Model Properties\n",
    "\n",
    "1. The integrated random walk allows for both stochastic level and slope, making the trend component very flexible.\n",
    "\n",
    "2. The three harmonics in the seasonal component can capture complex weekly patterns in daily data, with:\n",
    "   - First harmonic (period 7 days)\n",
    "   - Second harmonic (period 3.5 days)\n",
    "   - Third harmonic (period 2.33 days)\n",
    "\n",
    "3. The AR(1) component captures additional short-term dynamics and autocorrelation not explained by the trend or seasonal components.\n",
    "\n",
    "4. The observation noise εt allows for measurement error and other irregular fluctuations.\n",
    "\n",
    "This formulation provides a flexible framework for modeling daily data with trend, seasonal patterns, and short-term dynamics, while maintaining interpretability of the individual components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 27:\n",
    "Come si può individuare un cambio di livello repentino nei modelli UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 28:\n",
    "Sia Yt un processo debolmente stazionario a media EYt = µ e funzione di autocovarianza Cov(Yt, Yt−k) = γk. Si fornisca la formula del previsore lineare ottimo per prvedere Y4 per mezzo di Y1, Y2, Y3 (non quella generica, ma quella specifica per questo problema con i contenuti delle matrici esplicitati)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 29:\n",
    "Sia Xt un processo debolmente stazionario. Fornire una formula per ottenere il processo Yt integrato di ordine uno e che una volta differenziato diventi Xt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 30:\n",
    "Definire il ciclo stocastico stazionario e spiegare (magari con un disegno) le funzioni di ogni elemento della sua equzione di transizione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 31:\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- local linear trend,\n",
    "- stagionalità a sinusoidi stocastiche con periodo s = 365 ma solo con le prime due armoniche,\n",
    "- AR(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 32:\n",
    "Nell’ambito dei modelli in forma state space, che cosa forniscono gli algoritmi Kalman filter, smoother e one-step-ahead predictor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 33:\n",
    "Sia Yt = Yt−1 + εt, con εt ∼ i.i.d.N(0, σ2) t = 0, 1, 2, . . ., un processo random walk con Y0 = 0 e si supponga di osservare Y1, Y2. Si calcoli la proiezione lineare P[Y3|Y1, Y2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 34:\n",
    "Il processo random walk è stazionario (motivare la risposta)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 35:\n",
    "La forma state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 36:\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- local linear trend,\n",
    "- stagionalità a sinusoidi stocastiche con periodo s = 365 ma solo con le prime tre armoniche,\n",
    "- errore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 37:\n",
    "Che strumento posso utilizzare per identificare cambi di livello in un modello in forma state space che include il local linear trend tra le componenti?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 38:\n",
    "Let Y = −1 + X + X2 + X3, with X ∼ N(0, 1). Compute the optimal linear prediction P[Y |X, X2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 39:\n",
    "Let Xt = 0.9Xt−1 + εt, with εt white noise, be an AR(1) process. Is it stationary? In the case of a positive answer, what are its mean and variance? What kind of ARIMA process is Yt = Yt−1 + Xt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 40:\n",
    "Seasonal components in UCM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 41:\n",
    " Write in state space form the time-varying regression model yt = µt + βtxt + εt, where εt is a white noise and µt and βt are both random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 42:\n",
    "How would you identify additive outliers in a time series modeled with UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kalman Filter: A Theoretical Foundation for Time Series Analysis\n",
    "\n",
    "The Kalman Filter serves as a fundamental tool in time series analysis, allowing us to estimate unobserved components (states) of a system using noisy measurements. Let me guide you through understanding this powerful estimation technique using the state space framework.\n",
    "\n",
    "## State Space Representation\n",
    "\n",
    "In time series analysis, we work with two primary equations that define our system:\n",
    "\n",
    "1. The Observation (or Measurement) Equation:\n",
    "$$\n",
    "\\underbrace{y_t}_{\\text{observation}} = \\underbrace{Z_t}_{\\text{observation matrix}} \\underbrace{\\alpha_t}_{\\text{state vector}} + \\underbrace{d_t}_{\\text{known input}} + \\underbrace{\\varepsilon_t}_{\\text{observation noise}}\n",
    "$$\n",
    "where $\\varepsilon_t \\sim N(0, H_t)$\n",
    "\n",
    "2. The State (or Transition) Equation:\n",
    "$$\n",
    "\\underbrace{\\alpha_t}_{\\text{current state}} = \\underbrace{T_t}_{\\text{transition matrix}} \\underbrace{\\alpha_{t-1}}_{\\text{previous state}} + \\underbrace{c_t}_{\\text{known input}} + \\underbrace{R_t\\eta_t}_{\\text{state noise}}\n",
    "$$\n",
    "where $\\eta_t \\sim N(0, Q_t)$\n",
    "\n",
    "The matrices $Z_t$, $T_t$, and $R_t$ might be time-varying or constant, depending on the specific model. The vectors $d_t$ and $c_t$ represent known inputs or deterministic components.\n",
    "\n",
    "## The Kalman Filter Algorithm\n",
    "\n",
    "The Kalman Filter operates recursively through two main steps:\n",
    "\n",
    "### 1. Prediction Step\n",
    "\n",
    "First, we predict the state vector and its covariance matrix:\n",
    "\n",
    "State Prediction:\n",
    "$$\n",
    "\\underbrace{a_{t|t-1}}_{\\text{predicted state}} = \\underbrace{T_t}_{\\text{transition matrix}} \\underbrace{a_{t-1}}_{\\text{previous estimate}} + \\underbrace{c_t}_{\\text{known input}}\n",
    "$$\n",
    "\n",
    "Covariance Prediction:\n",
    "$$\n",
    "\\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} = \\underbrace{T_t}_{\\text{transition matrix}} \\underbrace{P_{t-1}}_{\\text{previous covariance}} \\underbrace{T_t'}_{\\text{transpose}} + \\underbrace{R_tQ_tR_t'}_{\\text{state noise covariance}}\n",
    "$$\n",
    "\n",
    "### 2. Update Step\n",
    "\n",
    "When new data arrives, we update our predictions:\n",
    "\n",
    "Innovation (Prediction Error):\n",
    "$$\n",
    "\\underbrace{v_t}_{\\text{innovation}} = \\underbrace{y_t}_{\\text{observation}} - \\underbrace{Z_ta_{t|t-1}}_{\\text{predicted observation}} - \\underbrace{d_t}_{\\text{known input}}\n",
    "$$\n",
    "\n",
    "Innovation Variance:\n",
    "$$\n",
    "\\underbrace{F_t}_{\\text{innovation variance}} = \\underbrace{Z_t}_{\\text{observation matrix}} \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} + \\underbrace{H_t}_{\\text{observation noise variance}}\n",
    "$$\n",
    "\n",
    "Kalman Gain:\n",
    "$$\n",
    "\\underbrace{K_t}_{\\text{Kalman gain}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} \\underbrace{F_t^{-1}}_{\\text{inverse innovation variance}}\n",
    "$$\n",
    "\n",
    "State Update:\n",
    "$$\n",
    "\\underbrace{a_t}_{\\text{updated state}} = \\underbrace{a_{t|t-1}}_{\\text{predicted state}} + \\underbrace{K_t}_{\\text{Kalman gain}} \\underbrace{v_t}_{\\text{innovation}}\n",
    "$$\n",
    "\n",
    "Covariance Update:\n",
    "$$\n",
    "\\underbrace{P_t}_{\\text{updated covariance}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} - \\underbrace{K_tF_tK_t'}_{\\text{correction term}}\n",
    "$$\n",
    "\n",
    "## Understanding the Filter's Logic\n",
    "\n",
    "The Kalman Filter achieves optimality through its careful balancing of predictions and observations. The Kalman gain $K_t$ plays a crucial role in this balance:\n",
    "\n",
    "1. When observation noise ($H_t$) is small relative to state uncertainty ($P_{t|t-1}$), the gain gives more weight to the new observation\n",
    "2. When observation noise is large, the gain gives more weight to our prediction\n",
    "\n",
    "The filter's operation can be understood as a Bayesian updating process:\n",
    "- The prediction step represents our prior belief\n",
    "- The observation provides new evidence\n",
    "- The update step combines these to form our posterior belief\n",
    "\n",
    "## Initialization\n",
    "\n",
    "The filter requires initial values:\n",
    "$$\n",
    "a_0 \\text{ and } P_0\n",
    "$$\n",
    "\n",
    "For stationary components, we can use the unconditional mean and variance. For non-stationary components, we often use diffuse initialization (very large initial variance).\n",
    "\n",
    "## Why the Filter is Optimal\n",
    "\n",
    "The Kalman Filter provides optimal estimates under three conditions:\n",
    "1. The system is linear (as shown in our state space equations)\n",
    "2. All noise terms are Gaussian\n",
    "3. The covariance matrices ($H_t$, $Q_t$) are known\n",
    "\n",
    "Under these conditions, the filter minimizes the mean squared error of our state estimates and provides the exact conditional distribution of the state given all past observations:\n",
    "$$\n",
    "\\alpha_t|Y_t \\sim N(a_t, P_t)\n",
    "$$\n",
    "where $Y_t$ represents all observations up to time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Kalman Filter Update Step\n",
    "\n",
    "Let's break down each equation of the Update Step and understand how they work together to refine our state estimates. Think of the Update Step as a careful weighing of new information against our previous beliefs.\n",
    "\n",
    "## 1. The Innovation Equation\n",
    "\n",
    "$$\n",
    "\\underbrace{v_t}_{\\text{innovation}} = \\underbrace{y_t}_{\\text{observation}} - \\underbrace{Z_ta_{t|t-1}}_{\\text{predicted observation}} - \\underbrace{d_t}_{\\text{known input}}\n",
    "$$\n",
    "\n",
    "This equation calculates how \"surprised\" we are by the new measurement. Let's break it down:\n",
    "\n",
    "- $y_t$ is what we actually observe\n",
    "- $Z_ta_{t|t-1}$ is what we expected to observe based on our prediction\n",
    "- $d_t$ accounts for any known external influences\n",
    "- $v_t$ is the difference between reality and expectation\n",
    "\n",
    "Think of it like checking your bank account: if you predicted you'd have $100 ($Z_ta_{t|t-1}$), but you actually have $90 ($y_t$), your innovation ($v_t$) is -$10. This tells you something unexpected happened.\n",
    "\n",
    "## 2. The Innovation Variance\n",
    "\n",
    "$$\n",
    "\\underbrace{F_t}_{\\text{innovation variance}} = \\underbrace{Z_t}_{\\text{observation matrix}} \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} + \\underbrace{H_t}_{\\text{observation noise variance}}\n",
    "$$\n",
    "\n",
    "This equation tells us how much uncertainty there is in our innovation. It combines two sources of uncertainty:\n",
    "\n",
    "1. $Z_tP_{t|t-1}Z_t'$: How uncertain we are about our prediction\n",
    "2. $H_t$: How noisy our measurements are\n",
    "\n",
    "Continuing our bank account analogy: if you're very uncertain about your prediction ($P_{t|t-1}$ is large) and your bank's reporting system sometimes has errors ($H_t$ is large), then $F_t$ will be large, indicating you shouldn't be too alarmed by discrepancies.\n",
    "\n",
    "## 3. The Kalman Gain\n",
    "\n",
    "$$\n",
    "\\underbrace{K_t}_{\\text{Kalman gain}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} \\underbrace{F_t^{-1}}_{\\text{inverse innovation variance}}\n",
    "$$\n",
    "\n",
    "The Kalman gain is perhaps the most crucial equation - it determines how much we should trust our new measurement versus our prediction. It's like a smart weighing scale that considers:\n",
    "\n",
    "- How uncertain we are about our prediction ($P_{t|t-1}$)\n",
    "- How uncertain we are about our measurement ($F_t^{-1}$ includes $H_t$)\n",
    "\n",
    "Properties of the Kalman gain:\n",
    "- If measurement noise ($H_t$) is small, $K_t$ will be larger, giving more weight to new measurements\n",
    "- If prediction uncertainty ($P_{t|t-1}$) is small, $K_t$ will be smaller, giving more weight to our predictions\n",
    "\n",
    "## 4. The State Update\n",
    "\n",
    "$$\n",
    "\\underbrace{a_t}_{\\text{updated state}} = \\underbrace{a_{t|t-1}}_{\\text{predicted state}} + \\underbrace{K_t}_{\\text{Kalman gain}} \\underbrace{v_t}_{\\text{innovation}}\n",
    "$$\n",
    "\n",
    "This is where everything comes together. We take our prediction and correct it based on the new information. The correction is:\n",
    "- Proportional to how wrong we were ($v_t$)\n",
    "- Scaled by how much we trust the new information ($K_t$)\n",
    "\n",
    "In our bank account example: if we predicted $100, saw $90, and our Kalman gain is 0.7, our new estimate would be:\n",
    "$100 + 0.7 \\times (-10) = 93$\n",
    "\n",
    "## 5. The Covariance Update\n",
    "\n",
    "$$\n",
    "\\underbrace{P_t}_{\\text{updated covariance}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} - \\underbrace{K_tF_tK_t'}_{\\text{correction term}}\n",
    "$$\n",
    "\n",
    "This final equation updates our uncertainty about the state. Notice that it always decreases our uncertainty (we subtract the correction term). This makes sense because:\n",
    "- New information, even if noisy, should make us more certain\n",
    "- The more we trust the measurement (larger $K_t$), the more our uncertainty decreases\n",
    "\n",
    "## How They Work Together\n",
    "\n",
    "The five equations form a coherent sequence:\n",
    "1. Calculate how wrong our prediction was ($v_t$)\n",
    "2. Determine how much we trust this error ($F_t$)\n",
    "3. Compute the optimal way to incorporate new information ($K_t$)\n",
    "4. Update our state estimate ($a_t$)\n",
    "5. Update our uncertainty about the state ($P_t$)\n",
    "\n",
    "This sequence ensures that each new observation improves our estimate in a statistically optimal way, carefully balancing our prior knowledge with new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Operations, Initialization, and Smoothing in Kalman Filters\n",
    "\n",
    "## Understanding Matrix Multiplication with Transposes\n",
    "\n",
    "When we see expressions like $Z_tP_{t|t-1}Z_t'$ in the Innovation Variance equation:\n",
    "$$\n",
    "F_t = Z_tP_{t|t-1}Z_t' + H_t\n",
    "$$\n",
    "we're dealing with a fundamental concept in covariance propagation. Let's understand why this happens.\n",
    "\n",
    "### Why We Multiply by Transposes\n",
    "\n",
    "The reason lies in how we transform variance-covariance matrices. When we multiply a random variable by a matrix, its covariance matrix transforms in a specific way. Consider a simple example:\n",
    "\n",
    "If we have a random vector $x$ with covariance matrix $P$, and we transform it by matrix $A$ to get $y = Ax$, then the covariance of $y$ is:\n",
    "$$\n",
    "\\text{Cov}(y) = APA'\n",
    "$$\n",
    "\n",
    "This $APA'$ pattern appears throughout the Kalman Filter because we're constantly transforming random variables and need to keep track of their uncertainties. Let's see what this means in practice:\n",
    "\n",
    "1. In the Innovation Variance equation:\n",
    "   - $P_{t|t-1}$ is our uncertainty about the state\n",
    "   - $Z_t$ transforms the state into measurement space\n",
    "   - $Z_tP_{t|t-1}Z_t'$ gives us the uncertainty of our prediction in measurement space\n",
    "\n",
    "2. The multiplication by transpose ensures:\n",
    "   - The resulting matrix has the correct dimensions\n",
    "   - The covariance matrix remains symmetric (as all covariance matrices must be)\n",
    "   - The variances (diagonal elements) remain positive\n",
    "\n",
    "## Initialization: Starting the Filter Right\n",
    "\n",
    "Initialization is crucial because it provides the starting point for our recursive estimations. We need to set:\n",
    "1. Initial state estimate ($a_0$)\n",
    "2. Initial covariance matrix ($P_0$)\n",
    "\n",
    "### Good Values for Initialization\n",
    "\n",
    "For the initial state $a_0$:\n",
    "1. For stationary components:\n",
    "   - Use the unconditional mean of the process\n",
    "   - For example, for a mean-reverting process, use its long-term mean\n",
    "\n",
    "2. For non-stationary components:\n",
    "   - Use the first few observations to make an educated guess\n",
    "   - For a trend, you might use the first observation as level and first difference as slope\n",
    "\n",
    "For the initial covariance matrix $P_0$:\n",
    "\n",
    "1. For stationary components:\n",
    "   - Use the unconditional variance of the process\n",
    "   - For AR(1) process with parameter $\\phi$ and innovation variance $\\sigma^2$, use $\\sigma^2/(1-\\phi^2)$\n",
    "\n",
    "2. For non-stationary components:\n",
    "   - Use a \"diffuse\" or large variance (e.g., $10^6$ or $10^7$)\n",
    "   - This indicates high uncertainty about initial values\n",
    "   - The filter will quickly converge to reasonable values\n",
    "\n",
    "Example initialization for a local level model:\n",
    "```\n",
    "P_0 = [\n",
    "    1e6    0    # Level uncertainty (diffuse)\n",
    "    0    1e2    # Slope uncertainty (moderately certain)\n",
    "]\n",
    "```\n",
    "\n",
    "## Smoothing: Looking Back for Better Estimates\n",
    "\n",
    "Smoothing is indeed a crucial concept in Kalman Filtering. While the regular Kalman Filter gives us estimates based on data up to time t (filtering), smoothing uses the entire dataset to improve our estimates.\n",
    "\n",
    "### Types of Smoothing\n",
    "\n",
    "1. Fixed-Interval Smoothing:\n",
    "   - Uses all data from t=1 to T\n",
    "   - Gives estimates $a_{t|T}$ for all t\n",
    "   - Most common in time series analysis\n",
    "   \n",
    "2. Fixed-Point Smoothing:\n",
    "   - Updates estimate of state at fixed time k as new data arrives\n",
    "   - Gives series of estimates $a_{k|t}$ for t > k\n",
    "\n",
    "### The Smoothing Equations\n",
    "\n",
    "The smoothing recursions run backwards from T to 1:\n",
    "\n",
    "$$\n",
    "\\underbrace{a_{t|T}}_{\\text{smoothed state}} = \\underbrace{a_t}_{\\text{filtered state}} + \\underbrace{P_t}_{\\text{filtered covariance}} \\underbrace{T_t'}_{\\text{transition}} \\underbrace{P_{t+1|t}^{-1}}_{\\text{inverse prediction}} (\\underbrace{a_{t+1|T}}_{\\text{next smooth}} - \\underbrace{T_ta_t}_{\\text{prediction}})\n",
    "$$\n",
    "\n",
    "This gives us better estimates because:\n",
    "- We use future information not available during filtering\n",
    "- The estimates are typically smoother (less jagged)\n",
    "- The uncertainty of smoothed estimates is smaller than filtered estimates\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "In time series analysis:\n",
    "- Use filtered estimates for real-time applications\n",
    "- Use smoothed estimates for historical analysis\n",
    "- Smoothed estimates are especially useful for:\n",
    "  - Trend estimation\n",
    "  - Seasonal adjustment\n",
    "  - Cycle extraction\n",
    "\n",
    "The improvement from smoothing is most noticeable when:\n",
    "- The signal-to-noise ratio is low\n",
    "- There are missing observations\n",
    "- The state dynamics are strongly persistent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
