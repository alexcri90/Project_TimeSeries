{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Definire i concetti di stazionarietà e integrazione e fornire le condizioni per le quali un processo AR(2) è integrato di ordine 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity and Integration in Time Series\n",
    "\n",
    "## Stationarity\n",
    "\n",
    "A stochastic process $\\{X_t\\}$ is said to be **weakly stationary** (or covariance stationary) if it satisfies the following conditions:\n",
    "\n",
    "1. The expected value is constant and independent of time:\n",
    "   $$E[X_t] = \\mu < \\infty, \\quad \\forall t$$\n",
    "\n",
    "2. The variance is finite and independent of time:\n",
    "   $$Var(X_t) = \\sigma^2 < \\infty, \\quad \\forall t$$\n",
    "\n",
    "3. The autocovariance function depends only on the time lag h and not on time t:\n",
    "   $$Cov(X_t, X_{t+h}) = \\gamma(h), \\quad \\forall t, h$$\n",
    "\n",
    "## Integration\n",
    "\n",
    "A time series is said to be **integrated of order d**, denoted as $I(d)$, if it needs to be differenced d times to become stationary. More formally:\n",
    "\n",
    "- If $Y_t \\sim I(d)$, then $\\Delta^d Y_t$ is stationary\n",
    "- Where $\\Delta$ is the difference operator: $\\Delta Y_t = Y_t - Y_{t-1}$\n",
    "- And $\\Delta^d$ represents applying the difference operator d times\n",
    "\n",
    "## AR(2) Process and Integration\n",
    "\n",
    "An AR(2) process is defined as:\n",
    "\n",
    "$$Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\varepsilon_t$$\n",
    "\n",
    "where $\\varepsilon_t$ is white noise.\n",
    "\n",
    "For an AR(2) process to be integrated of order 1, $I(1)$, it must satisfy two conditions:\n",
    "\n",
    "1. The characteristic equation $1 - \\phi_1z - \\phi_2z^2 = 0$ must have exactly one unit root $(z = 1)$\n",
    "2. The other root must lie outside the unit circle\n",
    "\n",
    "This translates to the following conditions on the parameters:\n",
    "\n",
    "1. $\\phi_1 + \\phi_2 = 1$ (ensures one unit root)\n",
    "2. $|\\phi_2| < 1$ (ensures the other root is outside the unit circle)\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the AR(2) process:\n",
    "$$Y_t = 1.5Y_{t-1} - 0.5Y_{t-2} + \\varepsilon_t$$\n",
    "\n",
    "Here, $\\phi_1 = 1.5$ and $\\phi_2 = -0.5$\n",
    "\n",
    "1. Check if $\\phi_1 + \\phi_2 = 1$:\n",
    "   $1.5 + (-0.5) = 1$ ✓\n",
    "\n",
    "2. Check if $|\\phi_2| < 1$:\n",
    "   $|-0.5| = 0.5 < 1$ ✓\n",
    "\n",
    "Therefore, this AR(2) process is integrated of order 1. This means that while $Y_t$ is non-stationary, its first difference $\\Delta Y_t$ will be stationary.\n",
    "\n",
    "The characteristic equation is:\n",
    "$$1 - 1.5z + 0.5z^2 = 0.5(z - 1)(z - 2) = 0$$\n",
    "\n",
    "As we can see, one root is $z = 1$ (the unit root) and the other is $z = 2$ (outside the unit circle), confirming our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The Unit Root Concept\n",
    "\n",
    "A \"unit root\" is a characteristic of a time series process where a root of the characteristic equation equals 1 (unity). The characteristic equation is obtained by:\n",
    "\n",
    "1. Writing the AR process in lag operator form: $(1 - \\phi_1L - \\phi_2L^2)Y_t = \\varepsilon_t$\n",
    "2. Replacing L with z: $1 - \\phi_1z - \\phi_2z^2 = 0$\n",
    "\n",
    "The \"unit circle\" in the complex plane is the circle with radius 1 centered at the origin. A root lying:\n",
    "- On the unit circle $(|z| = 1)$ → Process is non-stationary\n",
    "- Outside the unit circle $(|z| > 1)$ → Process is stationary\n",
    "- Inside the unit circle $(|z| < 1)$ → Process is explosive\n",
    "\n",
    "## Visual Representation\n",
    "\n",
    "For an AR(2) process, we can visualize the roots in the complex plane:\n",
    "```\n",
    "                    Im\n",
    "                     ↑\n",
    "            Unit Circle → |z| = 1\n",
    "                     |\n",
    "          -1 ←------+-----→ 1   Re\n",
    "                     |\n",
    "                     ↓\n",
    "```\n",
    "\n",
    "## Implications for Different AR Processes\n",
    "\n",
    "### AR(1) Process\n",
    "For an AR(1) process $Y_t = \\phi Y_{t-1} + \\varepsilon_t$:\n",
    "- Characteristic equation: $1 - \\phi z = 0$\n",
    "- Single root: $z = \\frac{1}{\\phi}$\n",
    "- To be I(1): must have exactly $\\phi = 1$\n",
    "\n",
    "### AR(2) Process\n",
    "For an AR(2) process $Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\varepsilon_t$:\n",
    "- Characteristic equation: $1 - \\phi_1z - \\phi_2z^2 = 0$\n",
    "- Two roots: both can be real or complex conjugates\n",
    "- To be I(1): one root must be 1, other outside unit circle\n",
    "\n",
    "### AR(3) Process\n",
    "For an AR(3) process $Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\phi_3Y_{t-3} + \\varepsilon_t$:\n",
    "- Characteristic equation: $1 - \\phi_1z - \\phi_2z^2 - \\phi_3z^3 = 0$\n",
    "- Three roots: can be all real or one real and two complex conjugates\n",
    "- To be I(1): one root must be 1, other two outside unit circle\n",
    "- Parameter conditions: $\\phi_1 + \\phi_2 + \\phi_3 = 1$ and other stability conditions\n",
    "\n",
    "## Behavior Examples\n",
    "\n",
    "1. **All roots outside unit circle** (Stationary):\n",
    "   - Series fluctuates around mean\n",
    "   - Shocks have temporary effects\n",
    "   - Example: AR(1) with $\\phi = 0.5$\n",
    "\n",
    "2. **One unit root** (I(1)):\n",
    "   - Series wanders without fixed mean\n",
    "   - Shocks have permanent effects\n",
    "   - Example: Random Walk $Y_t = Y_{t-1} + \\varepsilon_t$\n",
    "\n",
    "3. **Root inside unit circle** (Explosive):\n",
    "   - Series diverges exponentially\n",
    "   - Shocks have amplifying effects\n",
    "   - Example: AR(1) with $\\phi = 1.2$\n",
    "\n",
    "## Implications for Time Series Analysis\n",
    "\n",
    "1. **Stationarity Testing**:\n",
    "   - Unit root tests (like ADF, KPSS) check for presence of unit roots\n",
    "   - Critical for choosing appropriate modeling strategy\n",
    "\n",
    "2. **Cointegration**:\n",
    "   - When two I(1) series share a common unit root\n",
    "   - Their linear combination might be stationary\n",
    "\n",
    "3. **Forecasting**:\n",
    "   - Unit roots affect forecast uncertainty\n",
    "   - Confidence intervals grow wider for I(1) processes\n",
    "\n",
    "4. **Model Selection**:\n",
    "   - I(1) series need differencing or ARIMA modeling\n",
    "   - Stationary series can use ARMA modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Quale processo della famiglia ARMA ha il seguente correlogramma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying ARMA Processes from Correlograms\n",
    "\n",
    "## Theoretical Patterns in ACF and PACF\n",
    "\n",
    "The identification of ARMA processes relies on the analysis of two key functions:\n",
    "\n",
    "1. **Autocorrelation Function (ACF)** $\\rho(k)$:\n",
    "   $$\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)} = \\frac{Cov(Y_t, Y_{t-k})}{Var(Y_t)}$$\n",
    "\n",
    "2. **Partial Autocorrelation Function (PACF)** $\\alpha(k)$:\n",
    "   Measures correlation between $Y_t$ and $Y_{t-k}$ after removing the linear effects of $Y_{t-1}, ..., Y_{t-k+1}$\n",
    "\n",
    "## Identifying Patterns\n",
    "\n",
    "### 1. AR(p) Processes\n",
    "\n",
    "For an AR(p) process:\n",
    "- ACF: Tails off gradually (exponential decay or damped sinusoidal)\n",
    "- PACF: Cuts off after lag p\n",
    "- Example AR(1): $Y_t = 0.7Y_{t-1} + \\varepsilon_t$\n",
    "  * ACF: $\\rho(k) = 0.7^k$\n",
    "  * PACF: $\\alpha(1) = 0.7$, $\\alpha(k) = 0$ for $k > 1$\n",
    "\n",
    "### 2. MA(q) Processes\n",
    "\n",
    "For an MA(q) process:\n",
    "- ACF: Cuts off after lag q\n",
    "- PACF: Tails off gradually\n",
    "- Example MA(1): $Y_t = \\varepsilon_t + 0.7\\varepsilon_{t-1}$\n",
    "  * ACF: $\\rho(1) = \\frac{0.7}{1+0.7^2}$, $\\rho(k) = 0$ for $k > 1$\n",
    "  * PACF: Decays exponentially\n",
    "\n",
    "### 3. ARMA(p,q) Processes\n",
    "\n",
    "For an ARMA(p,q) process:\n",
    "- ACF: Tails off after lag q\n",
    "- PACF: Tails off after lag p\n",
    "- More complex patterns that combine AR and MA characteristics\n",
    "\n",
    "## Common Correlogram Patterns\n",
    "\n",
    "1. **White Noise**\n",
    "   - ACF: All zero except at lag 0\n",
    "   - PACF: All zero except at lag 0\n",
    "\n",
    "2. **AR(1)**\n",
    "   - ACF: Exponential decay\n",
    "   - PACF: Single spike at lag 1\n",
    "\n",
    "3. **AR(2)**\n",
    "   - ACF: Damped exponential or sinusoidal decay\n",
    "   - PACF: Two spikes, zero afterward\n",
    "\n",
    "4. **MA(1)**\n",
    "   - ACF: Single spike at lag 1\n",
    "   - PACF: Exponential decay\n",
    "\n",
    "5. **MA(2)**\n",
    "   - ACF: Two spikes, zero afterward\n",
    "   - PACF: Damped exponential decay\n",
    "\n",
    "## Identification Steps\n",
    "\n",
    "1. **Examine ACF**:\n",
    "   - If cuts off: Suggests MA component\n",
    "   - If decays: Suggests AR component\n",
    "   - Count significant lags before cutoff\n",
    "\n",
    "2. **Examine PACF**:\n",
    "   - If cuts off: Suggests AR component\n",
    "   - If decays: Suggests MA component\n",
    "   - Count significant lags before cutoff\n",
    "\n",
    "3. **Combine Information**:\n",
    "   - If both tail off: ARMA process\n",
    "   - If one cuts off: Pure AR or MA\n",
    "   - Note the lags where patterns change\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "1. **Sample Size Effects**:\n",
    "   - Larger samples give clearer patterns\n",
    "   - Use confidence bands (typically ±2/√n)\n",
    "\n",
    "2. **Stationarity**:\n",
    "   - Patterns only valid for stationary series\n",
    "   - May need differencing first\n",
    "\n",
    "3. **Seasonality**:\n",
    "   - Look for spikes at seasonal lags\n",
    "   - May need seasonal differencing\n",
    "\n",
    "4. **Model Validation**:\n",
    "   - Check residual correlograms\n",
    "   - Should resemble white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Le matrici T e Q dei due tipi di stagionalità (ogni sette giorni)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T and Q Matrices for Weekly Seasonality (s=7)\n",
    "\n",
    "## 1. Dummy Variables Seasonality\n",
    "\n",
    "For weekly seasonality using dummy variables, we need 6 state variables (the 7th is determined by the constraint that they sum to zero).\n",
    "\n",
    "### Transition Matrix T\n",
    "For s = 7, the transition matrix T is a 6×6 matrix:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix} \n",
    "-1 & -1 & -1 & -1 & -1 & -1 \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Disturbance Variance Matrix Q\n",
    "The Q matrix is a 6×6 matrix with only one non-zero element:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "\\sigma_{\\omega}^2 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## 2. Trigonometric Seasonality\n",
    "\n",
    "For weekly seasonality using trigonometric form, we need 3 harmonics (since ⌊7/2⌋ = 3).\n",
    "\n",
    "### Transition Matrix T\n",
    "For s = 7, the transition matrix T is a 6×6 block diagonal matrix:\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\lambda_1) & \\sin(\\lambda_1) \\\\\n",
    "-\\sin(\\lambda_1) & \\cos(\\lambda_1)\n",
    "\\end{bmatrix} & 0 & 0 \\\\\n",
    "0 & \\begin{bmatrix}\n",
    "\\cos(\\lambda_2) & \\sin(\\lambda_2) \\\\\n",
    "-\\sin(\\lambda_2) & \\cos(\\lambda_2)\n",
    "\\end{bmatrix} & 0 \\\\\n",
    "0 & 0 & \\begin{bmatrix}\n",
    "\\cos(\\lambda_3) & \\sin(\\lambda_3) \\\\\n",
    "-\\sin(\\lambda_3) & \\cos(\\lambda_3)\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\lambda_j = \\frac{2\\pi j}{7}$ for j = 1, 2, 3\n",
    "\n",
    "### Disturbance Variance Matrix Q\n",
    "The Q matrix is a 6×6 diagonal matrix:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "\\sigma_{\\omega}^2 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma_{\\omega}^2 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\sigma_{\\omega}^2 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & \\sigma_{\\omega}^2 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\sigma_{\\omega}^2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\sigma_{\\omega}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Properties and Interpretation\n",
    "\n",
    "### Dummy Variables Form:\n",
    "- First row of T matrix ensures sum-to-zero constraint\n",
    "- Subsequent rows shift the seasonal effects\n",
    "- Single variance parameter in Q controls evolution\n",
    "- State vector directly represents seasonal effects\n",
    "\n",
    "### Trigonometric Form:\n",
    "- Block diagonal structure in T represents harmonics\n",
    "- Each 2×2 block is a rotation matrix\n",
    "- Equal variances in Q for all components\n",
    "- State vector represents amplitudes of harmonics\n",
    "\n",
    "## Key Differences:\n",
    "1. **Size**: Both are 6×6 but structured differently\n",
    "2. **Evolution**: \n",
    "   - Dummy: Direct shifts with one shock\n",
    "   - Trigonometric: Smooth rotation with multiple shocks\n",
    "3. **Interpretation**:\n",
    "   - Dummy: Direct seasonal effects\n",
    "   - Trigonometric: Frequency components\n",
    "4. **Smoothness**:\n",
    "   - Dummy: Can have sharp changes\n",
    "   - Trigonometric: Naturally smoother transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:\n",
    "\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "\n",
    "- random walk,\n",
    "- ciclo stocastico,\n",
    "- regressione su xt,\n",
    "- rumore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Space Matrices for UCM Model\n",
    "\n",
    "## Model Components\n",
    "\n",
    "The model contains:\n",
    "1. Random walk (level component)\n",
    "2. Stochastic cycle\n",
    "3. Regression on $x_t$\n",
    "4. Observation noise\n",
    "\n",
    "The model can be written as:\n",
    "\n",
    "$$y_t = \\mu_t + \\psi_t + \\beta x_t + \\varepsilon_t$$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the random walk\n",
    "- $\\psi_t$ is the stochastic cycle\n",
    "- $\\beta x_t$ is the regression term\n",
    "- $\\varepsilon_t$ is the observation noise\n",
    "\n",
    "## State Space Representation\n",
    "\n",
    "### State Vector\n",
    "The state vector $\\alpha_t$ contains:\n",
    "- Random walk level ($\\mu_t$)\n",
    "- Cycle ($\\psi_t$) and auxiliary cycle component ($\\psi_t^*$)\n",
    "- Regression coefficient ($\\beta_t$)\n",
    "\n",
    "$$\\alpha_t = \\begin{bmatrix} \n",
    "\\mu_t \\\\\n",
    "\\psi_t \\\\\n",
    "\\psi_t^* \\\\\n",
    "\\beta_t\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Transition Matrix T\n",
    "$$T = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & \\rho\\cos(\\lambda) & \\rho\\sin(\\lambda) & 0 \\\\\n",
    "0 & -\\rho\\sin(\\lambda) & \\rho\\cos(\\lambda) & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\rho$ is the damping factor of the cycle $(0 < \\rho < 1)$\n",
    "- $\\lambda$ is the cycle frequency $(0 < \\lambda < \\pi)$\n",
    "\n",
    "### Observation Matrix Z\n",
    "$$Z = \\begin{bmatrix} 1 & 1 & 0 & x_t \\end{bmatrix}$$\n",
    "\n",
    "Note that $x_t$ enters in the observation matrix as it multiplies $\\beta_t$\n",
    "\n",
    "### System Disturbance Matrix R\n",
    "$$R = I_4$$ \n",
    "(4×4 identity matrix)\n",
    "\n",
    "### System Disturbance Covariance Matrix Q\n",
    "$$Q = \\begin{bmatrix}\n",
    "\\sigma_\\eta^2 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma_\\kappa^2 & 0 & 0 \\\\\n",
    "0 & 0 & \\sigma_\\kappa^2 & 0 \\\\\n",
    "0 & 0 & 0 & \\sigma_\\beta^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\sigma_\\eta^2$ is the variance of random walk innovations\n",
    "- $\\sigma_\\kappa^2$ is the variance of cycle disturbances\n",
    "- $\\sigma_\\beta^2$ is the variance of regression coefficient innovations\n",
    "\n",
    "### Observation Disturbance Variance H\n",
    "$$H = \\sigma_\\varepsilon^2$$\n",
    "\n",
    "## Properties and Interpretation\n",
    "\n",
    "1. **Random Walk Component**:\n",
    "   - Single state element ($\\mu_t$)\n",
    "   - Unit coefficient in T matrix\n",
    "   - Innovation variance $\\sigma_\\eta^2$\n",
    "\n",
    "2. **Cycle Component**:\n",
    "   - Two state elements ($\\psi_t, \\psi_t^*$)\n",
    "   - 2×2 rotation matrix in T\n",
    "   - Equal variances $\\sigma_\\kappa^2$ for both components\n",
    "\n",
    "3. **Regression Component**:\n",
    "   - Time-varying coefficient $\\beta_t$\n",
    "   - Random walk evolution\n",
    "   - Innovation variance $\\sigma_\\beta^2$\n",
    "\n",
    "4. **Complete System**:\n",
    "   - State dimension: 4\n",
    "   - $x_t$ enters via Z matrix\n",
    "   - All disturbances are uncorrelated\n",
    "\n",
    "## State Evolution Equations\n",
    "\n",
    "1. Random walk:\n",
    "   $$\\mu_t = \\mu_{t-1} + \\eta_t$$\n",
    "\n",
    "2. Stochastic cycle:\n",
    "   $$\\begin{bmatrix} \\psi_t \\\\ \\psi_t^* \\end{bmatrix} = \\rho\\begin{bmatrix} \\cos(\\lambda) & \\sin(\\lambda) \\\\ -\\sin(\\lambda) & \\cos(\\lambda) \\end{bmatrix} \\begin{bmatrix} \\psi_{t-1} \\\\ \\psi_{t-1}^* \\end{bmatrix} + \\begin{bmatrix} \\kappa_t \\\\ \\kappa_t^* \\end{bmatrix}$$\n",
    "\n",
    "3. Regression coefficient:\n",
    "   $$\\beta_t = \\beta_{t-1} + \\zeta_t$$\n",
    "\n",
    "## Observation Equation\n",
    "$$y_t = \\begin{bmatrix} 1 & 1 & 0 & x_t \\end{bmatrix} \\begin{bmatrix} \\mu_t \\\\ \\psi_t \\\\ \\psi_t^* \\\\ \\beta_t \\end{bmatrix} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Stochastic Cycle\n",
    "\n",
    "## 1. Single Form vs Seasonality\n",
    "\n",
    "Unlike seasonality, the stochastic cycle comes in only one form. This is because the cycle is inherently defined using trigonometric functions (sine and cosine). The reason is fundamental:\n",
    "\n",
    "- **Seasonality** models a pattern that repeats at fixed, known intervals (like days of the week). This can be done either by directly modeling each period's effect (dummy approach) or by using trigonometric functions.\n",
    "\n",
    "- **Cycle** models a smooth, wave-like pattern where the period itself might vary over time. It can only be effectively modeled using trigonometric functions.\n",
    "\n",
    "## 2. The Role of ψ and ψ*\n",
    "\n",
    "The stochastic cycle uses two components (ψ_t and ψ*_t) to create a flexible cyclical pattern. Here's why:\n",
    "\n",
    "### Basic Cycle Evolution\n",
    "```\n",
    "[ψ_t   ]  =  ρ[cos(λ)  sin(λ) ] [ψ_{t-1}  ]  +  [κ_t  ]\n",
    "[ψ*_t  ]     [-sin(λ)  cos(λ) ] [ψ*_{t-1} ]     [κ*_t ]\n",
    "```\n",
    "\n",
    "where:\n",
    "- λ is the frequency (determines cycle length)\n",
    "- ρ is the damping factor (0 < ρ ≤ 1)\n",
    "- κ_t and κ*_t are independent disturbances\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "1. **ψ_t (Primary Component)**:\n",
    "   - This is the actual cycle component that enters the observation equation\n",
    "   - Represents the current position in the cycle\n",
    "\n",
    "2. **ψ*_t (Auxiliary Component)**:\n",
    "   - Doesn't enter the observation equation directly\n",
    "   - Helps create the circular motion of the cycle\n",
    "   - Acts like a \"memory\" of where the cycle is heading\n",
    "\n",
    "Together, they create a flexible rotating movement in a 2-dimensional space where:\n",
    "- ψ_t represents the x-coordinate\n",
    "- ψ*_t represents the y-coordinate\n",
    "\n",
    "## Why Two Components are Necessary\n",
    "\n",
    "The two components are needed because:\n",
    "\n",
    "1. **Single Dimension Limitation**:\n",
    "   - With just one component, you could only move back and forth along a line\n",
    "   - You couldn't capture the smooth, circular nature of cycles\n",
    "\n",
    "2. **Phase Information**:\n",
    "   - ψ*_t stores information about the phase of the cycle\n",
    "   - Helps determine whether the cycle is increasing or decreasing\n",
    "\n",
    "3. **Smooth Transitions**:\n",
    "   - The interaction between ψ_t and ψ*_t creates smooth transitions\n",
    "   - Prevents sudden jumps that would occur with a single component\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "1. **Frequency (λ)**:\n",
    "   - Controls how fast the cycle completes one rotation\n",
    "   - Period = 2π/λ\n",
    "   - Fixed parameter (estimated from data)\n",
    "\n",
    "2. **Damping Factor (ρ)**:\n",
    "   - Controls how quickly the cycle dies out\n",
    "   - ρ = 1: persistent cycle\n",
    "   - ρ < 1: dying cycle\n",
    "   - Also fixed parameter\n",
    "\n",
    "3. **Disturbances (κ_t, κ*_t)**:\n",
    "   - Allow the cycle to evolve stochastically\n",
    "   - Make each cycle different from the last\n",
    "   - Usually assumed to have equal variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5:\n",
    "Come si costruisce la funzione di verosimiglianza di un modello Gaussiano in forma state-space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Function Construction for Gaussian State Space Models\n",
    "\n",
    "## 1. State Space Model Structure\n",
    "\n",
    "Consider a state space model in its general form:\n",
    "\n",
    "**Observation equation:**\n",
    "$$y_t = Z_t\\alpha_t + d_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, H_t)$$\n",
    "\n",
    "**State equation:**\n",
    "$$\\alpha_t = T_t\\alpha_{t-1} + c_t + R_t\\eta_t, \\quad \\eta_t \\sim N(0, Q_t)$$\n",
    "\n",
    "## 2. Likelihood Function Components\n",
    "\n",
    "The log-likelihood function is built from the prediction errors (innovations):\n",
    "\n",
    "$$\\ell(\\theta) = -\\frac{1}{2}\\sum_{t=1}^n \\left[ k\\log(2\\pi) + \\log|F_t| + v_t'F_t^{-1}v_t \\right]$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ is the vector of parameters to be estimated\n",
    "- $k$ is the dimension of the observation vector $y_t$\n",
    "- $v_t$ is the innovation vector\n",
    "- $F_t$ is the variance matrix of the innovations\n",
    "- $n$ is the sample size\n",
    "\n",
    "## 3. Construction Steps\n",
    "\n",
    "### Step 1: Initialize\n",
    "- Set initial state: $a_0 = E(\\alpha_0)$\n",
    "- Set initial variance: $P_0 = Var(\\alpha_0)$\n",
    "\n",
    "### Step 2: Kalman Filter Recursions\n",
    "For t = 1 to n:\n",
    "\n",
    "1. **Prediction step:**\n",
    "   $$a_{t|t-1} = T_ta_{t-1} + c_t$$\n",
    "   $$P_{t|t-1} = T_tP_{t-1}T_t' + R_tQ_tR_t'$$\n",
    "\n",
    "2. **Innovation calculations:**\n",
    "   $$v_t = y_t - Z_ta_{t|t-1} - d_t$$\n",
    "   $$F_t = Z_tP_{t|t-1}Z_t' + H_t$$\n",
    "\n",
    "3. **Update step:**\n",
    "   $$a_t = a_{t|t-1} + P_{t|t-1}Z_t'F_t^{-1}v_t$$\n",
    "   $$P_t = P_{t|t-1} - P_{t|t-1}Z_t'F_t^{-1}Z_tP_{t|t-1}$$\n",
    "\n",
    "### Step 3: Accumulate Log-Likelihood\n",
    "For each t, add to the log-likelihood:\n",
    "$$\\ell_t = -\\frac{1}{2}[k\\log(2\\pi) + \\log|F_t| + v_t'F_t^{-1}v_t]$$\n",
    "\n",
    "## 4. Practical Implementation\n",
    "\n",
    "1. **Initialization Approaches:**\n",
    "   - For stationary components: use unconditional distribution\n",
    "   - For non-stationary components: use diffuse initialization\n",
    "\n",
    "2. **Numerical Considerations:**\n",
    "   - Use log-sum to prevent numerical overflow\n",
    "   - Handle missing values by skipping their contribution\n",
    "   - Check for positive definiteness of $F_t$\n",
    "\n",
    "3. **Parameter Constraints:**\n",
    "   - Ensure variance matrices remain positive definite\n",
    "   - Maintain stationarity conditions where required\n",
    "   - Handle boundary conditions appropriately\n",
    "\n",
    "## 5. Special Cases\n",
    "\n",
    "### Diffuse Initialization\n",
    "When some states have infinite variance:\n",
    "1. Skip likelihood contribution for first d observations\n",
    "2. Use modified likelihood for subsequent observations\n",
    "\n",
    "### Missing Observations\n",
    "When $y_t$ is partially missing:\n",
    "1. Remove missing elements from observation equation\n",
    "2. Adjust dimensions of $Z_t$ and $H_t$ accordingly\n",
    "\n",
    "### Time-Invariant Systems\n",
    "When matrices are constant:\n",
    "1. Simplified storage requirements\n",
    "2. Potential for computational optimizations\n",
    "\n",
    "## 6. Maximum Likelihood Estimation\n",
    "\n",
    "The likelihood function is maximized numerically:\n",
    "\n",
    "1. **Optimization Methods:**\n",
    "   - Quasi-Newton methods (BFGS)\n",
    "   - Simplex algorithm (Nelder-Mead)\n",
    "   - Grid search for initial values\n",
    "\n",
    "2. **Parameter Transformations:**\n",
    "   - Log transform for variances\n",
    "   - Logit transform for correlations\n",
    "   - Ensure parameter constraints\n",
    "\n",
    "3. **Standard Errors:**\n",
    "   Obtained from numerical second derivatives:\n",
    "   $$Var(\\hat{\\theta}) \\approx \\left[-\\frac{\\partial^2\\ell(\\theta)}{\\partial\\theta\\partial\\theta'}\\right]^{-1}_{\\theta=\\hat{\\theta}}$$\n",
    "\n",
    "## 7. Diagnostic Checks\n",
    "\n",
    "After maximizing the likelihood:\n",
    "\n",
    "1. Check standardized innovations for:\n",
    "   - Serial correlation\n",
    "   - Normality\n",
    "   - Homoscedasticity\n",
    "\n",
    "2. Check parameter significance using:\n",
    "   - t-statistics\n",
    "   - Likelihood ratio tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Likelihood in State Space Models: A Simple Guide\n",
    "\n",
    "## The Basic Idea\n",
    "\n",
    "Imagine you're tracking the position of a moving object, but you can only see it through a foggy window (noisy observations). You want to:\n",
    "1. Know where the object really is (state estimation)\n",
    "2. Know how good your tracking system is (likelihood)\n",
    "\n",
    "## What is the Kalman Filter?\n",
    "\n",
    "The Kalman filter is like a smart prediction system that:\n",
    "1. Makes a guess about where the object will be (prediction)\n",
    "2. Looks at the actual observation\n",
    "3. Updates its guess based on how wrong it was (updating)\n",
    "4. Learns how much to trust its predictions vs observations\n",
    "\n",
    "Think of it like GPS navigation:\n",
    "- Your phone predicts where you'll be based on your speed and direction\n",
    "- It gets actual GPS readings\n",
    "- It combines both pieces of information to give you your best estimated position\n",
    "\n",
    "## How Does Likelihood Come Into Play?\n",
    "\n",
    "The likelihood tells us \"how likely\" our model is to produce the data we see. It's built by:\n",
    "\n",
    "1. **Making Predictions**\n",
    "   - Using our model to predict the next observation\n",
    "   - Like guessing where a ball will land based on its trajectory\n",
    "\n",
    "2. **Comparing to Reality**\n",
    "   - Seeing how far off our predictions were\n",
    "   - The smaller the errors, the better our model\n",
    "\n",
    "3. **Building the Score (Likelihood)**\n",
    "   - Good predictions (small errors) → Higher likelihood\n",
    "   - Bad predictions (large errors) → Lower likelihood\n",
    "\n",
    "## Simple Example\n",
    "\n",
    "Let's say we're tracking temperature:\n",
    "\n",
    "1. **State Space Model Components:**\n",
    "   - True temperature (state we can't directly observe)\n",
    "   - Thermometer reading (noisy observation)\n",
    "   - How temperature typically evolves\n",
    "   - How noisy our thermometer is\n",
    "\n",
    "2. **For Each New Reading:**\n",
    "   - Predict temperature based on previous information\n",
    "   - Take new thermometer reading\n",
    "   - Compare prediction to reading\n",
    "   - Update our understanding\n",
    "   - Add to our likelihood score\n",
    "\n",
    "3. **Final Likelihood:**\n",
    "   - Combines all these prediction errors\n",
    "   - Tells us how well our model fits the data\n",
    "   - Helps us choose the best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Kalman Filter Estimation\n",
    "\n",
    "## Basic Concept\n",
    "\n",
    "The Kalman filter is like a \"smart averaging\" system that combines:\n",
    "1. What we expect based on our model\n",
    "2. What we actually observe\n",
    "3. How much we trust each piece of information\n",
    "\n",
    "## Simple Example: Tracking a Car's Position\n",
    "\n",
    "Imagine tracking a car's position with GPS. At each moment:\n",
    "\n",
    "### 1. Prediction Step\n",
    "We predict where the car should be based on:\n",
    "- Last known position\n",
    "- Speed\n",
    "- Direction\n",
    "\n",
    "$$\\underbrace{\\hat{x}_{t|t-1}}_{\\text{prediction}} = \\underbrace{\\hat{x}_{t-1}}_{\\text{last position}} + \\underbrace{v\\Delta t}_{\\text{speed × time}}$$\n",
    "\n",
    "### 2. Measurement Step\n",
    "We get a GPS reading (with some error):\n",
    "\n",
    "$$\\underbrace{z_t}_{\\text{GPS reading}} = \\underbrace{x_t}_{\\text{true position}} + \\underbrace{\\varepsilon_t}_{\\text{measurement noise}}$$\n",
    "\n",
    "### 3. Update Step\n",
    "We combine our prediction with the GPS reading:\n",
    "\n",
    "$$\\underbrace{\\hat{x}_t}_{\\text{final estimate}} = \\underbrace{\\hat{x}_{t|t-1}}_{\\text{prediction}} + \\underbrace{K_t}_{\\text{Kalman gain}} (\\underbrace{z_t - \\hat{x}_{t|t-1}}_{\\text{measurement error}})$$\n",
    "\n",
    "The Kalman gain $K_t$ is like a \"trust factor\" that decides how much to trust:\n",
    "- Our prediction vs. GPS reading\n",
    "- Higher $K_t$ → Trust GPS more\n",
    "- Lower $K_t$ → Trust prediction more\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Adaptive Trust**:\n",
    "   - If GPS is usually accurate → Trust it more\n",
    "   - If car moves predictably → Trust predictions more\n",
    "   - Automatically adjusts based on performance\n",
    "\n",
    "2. **Error Handling**:\n",
    "   - Accounts for both prediction and measurement errors\n",
    "   - More uncertain → Less trust\n",
    "   - More precise → More trust\n",
    "\n",
    "3. **Memory**:\n",
    "   - Maintains running estimates\n",
    "   - Uses all past information efficiently\n",
    "   - Updates beliefs smoothly\n",
    "\n",
    "## Why It Works\n",
    "\n",
    "The Kalman filter is optimal because it:\n",
    "1. Minimizes estimation errors\n",
    "2. Accounts for all known uncertainties\n",
    "3. Updates estimates efficiently\n",
    "4. Adapts to changing conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6:\n",
    "\n",
    "Condizioni di stazionarietà di un processo ARMA. Il processo AR(2) yt = 1.5yt−1 − 0.5yt−2 + εt è stazionario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA Process Stationarity\n",
    "\n",
    "## Understanding Stationarity\n",
    "\n",
    "Let's start with what stationarity means in practical terms. A process is stationary if its statistical properties don't change over time. This means:\n",
    "\n",
    "1. Constant mean: $E[Y_t] = \\mu$ (same for all t)\n",
    "2. Constant variance: $Var(Y_t) = \\sigma^2$ (same for all t)\n",
    "3. Covariance depends only on time difference: $Cov(Y_t, Y_{t+h}) = \\gamma(h)$\n",
    "\n",
    "## Stationarity Conditions for AR Processes\n",
    "\n",
    "For an AR(p) process:\n",
    "$$Y_t = \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + ... + \\phi_pY_{t-p} + \\varepsilon_t$$\n",
    "\n",
    "The stationarity condition involves the characteristic equation:\n",
    "$$1 - \\phi_1z - \\phi_2z^2 - ... - \\phi_pz^p = 0$$\n",
    "\n",
    "The process is stationary if and only if all roots of this equation lie outside the unit circle (have modulus greater than 1).\n",
    "\n",
    "## For Our Specific AR(2) Process\n",
    "\n",
    "Let's analyze: $y_t = 1.5y_{t-1} - 0.5y_{t-2} + \\varepsilon_t$\n",
    "\n",
    "Step 1: Write the characteristic equation\n",
    "- Original equation: $y_t = 1.5y_{t-1} - 0.5y_{t-2} + \\varepsilon_t$\n",
    "- Rearrange: $y_t - 1.5y_{t-1} + 0.5y_{t-2} = \\varepsilon_t$\n",
    "- Characteristic equation: $1 - 1.5z + 0.5z^2 = 0$\n",
    "\n",
    "Step 2: Find the roots\n",
    "- This is a quadratic equation: $0.5z^2 - 1.5z + 1 = 0$\n",
    "- Using the quadratic formula: $z = \\frac{1.5 \\pm \\sqrt{2.25 - 2}}{1}$\n",
    "- $z = \\frac{1.5 \\pm \\sqrt{0.25}}{1}$\n",
    "- $z = \\frac{1.5 \\pm 0.5}{1}$\n",
    "- Roots are: $z_1 = 2$ and $z_2 = 1$\n",
    "\n",
    "Step 3: Check stationarity\n",
    "- One root is $z_1 = 2$ (outside unit circle)\n",
    "- Other root is $z_2 = 1$ (exactly on unit circle)\n",
    "- Since we have a root on the unit circle, this process is NOT stationary\n",
    "\n",
    "## Visual Explanation\n",
    "\n",
    "Consider what this means:\n",
    "1. Having a root on the unit circle means the process has \"infinite memory\"\n",
    "2. The process won't \"forget\" past shocks\n",
    "3. This creates persistent effects that prevent mean reversion\n",
    "4. Therefore, the process can wander without returning to any fixed mean\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "A non-stationary process like this one:\n",
    "1. Won't have a constant mean\n",
    "2. Won't have a constant variance\n",
    "3. Will show persistent effects from shocks\n",
    "4. May need differencing to become stationary\n",
    "\n",
    "## Alternative Form: Factored Representation\n",
    "\n",
    "We can write our characteristic equation in factored form:\n",
    "$$(1 - \\frac{1}{2}z)(1 - z) = 0$$\n",
    "\n",
    "This clearly shows:\n",
    "1. One root at $z = 2$ (stationary component)\n",
    "2. One root at $z = 1$ (unit root, non-stationary component)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This AR(2) process is not stationary because:\n",
    "1. It has a unit root $(z = 1)$\n",
    "2. It would need first differencing to become stationary\n",
    "3. It is actually an integrated process of order 1, or I(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8:\n",
    "\n",
    "Genesi e proprietà del ciclo stocastico stazionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genesis and Properties of the Stationary Stochastic Cycle\n",
    "\n",
    "The stochastic cycle emerges from the deterministic cycle through a process of \"stochasticization\". Let's understand this step by step:\n",
    "\n",
    "## 1. Starting from the Deterministic Cycle\n",
    "\n",
    "A deterministic cycle can be represented as a sinusoidal function:\n",
    "\n",
    "$$f(t) = R \\cos(\\phi + \\lambda t)$$\n",
    "\n",
    "where:\n",
    "- $R$ is the amplitude (the cycle oscillates between $+R$ and $-R$)\n",
    "- $\\lambda$ is the frequency (number of cycles per unit time)\n",
    "- $\\phi$ is the phase (which shifts the cosine left or right)\n",
    "\n",
    "This can be rewritten equivalently as:\n",
    "\n",
    "$$f(t) = A\\cos(\\lambda t) + B\\sin(\\lambda t)$$\n",
    "\n",
    "where:\n",
    "- $A = R\\cos(\\phi)$\n",
    "- $B = -R\\sin(\\phi)$\n",
    "\n",
    "## 2. Markov Representation\n",
    "\n",
    "For discrete time $t$, we can write this in a recursive form:\n",
    "\n",
    "$$\\begin{pmatrix} f_t \\\\ f^*_t \\end{pmatrix} = \\begin{pmatrix} \\cos\\lambda & \\sin\\lambda \\\\ -\\sin\\lambda & \\cos\\lambda \\end{pmatrix} \\begin{pmatrix} f_{t-1} \\\\ f^*_{t-1} \\end{pmatrix}$$\n",
    "\n",
    "where $f^*_t$ is an auxiliary variable that helps generate the cycle.\n",
    "\n",
    "## 3. Making it Stochastic\n",
    "\n",
    "To create a stochastic cycle, we add random innovations and a damping factor:\n",
    "\n",
    "$$\\begin{pmatrix} \\psi_t \\\\ \\psi^*_t \\end{pmatrix} = \\rho \\begin{pmatrix} \\cos\\lambda & \\sin\\lambda \\\\ -\\sin\\lambda & \\cos\\lambda \\end{pmatrix} \\begin{pmatrix} \\psi_{t-1} \\\\ \\psi^*_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\kappa_t \\\\ \\kappa^*_t \\end{pmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\rho$ is the damping factor $(0 \\leq \\rho < 1)$\n",
    "- $\\kappa_t, \\kappa^*_t$ are white noise disturbances with variance $\\sigma^2_\\kappa$\n",
    "- $\\psi_t$ is the stochastic cycle\n",
    "- $\\psi^*_t$ is its auxiliary component\n",
    "\n",
    "## 4. Key Properties\n",
    "\n",
    "1. **Stationarity**: The cycle is stationary when $0 \\leq \\rho < 1$. The damping factor $\\rho$ ensures that shocks have a diminishing effect over time.\n",
    "\n",
    "2. **Period**: The period of the cycle is $2\\pi/\\lambda$. For example, if we want a cycle of 20 time units, we set $\\lambda = 2\\pi/20$.\n",
    "\n",
    "3. **Persistence**: $\\rho$ controls how long cycles persist. Values close to 1 create long-lasting cycles, while smaller values create more rapidly dampening cycles.\n",
    "\n",
    "4. **Innovation Variance**: $\\sigma^2_\\kappa$ determines how much random variation enters the cycle at each time point.\n",
    "\n",
    "5. **Complex Roots**: The transition matrix has complex eigenvalues $\\rho(\\cos\\lambda \\pm i\\sin\\lambda)$, which create the cyclical behavior.\n",
    "\n",
    "## 5. Interpretation\n",
    "\n",
    "The stochastic cycle combines:\n",
    "- Regular cyclical movement (from the rotation matrix)\n",
    "- Persistence (through $\\rho$)\n",
    "- Random innovations (via $\\kappa_t$)\n",
    "\n",
    "This makes it ideal for modeling economic cycles, where we observe:\n",
    "- Regular but not perfectly periodic fluctuations\n",
    "- Gradual changes in amplitude and phase\n",
    "- Random shocks that affect the cycle\n",
    "\n",
    "The stochastic cycle is a key component in structural time series models, often combined with trend and seasonal components to create comprehensive models of economic time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9:\n",
    "\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- random walk integrato,\n",
    "- stagionalità trimestrale a dummy stocastiche,\n",
    "- rumore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10:\n",
    "\n",
    "L’inizializzazione del vettore di stato in un modello in forma state space: si considerino i casi di variabili di stato stazionarie e non stazionarie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11:\n",
    "\n",
    "Condizioni di stazionarietà di un processo AR(p). Il processo Yt = 1.5Yt−1 − 0.6Yt−1 + εt, εt ∼ WN è stazionario? Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12:\n",
    "\n",
    "Descrivere le funzioni di autocorrelaione e autocorrelazione parziale di un processo AR(p)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 13:\n",
    "\n",
    "Le matrici T , Q, a1|0, P1|0 del ciclo stocastico stazionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 14:\n",
    "\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- random walk integrato,\n",
    "- stagionalità trimestrale a dummy stocastiche,\n",
    "- rumore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 15:\n",
    "\n",
    "Come modellereste un improvviso cambio di pendenza in un modello UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 16:\n",
    "\n",
    "a) Si enunci la condizione di stazionarietà di un processo AR(p) (causale).\n",
    "\n",
    "b) Il processo Yt = 1.5Yt−1 − 0.5Yt−1 + εt, εt ∼ WN è stazionario? Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 17:\n",
    "\n",
    "Descrivere le funzioni di autocorrelaione e autocorrelazione parziale di un processo MA(q)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 18:\n",
    "\n",
    "Che cosa sono e a che cosa servono i residui ausiliari nei modelli UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 19:\n",
    "\n",
    "Disegnare una possibile coppia di funzioni di autocorrelaione e autocorrelazione parziale per un processo MA(2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 20:\n",
    "\n",
    "Le matrici T , Q, a1|0, P1|0 per la stagionalità a sinusoidi stocastiche a con periodo base s = 7 (per esempio per dati giornalieri).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 21:\n",
    "\n",
    "Che cosa significa che un processo stocastico è integrato di ordine d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 22:\n",
    "\n",
    "A che cosa servono il filtro di Kalman e lo smoother (che quantità calcolano)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 23:\n",
    "Che cosa significa “Xt è un processo debolmente stazionario”? Come posso trasformare Xt in modo che diventi un processo integrato di ordine 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 24:\n",
    "Disegnare una possibile coppia di funzioni di autocorrelaione/autocorrelazione parziale per un processo MA(3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 25:\n",
    "Le matrici T , Q, a1|0, P1|0 per la stagionalità a sinusoidi stocastiche con periodo base s = 365 composta solo dalle prime due armoniche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 26:\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- random walk integrato,\n",
    "- stagionalità a sinusoidi stocastiche con periodo s = 7 (dati giornalieri),\n",
    "- AR(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 27:\n",
    "Come si può individuare un cambio di livello repentino nei modelli UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 28:\n",
    "Sia Yt un processo debolmente stazionario a media EYt = µ e funzione di autocovarianza Cov(Yt, Yt−k) = γk. Si fornisca la formula del previsore lineare ottimo per prvedere Y4 per mezzo di Y1, Y2, Y3 (non quella generica, ma quella specifica per questo problema con i contenuti delle matrici esplicitati)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 29:\n",
    "Sia Xt un processo debolmente stazionario. Fornire una formula per ottenere il processo Yt integrato di ordine uno e che una volta differenziato diventi Xt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 30:\n",
    "Definire il ciclo stocastico stazionario e spiegare (magari con un disegno) le funzioni di ogni elemento della sua equzione di transizione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 31:\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- local linear trend,\n",
    "- stagionalità a sinusoidi stocastiche con periodo s = 365 ma solo con le prime due armoniche,\n",
    "- AR(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 32:\n",
    "Nell’ambito dei modelli in forma state space, che cosa forniscono gli algoritmi Kalman filter, smoother e one-step-ahead predictor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 33:\n",
    "Sia Yt = Yt−1 + εt, con εt ∼ i.i.d.N(0, σ2) t = 0, 1, 2, . . ., un processo random walk con Y0 = 0 e si supponga di osservare Y1, Y2. Si calcoli la proiezione lineare P[Y3|Y1, Y2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 34:\n",
    "Il processo random walk è stazionario (motivare la risposta)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 35:\n",
    "La forma state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 36:\n",
    "Le matrici della forma state space di un modello UCM con le seguenti componenti:\n",
    "- local linear trend,\n",
    "- stagionalità a sinusoidi stocastiche con periodo s = 365 ma solo con le prime tre armoniche,\n",
    "- errore di osservazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 37:\n",
    "Che strumento posso utilizzare per identificare cambi di livello in un modello in forma state space che include il local linear trend tra le componenti?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 38:\n",
    "Let Y = −1 + X + X2 + X3, with X ∼ N(0, 1). Compute the optimal linear prediction P[Y |X, X2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 39:\n",
    "Let Xt = 0.9Xt−1 + εt, with εt white noise, be an AR(1) process. Is it stationary? In the case of a positive answer, what are its mean and variance? What kind of ARIMA process is Yt = Yt−1 + Xt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 40:\n",
    "Seasonal components in UCM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 41:\n",
    " Write in state space form the time-varying regression model yt = µt + βtxt + εt, where εt is a white noise and µt and βt are both random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 42:\n",
    "How would you identify additive outliers in a time series modeled with UCM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kalman Filter: A Theoretical Foundation for Time Series Analysis\n",
    "\n",
    "The Kalman Filter serves as a fundamental tool in time series analysis, allowing us to estimate unobserved components (states) of a system using noisy measurements. Let me guide you through understanding this powerful estimation technique using the state space framework.\n",
    "\n",
    "## State Space Representation\n",
    "\n",
    "In time series analysis, we work with two primary equations that define our system:\n",
    "\n",
    "1. The Observation (or Measurement) Equation:\n",
    "$$\n",
    "\\underbrace{y_t}_{\\text{observation}} = \\underbrace{Z_t}_{\\text{observation matrix}} \\underbrace{\\alpha_t}_{\\text{state vector}} + \\underbrace{d_t}_{\\text{known input}} + \\underbrace{\\varepsilon_t}_{\\text{observation noise}}\n",
    "$$\n",
    "where $\\varepsilon_t \\sim N(0, H_t)$\n",
    "\n",
    "2. The State (or Transition) Equation:\n",
    "$$\n",
    "\\underbrace{\\alpha_t}_{\\text{current state}} = \\underbrace{T_t}_{\\text{transition matrix}} \\underbrace{\\alpha_{t-1}}_{\\text{previous state}} + \\underbrace{c_t}_{\\text{known input}} + \\underbrace{R_t\\eta_t}_{\\text{state noise}}\n",
    "$$\n",
    "where $\\eta_t \\sim N(0, Q_t)$\n",
    "\n",
    "The matrices $Z_t$, $T_t$, and $R_t$ might be time-varying or constant, depending on the specific model. The vectors $d_t$ and $c_t$ represent known inputs or deterministic components.\n",
    "\n",
    "## The Kalman Filter Algorithm\n",
    "\n",
    "The Kalman Filter operates recursively through two main steps:\n",
    "\n",
    "### 1. Prediction Step\n",
    "\n",
    "First, we predict the state vector and its covariance matrix:\n",
    "\n",
    "State Prediction:\n",
    "$$\n",
    "\\underbrace{a_{t|t-1}}_{\\text{predicted state}} = \\underbrace{T_t}_{\\text{transition matrix}} \\underbrace{a_{t-1}}_{\\text{previous estimate}} + \\underbrace{c_t}_{\\text{known input}}\n",
    "$$\n",
    "\n",
    "Covariance Prediction:\n",
    "$$\n",
    "\\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} = \\underbrace{T_t}_{\\text{transition matrix}} \\underbrace{P_{t-1}}_{\\text{previous covariance}} \\underbrace{T_t'}_{\\text{transpose}} + \\underbrace{R_tQ_tR_t'}_{\\text{state noise covariance}}\n",
    "$$\n",
    "\n",
    "### 2. Update Step\n",
    "\n",
    "When new data arrives, we update our predictions:\n",
    "\n",
    "Innovation (Prediction Error):\n",
    "$$\n",
    "\\underbrace{v_t}_{\\text{innovation}} = \\underbrace{y_t}_{\\text{observation}} - \\underbrace{Z_ta_{t|t-1}}_{\\text{predicted observation}} - \\underbrace{d_t}_{\\text{known input}}\n",
    "$$\n",
    "\n",
    "Innovation Variance:\n",
    "$$\n",
    "\\underbrace{F_t}_{\\text{innovation variance}} = \\underbrace{Z_t}_{\\text{observation matrix}} \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} + \\underbrace{H_t}_{\\text{observation noise variance}}\n",
    "$$\n",
    "\n",
    "Kalman Gain:\n",
    "$$\n",
    "\\underbrace{K_t}_{\\text{Kalman gain}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} \\underbrace{F_t^{-1}}_{\\text{inverse innovation variance}}\n",
    "$$\n",
    "\n",
    "State Update:\n",
    "$$\n",
    "\\underbrace{a_t}_{\\text{updated state}} = \\underbrace{a_{t|t-1}}_{\\text{predicted state}} + \\underbrace{K_t}_{\\text{Kalman gain}} \\underbrace{v_t}_{\\text{innovation}}\n",
    "$$\n",
    "\n",
    "Covariance Update:\n",
    "$$\n",
    "\\underbrace{P_t}_{\\text{updated covariance}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} - \\underbrace{K_tF_tK_t'}_{\\text{correction term}}\n",
    "$$\n",
    "\n",
    "## Understanding the Filter's Logic\n",
    "\n",
    "The Kalman Filter achieves optimality through its careful balancing of predictions and observations. The Kalman gain $K_t$ plays a crucial role in this balance:\n",
    "\n",
    "1. When observation noise ($H_t$) is small relative to state uncertainty ($P_{t|t-1}$), the gain gives more weight to the new observation\n",
    "2. When observation noise is large, the gain gives more weight to our prediction\n",
    "\n",
    "The filter's operation can be understood as a Bayesian updating process:\n",
    "- The prediction step represents our prior belief\n",
    "- The observation provides new evidence\n",
    "- The update step combines these to form our posterior belief\n",
    "\n",
    "## Initialization\n",
    "\n",
    "The filter requires initial values:\n",
    "$$\n",
    "a_0 \\text{ and } P_0\n",
    "$$\n",
    "\n",
    "For stationary components, we can use the unconditional mean and variance. For non-stationary components, we often use diffuse initialization (very large initial variance).\n",
    "\n",
    "## Why the Filter is Optimal\n",
    "\n",
    "The Kalman Filter provides optimal estimates under three conditions:\n",
    "1. The system is linear (as shown in our state space equations)\n",
    "2. All noise terms are Gaussian\n",
    "3. The covariance matrices ($H_t$, $Q_t$) are known\n",
    "\n",
    "Under these conditions, the filter minimizes the mean squared error of our state estimates and provides the exact conditional distribution of the state given all past observations:\n",
    "$$\n",
    "\\alpha_t|Y_t \\sim N(a_t, P_t)\n",
    "$$\n",
    "where $Y_t$ represents all observations up to time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Kalman Filter Update Step\n",
    "\n",
    "Let's break down each equation of the Update Step and understand how they work together to refine our state estimates. Think of the Update Step as a careful weighing of new information against our previous beliefs.\n",
    "\n",
    "## 1. The Innovation Equation\n",
    "\n",
    "$$\n",
    "\\underbrace{v_t}_{\\text{innovation}} = \\underbrace{y_t}_{\\text{observation}} - \\underbrace{Z_ta_{t|t-1}}_{\\text{predicted observation}} - \\underbrace{d_t}_{\\text{known input}}\n",
    "$$\n",
    "\n",
    "This equation calculates how \"surprised\" we are by the new measurement. Let's break it down:\n",
    "\n",
    "- $y_t$ is what we actually observe\n",
    "- $Z_ta_{t|t-1}$ is what we expected to observe based on our prediction\n",
    "- $d_t$ accounts for any known external influences\n",
    "- $v_t$ is the difference between reality and expectation\n",
    "\n",
    "Think of it like checking your bank account: if you predicted you'd have $100 ($Z_ta_{t|t-1}$), but you actually have $90 ($y_t$), your innovation ($v_t$) is -$10. This tells you something unexpected happened.\n",
    "\n",
    "## 2. The Innovation Variance\n",
    "\n",
    "$$\n",
    "\\underbrace{F_t}_{\\text{innovation variance}} = \\underbrace{Z_t}_{\\text{observation matrix}} \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} + \\underbrace{H_t}_{\\text{observation noise variance}}\n",
    "$$\n",
    "\n",
    "This equation tells us how much uncertainty there is in our innovation. It combines two sources of uncertainty:\n",
    "\n",
    "1. $Z_tP_{t|t-1}Z_t'$: How uncertain we are about our prediction\n",
    "2. $H_t$: How noisy our measurements are\n",
    "\n",
    "Continuing our bank account analogy: if you're very uncertain about your prediction ($P_{t|t-1}$ is large) and your bank's reporting system sometimes has errors ($H_t$ is large), then $F_t$ will be large, indicating you shouldn't be too alarmed by discrepancies.\n",
    "\n",
    "## 3. The Kalman Gain\n",
    "\n",
    "$$\n",
    "\\underbrace{K_t}_{\\text{Kalman gain}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} \\underbrace{Z_t'}_{\\text{transpose}} \\underbrace{F_t^{-1}}_{\\text{inverse innovation variance}}\n",
    "$$\n",
    "\n",
    "The Kalman gain is perhaps the most crucial equation - it determines how much we should trust our new measurement versus our prediction. It's like a smart weighing scale that considers:\n",
    "\n",
    "- How uncertain we are about our prediction ($P_{t|t-1}$)\n",
    "- How uncertain we are about our measurement ($F_t^{-1}$ includes $H_t$)\n",
    "\n",
    "Properties of the Kalman gain:\n",
    "- If measurement noise ($H_t$) is small, $K_t$ will be larger, giving more weight to new measurements\n",
    "- If prediction uncertainty ($P_{t|t-1}$) is small, $K_t$ will be smaller, giving more weight to our predictions\n",
    "\n",
    "## 4. The State Update\n",
    "\n",
    "$$\n",
    "\\underbrace{a_t}_{\\text{updated state}} = \\underbrace{a_{t|t-1}}_{\\text{predicted state}} + \\underbrace{K_t}_{\\text{Kalman gain}} \\underbrace{v_t}_{\\text{innovation}}\n",
    "$$\n",
    "\n",
    "This is where everything comes together. We take our prediction and correct it based on the new information. The correction is:\n",
    "- Proportional to how wrong we were ($v_t$)\n",
    "- Scaled by how much we trust the new information ($K_t$)\n",
    "\n",
    "In our bank account example: if we predicted $100, saw $90, and our Kalman gain is 0.7, our new estimate would be:\n",
    "$100 + 0.7 \\times (-10) = 93$\n",
    "\n",
    "## 5. The Covariance Update\n",
    "\n",
    "$$\n",
    "\\underbrace{P_t}_{\\text{updated covariance}} = \\underbrace{P_{t|t-1}}_{\\text{predicted covariance}} - \\underbrace{K_tF_tK_t'}_{\\text{correction term}}\n",
    "$$\n",
    "\n",
    "This final equation updates our uncertainty about the state. Notice that it always decreases our uncertainty (we subtract the correction term). This makes sense because:\n",
    "- New information, even if noisy, should make us more certain\n",
    "- The more we trust the measurement (larger $K_t$), the more our uncertainty decreases\n",
    "\n",
    "## How They Work Together\n",
    "\n",
    "The five equations form a coherent sequence:\n",
    "1. Calculate how wrong our prediction was ($v_t$)\n",
    "2. Determine how much we trust this error ($F_t$)\n",
    "3. Compute the optimal way to incorporate new information ($K_t$)\n",
    "4. Update our state estimate ($a_t$)\n",
    "5. Update our uncertainty about the state ($P_t$)\n",
    "\n",
    "This sequence ensures that each new observation improves our estimate in a statistically optimal way, carefully balancing our prior knowledge with new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Operations, Initialization, and Smoothing in Kalman Filters\n",
    "\n",
    "## Understanding Matrix Multiplication with Transposes\n",
    "\n",
    "When we see expressions like $Z_tP_{t|t-1}Z_t'$ in the Innovation Variance equation:\n",
    "$$\n",
    "F_t = Z_tP_{t|t-1}Z_t' + H_t\n",
    "$$\n",
    "we're dealing with a fundamental concept in covariance propagation. Let's understand why this happens.\n",
    "\n",
    "### Why We Multiply by Transposes\n",
    "\n",
    "The reason lies in how we transform variance-covariance matrices. When we multiply a random variable by a matrix, its covariance matrix transforms in a specific way. Consider a simple example:\n",
    "\n",
    "If we have a random vector $x$ with covariance matrix $P$, and we transform it by matrix $A$ to get $y = Ax$, then the covariance of $y$ is:\n",
    "$$\n",
    "\\text{Cov}(y) = APA'\n",
    "$$\n",
    "\n",
    "This $APA'$ pattern appears throughout the Kalman Filter because we're constantly transforming random variables and need to keep track of their uncertainties. Let's see what this means in practice:\n",
    "\n",
    "1. In the Innovation Variance equation:\n",
    "   - $P_{t|t-1}$ is our uncertainty about the state\n",
    "   - $Z_t$ transforms the state into measurement space\n",
    "   - $Z_tP_{t|t-1}Z_t'$ gives us the uncertainty of our prediction in measurement space\n",
    "\n",
    "2. The multiplication by transpose ensures:\n",
    "   - The resulting matrix has the correct dimensions\n",
    "   - The covariance matrix remains symmetric (as all covariance matrices must be)\n",
    "   - The variances (diagonal elements) remain positive\n",
    "\n",
    "## Initialization: Starting the Filter Right\n",
    "\n",
    "Initialization is crucial because it provides the starting point for our recursive estimations. We need to set:\n",
    "1. Initial state estimate ($a_0$)\n",
    "2. Initial covariance matrix ($P_0$)\n",
    "\n",
    "### Good Values for Initialization\n",
    "\n",
    "For the initial state $a_0$:\n",
    "1. For stationary components:\n",
    "   - Use the unconditional mean of the process\n",
    "   - For example, for a mean-reverting process, use its long-term mean\n",
    "\n",
    "2. For non-stationary components:\n",
    "   - Use the first few observations to make an educated guess\n",
    "   - For a trend, you might use the first observation as level and first difference as slope\n",
    "\n",
    "For the initial covariance matrix $P_0$:\n",
    "\n",
    "1. For stationary components:\n",
    "   - Use the unconditional variance of the process\n",
    "   - For AR(1) process with parameter $\\phi$ and innovation variance $\\sigma^2$, use $\\sigma^2/(1-\\phi^2)$\n",
    "\n",
    "2. For non-stationary components:\n",
    "   - Use a \"diffuse\" or large variance (e.g., $10^6$ or $10^7$)\n",
    "   - This indicates high uncertainty about initial values\n",
    "   - The filter will quickly converge to reasonable values\n",
    "\n",
    "Example initialization for a local level model:\n",
    "```\n",
    "P_0 = [\n",
    "    1e6    0    # Level uncertainty (diffuse)\n",
    "    0    1e2    # Slope uncertainty (moderately certain)\n",
    "]\n",
    "```\n",
    "\n",
    "## Smoothing: Looking Back for Better Estimates\n",
    "\n",
    "Smoothing is indeed a crucial concept in Kalman Filtering. While the regular Kalman Filter gives us estimates based on data up to time t (filtering), smoothing uses the entire dataset to improve our estimates.\n",
    "\n",
    "### Types of Smoothing\n",
    "\n",
    "1. Fixed-Interval Smoothing:\n",
    "   - Uses all data from t=1 to T\n",
    "   - Gives estimates $a_{t|T}$ for all t\n",
    "   - Most common in time series analysis\n",
    "   \n",
    "2. Fixed-Point Smoothing:\n",
    "   - Updates estimate of state at fixed time k as new data arrives\n",
    "   - Gives series of estimates $a_{k|t}$ for t > k\n",
    "\n",
    "### The Smoothing Equations\n",
    "\n",
    "The smoothing recursions run backwards from T to 1:\n",
    "\n",
    "$$\n",
    "\\underbrace{a_{t|T}}_{\\text{smoothed state}} = \\underbrace{a_t}_{\\text{filtered state}} + \\underbrace{P_t}_{\\text{filtered covariance}} \\underbrace{T_t'}_{\\text{transition}} \\underbrace{P_{t+1|t}^{-1}}_{\\text{inverse prediction}} (\\underbrace{a_{t+1|T}}_{\\text{next smooth}} - \\underbrace{T_ta_t}_{\\text{prediction}})\n",
    "$$\n",
    "\n",
    "This gives us better estimates because:\n",
    "- We use future information not available during filtering\n",
    "- The estimates are typically smoother (less jagged)\n",
    "- The uncertainty of smoothed estimates is smaller than filtered estimates\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "In time series analysis:\n",
    "- Use filtered estimates for real-time applications\n",
    "- Use smoothed estimates for historical analysis\n",
    "- Smoothed estimates are especially useful for:\n",
    "  - Trend estimation\n",
    "  - Seasonal adjustment\n",
    "  - Cycle extraction\n",
    "\n",
    "The improvement from smoothing is most noticeable when:\n",
    "- The signal-to-noise ratio is low\n",
    "- There are missing observations\n",
    "- The state dynamics are strongly persistent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
