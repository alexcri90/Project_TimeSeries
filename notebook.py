# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f9LUS4R9m6mNbr4jeOeOMc9GbjrO49dh

# Environment Setup

## Package Installation
"""

# %pip install pandas matplotlib numpy seaborn scikit-learn xgboost patsy statsmodels
# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# %pip install seaborn
# %pip install pmdarima
# %pip install tqdm

"""## Imports and GPU Configuration"""

# Cell 1 - Environment Setup and Imports
import warnings
import os
warnings.filterwarnings('ignore')

# Essential libraries
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import platform
from scipy import stats

# Time series specific
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.structural import UnobservedComponents
import statsmodels.api as sm
from statsmodels.stats.diagnostic import acorr_ljungbox

# ML/DL libraries
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# Check GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Plotting settings - using a more reliable approach
plt.style.use('default')  # Reset to default style
sns.set_style("whitegrid")  # Apply seaborn styling
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['figure.dpi'] = 100

def print_gpu_info():
    """
    Print detailed information about GPU configuration
    """
    print("System Information:")
    print(f"Python version: {sys.version}")
    print(f"Platform: {platform.platform()}")
    print(f"PyTorch version: {torch.__version__}")

    print("\nGPU Information:")
    if torch.cuda.is_available():
        print(f"GPU Available: Yes")
        print(f"Number of GPUs: {torch.cuda.device_count()}")
        print(f"Current GPU: {torch.cuda.current_device()}")
        print(f"GPU Name: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB")
        print(f"GPU Memory Reserved: {torch.cuda.memory_reserved(0)/1024**2:.2f} MB")

        # Test GPU with a simple operation
        print("\nTesting GPU...")
        x = torch.randn(1000, 1000).cuda()
        y = torch.matmul(x, x.t())
        print("GPU test successful!")
    else:
        print("No GPU available. Using CPU.")

print_gpu_info()

"""# Data Loading and Preprocessing

- Data Import
- Initial Data Quality Checks
- Missing Values Analysis
- DateTime Index Setup
"""

# Cell 2 - Data Loading and Basic Checks
def load_data(filepath):
    """
    Load and preprocess the time series data
    Parameters:
    filepath (str): Path to the CSV file
    Returns:
    pd.DataFrame: Preprocessed DataFrame with DateTime index
    """
    # Read the CSV file
    df = pd.read_csv(filepath)

    # Try to detect the datetime format automatically
    try:
        df['DateTime'] = pd.to_datetime(df['DateTime'], format='mixed')
    except ValueError as e:
        print(f"Warning: {e}")
        print("Attempting alternative datetime parsing...")
        try:
            df['DateTime'] = pd.to_datetime(df['Date']) + pd.to_timedelta(df['Hour'], unit='h')
        except Exception as e:
            print(f"Error in datetime parsing: {e}")
            raise

    # Set DateTime as index
    df.set_index('DateTime', inplace=True)

    # Sort index to ensure chronological order
    df.sort_index(inplace=True)

    # Display basic information
    print("\nDataset Info:")
    print(df.info())
    print("\nFirst few rows:")
    print(df.head())
    print("\nLast few rows:")
    print(df.tail())
    print("\nBasic statistics:")
    print(df.describe())

    return df

# Load the data
try:
    df = load_data('ts2024.csv')
except Exception as e:
    print(f"Error loading data: {e}")
    raise

# Cell 3 - Data Quality Analysis and Cleaning
def analyze_data_quality(df):
    """
    Perform comprehensive data quality analysis
    """
    print("=== Data Quality Analysis ===")

    # Basic dataset properties
    print("\n1. Dataset Properties:")
    print(f"Time range: {df.index.min()} to {df.index.max()}")
    print(f"Total number of records: {len(df)}")
    print(f"Sampling frequency: {df.index.freq if df.index.freq else 'Inferred hourly'}")

    # Check for duplicates
    duplicates = df.index.duplicated(keep=False)
    if duplicates.any():
        n_duplicates = duplicates.sum()
        print(f"\nWarning: Found {n_duplicates} duplicate timestamps!")
        print("\nExample of duplicates:")
        print(df[duplicates].sort_index().head())

    # Check for time series continuity
    expected_periods = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')
    missing_periods = set(expected_periods) - set(df.index)

    print("\n2. Time Series Continuity:")
    print(f"Number of missing timestamps: {len(missing_periods)}")
    if missing_periods:
        print("Sample of missing timestamps:")
        print(sorted(list(missing_periods))[:5])

    # Value distribution analysis
    print("\n3. Value Distribution (X):")
    print(df['X'].describe())

    # Outlier detection
    Q1 = df['X'].quantile(0.25)
    Q3 = df['X'].quantile(0.75)
    IQR = Q3 - Q1
    outliers = df[(df['X'] < (Q1 - 1.5 * IQR)) | (df['X'] > (Q3 + 1.5 * IQR))]

    print("\n4. Outlier Analysis:")
    print(f"Number of potential outliers: {len(outliers)}")
    if len(outliers) > 0:
        print("Sample of outliers:")
        print(outliers.head())

    # Missing data patterns
    print("\n5. Missing Data Patterns:")
    missing_by_month = df['X'].isna().groupby(df.index.month).sum()
    print("Missing values by month:")
    print(missing_by_month)

    return missing_periods, outliers, duplicates

def clean_data(df, duplicates):
    """
    Clean the data by:
    1. Handling duplicates by taking their mean
    2. Filling ONLY the two specific missing timestamps in historical data
    3. Keeping the last 744 values as missing for forecasting
    """
    print("=== Data Cleaning ===")

    df_cleaned = df.copy()

    # Step 1: Handle duplicates by taking the mean of X values for same timestamps
    if duplicates.any():
        print("\nHandling duplicates by taking the mean value for each timestamp...")
        df_cleaned = df_cleaned.groupby(level=0).agg({
            'Date': 'first',  # Keep first date string
            'Hour': 'first',  # Keep first hour
            'X': 'mean'      # Take mean of X values
        })
        print(f"After handling duplicates: {len(df_cleaned)} rows")

    # Step 2: Handle ONLY the two specific missing timestamps
    specific_missing = pd.to_datetime(['2015-03-29 02:00:00', '2016-03-27 02:00:00'])

    for timestamp in specific_missing:
        if timestamp in df_cleaned.index:
            continue

        # Get the same hour from the day before and after
        prev_day = df_cleaned.loc[timestamp - pd.Timedelta(days=1):timestamp - pd.Timedelta(days=1), 'X'].iloc[0]
        next_day = df_cleaned.loc[timestamp + pd.Timedelta(days=1):timestamp + pd.Timedelta(days=1), 'X'].iloc[0]

        # Create new row with interpolated value
        df_cleaned.loc[timestamp] = {
            'Date': timestamp.date(),
            'Hour': timestamp.hour,
            'X': (prev_day + next_day) / 2
        }

    # Sort index to ensure chronological order
    df_cleaned.sort_index(inplace=True)

    # Verify the cleaning worked correctly
    print("\nVerification after cleaning:")
    n_missing = df_cleaned['X'].isna().sum()
    print(f"Total missing values (should be 744): {n_missing}")

    if n_missing != 744:
        raise ValueError(f"ERROR: Expected 744 missing values, but found {n_missing}")

    return df_cleaned

# Analyze and clean data
missing_periods, outliers, duplicates = analyze_data_quality(df)

df = clean_data(df, duplicates)

# Cell 4 - Data Preprocessing and Feature Engineering
def preprocess_data(df):
    """
    Preprocess the time series data and engineer features
    """
    print("=== Data Preprocessing and Feature Engineering ===")

    # Create a copy to avoid modifying original data
    df_processed = df.copy()

    # Add time-based features
    df_processed['hour'] = df_processed.index.hour
    df_processed['day_of_week'] = df_processed.index.dayofweek
    df_processed['month'] = df_processed.index.month
    df_processed['is_weekend'] = df_processed.index.dayofweek.isin([5, 6]).astype(int)

    # Calculate rolling statistics (excluding the missing period)
    non_missing = df_processed[df_processed['X'].notna()]
    df_processed['rolling_mean_24h'] = non_missing['X'].rolling(window=24, min_periods=1).mean()
    df_processed['rolling_std_24h'] = non_missing['X'].rolling(window=24, min_periods=1).std()

    # Print summary of changes
    print("\nAdded features:")
    for col in df_processed.columns:
        print(f"- {col}")

    print("\nProcessing Summary:")
    print(f"Original shape: {df.shape}")
    print(f"Processed shape: {df_processed.shape}")

    return df_processed

# Preprocess the data
df_processed = preprocess_data(df)

# Cell 5 - Initial Visualizations
def plot_time_series(df):
    """
    Create initial visualizations of the time series data
    """
    # Plot the entire time series
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 1, 1)
    plt.plot(df.index, df['X'])
    plt.title('Time Series Plot of X')
    plt.xlabel('DateTime')
    plt.ylabel('Value')
    plt.grid(True)

    # Plot the last 2 months of data
    last_2_months = df.last('60D')
    plt.subplot(2, 1, 2)
    plt.plot(last_2_months.index, last_2_months['X'])
    plt.title('Last 2 Months of Data')
    plt.xlabel('DateTime')
    plt.ylabel('Value')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

def plot_patterns(df):
    """
    Plot daily and weekly patterns
    """
    plt.figure(figsize=(15, 10))

    # Daily pattern
    plt.subplot(2, 1, 1)
    daily_pattern = df.groupby(df.index.hour)['X'].mean()
    plt.plot(daily_pattern.index, daily_pattern.values)
    plt.title('Average Daily Pattern')
    plt.xlabel('Hour of Day')
    plt.ylabel('Average Value')
    plt.grid(True)

    # Weekly pattern
    plt.subplot(2, 1, 2)
    weekly_pattern = df.groupby(df.index.dayofweek)['X'].mean()
    plt.plot(weekly_pattern.index, weekly_pattern.values)
    plt.title('Average Weekly Pattern')
    plt.xlabel('Day of Week (0=Monday)')
    plt.ylabel('Average Value')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# Create visualizations
plot_time_series(df)
plot_patterns(df)

# Cell 6 - Statistical Analysis
def analyze_stationarity(series):
    """
    Perform stationarity analysis and seasonal decomposition
    """
    # Stationarity test
    print("=== Stationarity Analysis ===")
    result = adfuller(series.dropna())
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    print('Critical values:')
    for key, value in result[4].items():
        print('\t%s: %.3f' % (key, value))

    # Seasonal decomposition
    decomposition = seasonal_decompose(series.dropna(), period=24)  # 24 hours for daily seasonality

    # Plot decomposition
    plt.figure(figsize=(15, 12))
    plt.subplot(411)
    plt.plot(decomposition.observed)
    plt.title('Observed')
    plt.subplot(412)
    plt.plot(decomposition.trend)
    plt.title('Trend')
    plt.subplot(413)
    plt.plot(decomposition.seasonal)
    plt.title('Seasonal')
    plt.subplot(414)
    plt.plot(decomposition.resid)
    plt.title('Residual')
    plt.tight_layout()
    plt.show()

    return decomposition

# Perform statistical analysis
decomposition = analyze_stationarity(df['X'])

# Cell 7 - Advanced Time Series Analysis
print("=== Advanced Time Series Analysis ===")

def plot_acf_pacf(series, lags=40):
    """
    Plot ACF and PACF for the time series
    """
    plt.figure(figsize=(15, 10))

    # Plot ACF
    plt.subplot(211)
    plot_acf(series.dropna(), lags=lags, ax=plt.gca())
    plt.title('Autocorrelation Function')

    # Plot PACF
    plt.subplot(212)
    plot_pacf(series.dropna(), lags=lags, ax=plt.gca())
    plt.title('Partial Autocorrelation Function')

    plt.tight_layout()
    plt.show()

def analyze_multiple_seasonality(df):
    """
    Analyze multiple seasonal patterns in the data
    """
    # Daily seasonality
    daily_pattern = df.groupby(df.index.hour)['X'].mean()

    # Weekly seasonality
    weekly_pattern = df.groupby([df.index.dayofweek, df.index.hour])['X'].mean()
    weekly_pattern = weekly_pattern.unstack()

    # Monthly seasonality
    monthly_pattern = df.groupby([df.index.month, df.index.hour])['X'].mean()
    monthly_pattern = monthly_pattern.unstack()

    # Plotting
    fig = plt.figure(figsize=(15, 15))

    # Daily pattern
    plt.subplot(311)
    daily_pattern.plot()
    plt.title('Average Daily Pattern')
    plt.xlabel('Hour of Day')
    plt.ylabel('Average Value')

    # Weekly pattern
    plt.subplot(312)
    sns.heatmap(weekly_pattern, cmap='YlOrRd', cbar_kws={'label': 'Average Value'})
    plt.title('Weekly Pattern Heatmap')
    plt.xlabel('Hour of Day')
    plt.ylabel('Day of Week')

    # Monthly pattern
    plt.subplot(313)
    sns.heatmap(monthly_pattern, cmap='YlOrRd', cbar_kws={'label': 'Average Value'})
    plt.title('Monthly Pattern Heatmap')
    plt.xlabel('Hour of Day')
    plt.ylabel('Month')

    plt.tight_layout()
    plt.show()

    return daily_pattern, weekly_pattern, monthly_pattern

# Execute the functions
print("\nAnalyzing multiple seasonality patterns...")
daily_pattern, weekly_pattern, monthly_pattern = analyze_multiple_seasonality(df)

# Cell 8 - ACF and PACF Analysis

# First, let's analyze the original series
print("=== ACF and PACF Analysis of Original Series ===")
plot_acf_pacf(df['X'], lags=50)

# Now let's look at first difference
print("\n=== ACF and PACF Analysis of First Difference ===")
diff1 = df['X'].diff().dropna()
plot_acf_pacf(diff1, lags=50)

# Let's also look at seasonal difference (24-hour seasonality)
print("\n=== ACF and PACF Analysis of Seasonal Difference (24 hours) ===")
seasonal_diff = df['X'].diff(24).dropna()
plot_acf_pacf(seasonal_diff, lags=50)

# And finally, both differences combined
print("\n=== ACF and PACF Analysis of First and Seasonal Differences Combined ===")
both_diff = seasonal_diff.diff().dropna()
plot_acf_pacf(both_diff, lags=50)

# Let's also show some statistics about each transformation
def print_series_stats(series, name):
    print(f"\nStatistics for {name}:")
    print(f"Mean: {series.mean():.4f}")
    print(f"Std: {series.std():.4f}")
    print(f"ADF Test p-value: {adfuller(series.dropna())[1]:.4f}")

print_series_stats(df['X'], "Original Series")
print_series_stats(diff1, "First Difference")
print_series_stats(seasonal_diff, "Seasonal Difference")
print_series_stats(both_diff, "Both Differences")

# Cell 9 - Updated ACF and PACF Analysis

def plot_advanced_acf_pacf(series, max_lags=None, periods=None):
    """
    Plot ACF and PACF with multiple lag ranges and highlighted seasonal periods

    Parameters:
    - series: time series data
    - max_lags: maximum number of lags to show
    - periods: list of tuples (period, label) to highlight
    """
    if max_lags is None:
        max_lags = 24 * 8  # Show 8 days by default

    if periods is None:
        periods = [
            (24, "Daily"),
            (24 * 7, "Weekly"),
            (24 * 30, "Monthly")
        ]

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))

    # Plot ACF
    plot_acf(series.dropna(), lags=max_lags, ax=ax1, alpha=0.05,
            markersize=5, c='blue', marker='o')
    ax1.set_title('Autocorrelation Function')

    # Plot PACF
    plot_pacf(series.dropna(), lags=max_lags, ax=ax2, alpha=0.05,
             markersize=5, c='blue', marker='o')
    ax2.set_title('Partial Autocorrelation Function')

    # Add vertical lines for seasonal periods
    for period, label in periods:
        if period <= max_lags:
            for ax in [ax1, ax2]:
                ax.axvline(x=period, color='red', linestyle='--', alpha=0.3)
                ax.text(period, ax.get_ylim()[1], f' {label}',
                       rotation=0, verticalalignment='bottom')

    # Enhance confidence intervals visibility
    for ax in [ax1, ax2]:
        # Get the confidence intervals (approximately at y = ±2/√T)
        ci = 2/np.sqrt(len(series))
        ax.axhline(y=ci, color='gray', linestyle=':', alpha=0.5)
        ax.axhline(y=-ci, color='gray', linestyle=':', alpha=0.5)
        ax.fill_between(range(max_lags + 1), -ci, ci, color='blue', alpha=0.1)

    plt.tight_layout()
    plt.show()

# Plot for different ranges
print("=== ACF and PACF Analysis of Original Series ===")
print("\nShort-term patterns (2 days):")
plot_advanced_acf_pacf(df['X'], max_lags=48)

print("\nMedium-term patterns (2 weeks):")
plot_advanced_acf_pacf(df['X'], max_lags=24*14)

print("\nLong-term patterns (1 month):")
plot_advanced_acf_pacf(df['X'], max_lags=24*30)

# Also show the differenced series
print("\n=== ACF and PACF Analysis of Differenced Series ===")
diff1 = df['X'].diff().dropna()
seasonal_diff = df['X'].diff(24).dropna()
both_diff = seasonal_diff.diff().dropna()

print("\nFirst Difference:")
plot_advanced_acf_pacf(diff1, max_lags=48)

print("\nSeasonal Difference (24 hours):")
plot_advanced_acf_pacf(seasonal_diff, max_lags=48)

print("\nBoth Differences Combined:")
plot_advanced_acf_pacf(both_diff, max_lags=48)

"""# ARIMA Simple"""

# Cell 10 - Common Data Preparation
from tqdm import tqdm  # For progress bars

def prepare_data_for_modeling(df, forecast_horizon=744):
    """
    Prepare data for modeling (common across all models)
    """
    print("=== Data Preparation ===")

    # Remove the last forecast_horizon hours (target forecast period)
    train_data = df[df['X'].notna()].copy()

    # Create validation set (last month of available data)
    validation_size = forecast_horizon
    train_set = train_data[:-validation_size]
    val_set = train_data[-validation_size:]

    print("\nData Split Summary:")
    print(f"Training set: {train_set.index.min()} to {train_set.index.max()}")
    print(f"Validation set: {val_set.index.min()} to {val_set.index.max()}")
    print(f"Forecast period: {val_set.index.max() + pd.Timedelta(hours=1)} to {df.index.max()}")

    return train_set, val_set

# Prepare the data
train_set, val_set = prepare_data_for_modeling(df)

# Cell 11 - ARIMA Model Development and Training
def develop_arima_model(train_data):
    """
    Develop ARIMA model based on theoretical analysis
    """
    print("=== ARIMA Model Development ===")

    # Convert to pandas Series with frequency and handle any preprocessing
    y = train_data['X'].astype(float)
    y.index = pd.date_range(start=y.index[0], periods=len(y), freq='H')

    # Define more conservative models
    models_to_try = [
        ((1,1,1), (0,1,1,24)),    # Simple model with seasonal MA only
        ((1,1,1), (1,1,0,24)),    # Simple model with seasonal AR only
        ((2,1,2), (1,1,1,24)),    # More complex model
        ((0,1,1), (0,1,1,24))     # Airline model (often works well)
    ]

    best_aic = float('inf')
    best_model = None
    best_order = None
    best_seasonal_order = None

    print("\nTrying specific model configurations...")
    for order, seasonal_order in tqdm(models_to_try, desc="Testing Models"):
        try:
            print(f"\nFitting ARIMA{order}{seasonal_order}")

            # Add start_params to help convergence
            model = ARIMA(y,
                         order=order,
                         seasonal_order=seasonal_order,
                         enforce_stationarity=False,
                         enforce_invertibility=True)  # Changed to True

            # Use conditional sum of squares to start
            results = model.fit(method='innovations_mle')
            current_aic = results.aic

            print(f"AIC: {current_aic:.2f}")

            # Check if the model is reasonable (non-flat forecast)
            test_forecast = results.forecast(steps=24)
            forecast_std = test_forecast.std()

            # Only consider model if forecasts show variation
            if forecast_std > 0.001 and current_aic < best_aic:
                best_aic = current_aic
                best_model = results
                best_order = order
                best_seasonal_order = seasonal_order

        except Exception as e:
            print(f"Error fitting ARIMA{order}{seasonal_order}:")
            print(f"Error message: {str(e)}")
            continue

    if best_model is None:
        raise ValueError("No models were successfully fitted!")

    print("\nBest Model Configuration:")
    print(f"Order: {best_order}")
    print(f"Seasonal Order: {best_seasonal_order}")
    print(f"AIC: {best_aic}")
    print("\nModel Summary:")
    print(best_model.summary())

    return best_model, best_order, best_seasonal_order

# Cell 12 - ARIMA Model Diagnostics
def perform_arima_diagnostics(model):
    """
    Perform diagnostic tests for ARIMA model
    """
    print("=== ARIMA Model Diagnostics ===")

    residuals = model.resid

    # Create diagnostic plots
    fig = plt.figure(figsize=(15, 15))

    # 1. Residuals over time
    ax1 = plt.subplot(411)
    plt.plot(residuals.index, residuals, 'b.', alpha=0.5, markersize=2)
    plt.plot(residuals.index, pd.Series(0, index=residuals.index), 'r--')
    plt.title('Residuals over Time')
    plt.grid(True, alpha=0.2)

    # 2. ACF of residuals
    ax2 = plt.subplot(412)
    plot_acf(residuals, lags=50, ax=ax2, alpha=0.05,
             title="ACF of Residuals",
             markersize=3)
    plt.grid(True, alpha=0.2)

    # 3. PACF of residuals
    ax3 = plt.subplot(413)
    plot_pacf(residuals, lags=50, ax=ax3, alpha=0.05,
              title="PACF of Residuals",
              markersize=3)
    plt.grid(True, alpha=0.2)

    # 4. Q-Q plot
    ax4 = plt.subplot(414)
    stats.probplot(residuals, dist="norm", plot=plt)
    ax4.set_title("Q-Q plot of residuals")
    plt.grid(True, alpha=0.2)

    plt.tight_layout()
    plt.show()

    # Print statistics
    print("\nResidual Statistics:")
    print(f"Mean: {residuals.mean():.4f}")
    print(f"Std Dev: {residuals.std():.4f}")
    print(f"Skewness: {stats.skew(residuals):.4f}")
    print(f"Kurtosis: {stats.kurtosis(residuals):.4f}")

# Cell 13 - ARIMA Forecasting (with non-negative constraints)
def generate_arima_forecasts(model, train_data, val_data, forecast_horizon=744):
    """
    Generate and evaluate ARIMA forecasts with non-negative constraints
    """
    print("=== ARIMA Forecasting ===")

    # In-sample predictions
    in_sample_pred = model.get_prediction(start=0)
    in_sample_mean = np.maximum(in_sample_pred.predicted_mean, 0)  # Force non-negative

    # Validation set predictions
    val_pred = model.get_prediction(
        start=len(train_data),
        end=len(train_data)+len(val_data)-1,
        dynamic=False
    )
    val_mean = np.maximum(val_pred.predicted_mean, 0)  # Force non-negative
    val_conf = val_pred.conf_int()
    val_conf.iloc[:, 0] = np.maximum(val_conf.iloc[:, 0], 0)  # Force non-negative lower bound

    # Future forecasts
    forecast = model.get_forecast(
        steps=forecast_horizon,
        alpha=0.05
    )
    forecast_mean = np.maximum(forecast.predicted_mean, 0)  # Force non-negative
    forecast_conf = forecast.conf_int()
    forecast_conf.iloc[:, 0] = np.maximum(forecast_conf.iloc[:, 0], 0)  # Force non-negative lower bound

    # Plot results
    plt.figure(figsize=(15, 10))

    # Plot training data
    plt.plot(train_data.index, train_data['X'], label='Training Data', alpha=0.7)

    # Plot validation data
    plt.plot(val_data.index, val_data['X'], label='Validation Data', color='green', alpha=0.7)

    # Plot validation predictions
    plt.plot(val_data.index, val_mean, label='Validation Predictions', color='red', linestyle='--')
    plt.fill_between(val_data.index,
                     val_conf.iloc[:, 0],
                     val_conf.iloc[:, 1],
                     color='red', alpha=0.1)

    # Plot forecasts
    forecast_index = pd.date_range(start=val_data.index[-1] + pd.Timedelta(hours=1),
                                 periods=forecast_horizon,
                                 freq='H')
    plt.plot(forecast_index, forecast_mean, label='ARIMA Forecasts', color='blue')
    plt.fill_between(forecast_index,
                     forecast_conf.iloc[:, 0],
                     forecast_conf.iloc[:, 1],
                     color='blue', alpha=0.1)

    plt.title('ARIMA Model Forecasts')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Calculate error metrics (using non-negative predictions)
    val_rmse = np.sqrt(mean_squared_error(val_data['X'], val_mean))
    val_mae = mean_absolute_error(val_data['X'], val_mean)
    val_mape = np.mean(np.abs((val_data['X'] - val_mean) / val_data['X'])) * 100

    print("\nValidation Set Metrics:")
    print(f"RMSE: {val_rmse:.4f}")
    print(f"MAE: {val_mae:.4f}")
    print(f"MAPE: {val_mape:.4f}%")

    return forecast_mean, forecast_conf, val_mean, val_conf

# Cell 14 - Run ARIMA Pipeline
print("Starting ARIMA modeling pipeline...")

try:
    # First develop and train the model
    arima_model, best_order, best_seasonal_order = develop_arima_model(train_set)

    # Perform diagnostics
    perform_arima_diagnostics(arima_model)

    # Generate forecasts
    forecast_mean, forecast_conf, val_mean, val_conf = generate_arima_forecasts(arima_model, train_set, val_set)

    # Save forecasts to CSV
    forecast_df = pd.DataFrame({
        'datetime': pd.date_range(start=val_set.index[-1] + pd.Timedelta(hours=1),
                                periods=744,
                                freq='H'),
        'ARIMA_forecast': forecast_mean,
        'ARIMA_lower': forecast_conf.iloc[:, 0],
        'ARIMA_upper': forecast_conf.iloc[:, 1]
    })
    forecast_df.set_index('datetime', inplace=True)
    forecast_df.to_csv('arima_forecasts.csv')

    print("\nARIMA modeling pipeline completed!")
    print("\nForecasts have been saved to 'arima_forecasts.csv'")

except Exception as e:
    print(f"Error in ARIMA pipeline: {str(e)}")
    raise

# Cell 15 - Create plot combining original data and forecasts

import os

def plot_with_forecasts(df, forecast_csv):
    """
    Plot original time series with forecasts appended
    """
    # Read the forecasts
    forecasts = pd.read_csv(forecast_csv)
    forecasts['datetime'] = pd.to_datetime(forecasts['datetime'])

    # Create plot
    plt.figure(figsize=(15, 10))

    # Plot original data until November 2016
    plt.plot(df.index, df['X'], label='Historical Data', color='blue')

    # Add forecasts as continuation
    plt.plot(forecasts['datetime'], forecasts['ARIMA_forecast'],
             label='ARIMA Forecasts', color='red')

    # Add confidence intervals
    plt.fill_between(forecasts['datetime'],
                     forecasts['ARIMA_lower'],
                     forecasts['ARIMA_upper'],
                     color='red', alpha=0.1)

    plt.title('Time Series with December 2016 Forecasts')
    plt.legend()
    plt.grid(True)
    plt.show()

# First check if we have the forecasts CSV
if os.path.exists('arima_forecasts.csv'):
    print("Found existing forecasts, creating plot...")
    plot_with_forecasts(df, 'arima_forecasts.csv')
else:
    print("No forecast file found. Please run the ARIMA model first.")

# Cell 16 - Plot just the forecasted values
def plot_forecasts_only(forecast_csv):
    """
    Plot only the forecasted values for December 2016
    """
    # Read the forecasts
    forecasts = pd.read_csv(forecast_csv)
    forecasts['datetime'] = pd.to_datetime(forecasts['datetime'])

    # Create plot
    plt.figure(figsize=(15, 8))

    # Plot forecasts
    plt.plot(forecasts['datetime'], forecasts['ARIMA_forecast'],
             label='ARIMA Forecasts', color='red', linewidth=2)

    # Add confidence intervals
    plt.fill_between(forecasts['datetime'],
                    forecasts['ARIMA_lower'],
                    forecasts['ARIMA_upper'],
                    color='red', alpha=0.2,
                    label='95% Confidence Interval')

    plt.title('ARIMA Forecasts for December 2016')
    plt.ylabel('Value')
    plt.xlabel('Date')
    plt.legend()
    plt.grid(True)

    # Print some statistics about the forecasts
    print("\nForecast Statistics:")
    print(f"Mean: {forecasts['ARIMA_forecast'].mean():.4f}")
    print(f"Std Dev: {forecasts['ARIMA_forecast'].std():.4f}")
    print(f"Min: {forecasts['ARIMA_forecast'].min():.4f}")
    print(f"Max: {forecasts['ARIMA_forecast'].max():.4f}")

    plt.show()

if os.path.exists('arima_forecasts.csv'):
    print("Plotting forecasts...")
    plot_forecasts_only('arima_forecasts.csv')
else:
    print("No forecast file found. Please run the ARIMA model first.")

# Cell 17 - Plot November actuals and December forecasts
def plot_nov_dec_comparison(df, forecast_csv):
    """
    Plot November 2016 actual values and December 2016 forecasts
    """
    # Read the forecasts
    forecasts = pd.read_csv(forecast_csv)
    forecasts['datetime'] = pd.to_datetime(forecasts['datetime'])

    # Get November data (last month before forecasts)
    november_start = '2016-11-01'
    november_end = '2016-11-30 23:00:00'
    november_data = df.loc[november_start:november_end]

    # Create plot
    plt.figure(figsize=(15, 8))

    # Plot November actual values
    plt.plot(november_data.index, november_data['X'],
             label='November Actual Values', color='blue', linewidth=2)

    # Plot December forecasts
    plt.plot(forecasts['datetime'], forecasts['ARIMA_forecast'],
             label='December Forecasts', color='red', linewidth=2)

    # Add confidence intervals for forecasts
    plt.fill_between(forecasts['datetime'],
                    forecasts['ARIMA_lower'],
                    forecasts['ARIMA_upper'],
                    color='red', alpha=0.2,
                    label='95% Confidence Interval')

    plt.title('November 2016 Actual Values vs December 2016 Forecasts')
    plt.ylabel('Value')
    plt.xlabel('Date')
    plt.legend()
    plt.grid(True)

    # Print statistics for comparison
    print("\nNovember Statistics:")
    print(f"Mean: {november_data['X'].mean():.4f}")
    print(f"Std Dev: {november_data['X'].std():.4f}")
    print(f"Min: {november_data['X'].min():.4f}")
    print(f"Max: {november_data['X'].max():.4f}")

    print("\nDecember Forecast Statistics:")
    print(f"Mean: {forecasts['ARIMA_forecast'].mean():.4f}")
    print(f"Std Dev: {forecasts['ARIMA_forecast'].std():.4f}")
    print(f"Min: {forecasts['ARIMA_forecast'].min():.4f}")
    print(f"Max: {forecasts['ARIMA_forecast'].max():.4f}")

    plt.show()

if os.path.exists('arima_forecasts.csv'):
    print("Plotting November comparison...")
    plot_nov_dec_comparison(df, 'arima_forecasts.csv')
else:
    print("No forecast file found. Please run the ARIMA model first.")

# Cell 18 - Plot November actuals, December forecasts, and December real values
def plot_complete_comparison(df, forecast_csv, solution_path):
    """
    Plot November 2016 actual values, December 2016 forecasts, and December 2016 real values
    Colors chosen for colorblind visibility:
    - November actuals: Dark Blue
    - December forecasts: Orange
    - December real values: Green
    - Confidence intervals: Light Orange shading
    """
    # Read the forecasts and solution with debug prints
    forecasts = pd.read_csv(forecast_csv)
    print("\nForecast columns:", forecasts.columns.tolist())
    forecasts['datetime'] = pd.to_datetime(forecasts['datetime'])

    # Read and inspect solution file
    solution = pd.read_csv(solution_path)
    print("\nSolution columns:", solution.columns.tolist())

    # Create datetime index for solution based on December 2016
    solution.index = pd.date_range(start='2016-12-01',
                                 periods=len(solution),
                                 freq='H')

    # Get November data
    november_start = '2016-11-01'
    november_end = '2016-11-30 23:00:00'
    november_data = df.loc[november_start:november_end]

    # Create plot
    plt.figure(figsize=(15, 8))

    # Plot November actual values (Dark Blue)
    plt.plot(november_data.index, november_data['X'],
             label='November Actual Values',
             color='#000080',  # Dark Blue
             linewidth=2)

    # Plot December forecasts (Orange)
    plt.plot(forecasts['datetime'], forecasts['ARIMA_forecast'],
             label='December Forecasts',
             color='#FFA500',  # Orange
             linewidth=2)

    # Plot December real values (Green) - assuming 'X' is the column name in solution
    plt.plot(solution.index, solution['X'],
             label='December Real Values',
             color='#008000',  # Green
             linewidth=2,
             linestyle='--')

    # Add confidence intervals for forecasts (Light Orange shading)
    plt.fill_between(forecasts['datetime'],
                    forecasts['ARIMA_lower'],
                    forecasts['ARIMA_upper'],
                    color='#FFA500',  # Orange
                    alpha=0.2,
                    label='95% Confidence Interval')

    plt.title('November 2016 Actual Values vs December 2016 Forecasts and Real Values')
    plt.ylabel('Value')
    plt.xlabel('Date')
    plt.legend()
    plt.grid(True)

    # Print statistics for comparison
    print("\nNovember Statistics:")
    print(f"Mean: {november_data['X'].mean():.4f}")
    print(f"Std Dev: {november_data['X'].std():.4f}")
    print(f"Min: {november_data['X'].min():.4f}")
    print(f"Max: {november_data['X'].max():.4f}")

    print("\nDecember Forecast Statistics:")
    print(f"Mean: {forecasts['ARIMA_forecast'].mean():.4f}")
    print(f"Std Dev: {forecasts['ARIMA_forecast'].std():.4f}")
    print(f"Min: {forecasts['ARIMA_forecast'].min():.4f}")
    print(f"Max: {forecasts['ARIMA_forecast'].max():.4f}")

    print("\nDecember Real Values Statistics:")
    print(f"Mean: {solution['X'].mean():.4f}")
    print(f"Std Dev: {solution['X'].std():.4f}")
    print(f"Min: {solution['X'].min():.4f}")
    print(f"Max: {solution['X'].max():.4f}")

    # Calculate and print error metrics between forecasts and real values
    print("\nForecast Error Metrics:")
    mae = mean_absolute_error(solution['X'], forecasts['ARIMA_forecast'])
    rmse = np.sqrt(mean_squared_error(solution['X'], forecasts['ARIMA_forecast']))
    mape = np.mean(np.abs((solution['X'] - forecasts['ARIMA_forecast']) / solution['X'])) * 100
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAPE: {mape:.4f}%")

    plt.show()

# Execute the plot
solution_path = os.path.join('solution', 't2_solution.csv')
if os.path.exists('arima_forecasts.csv') and os.path.exists(solution_path):
    print("Plotting complete comparison...")
    plot_complete_comparison(df, 'arima_forecasts.csv', solution_path)
else:
    print("Missing required files. Please check both forecast and solution files exist.")

"""## ARIMA Advanced - SARIMAX with Christmas"""

# Cell 19 - More Efficient Seasonal Features
def create_efficient_features(dates):
    """
    Create efficient features for both daily and weekly patterns
    """
    features = pd.DataFrame(index=dates)

    # Hour of day (0-23) as cyclical features
    hours = 24
    features['hour_sin'] = np.sin(2 * np.pi * dates.hour / hours)
    features['hour_cos'] = np.cos(2 * np.pi * dates.hour / hours)

    # Day of week (0-6) as cyclical features
    days = 7
    features['day_sin'] = np.sin(2 * np.pi * dates.dayofweek / days)
    features['day_cos'] = np.cos(2 * np.pi * dates.dayofweek / days)

    # Christmas evening indicator
    christmas_evenings = ((dates.day.isin([25, 26])) &
                         (dates.hour.isin(range(20, 24))) |
                         (dates.day.isin([26, 27])) &
                         (dates.hour.isin(range(0, 6))))
    features['christmas_evening'] = christmas_evenings.astype(int)

    return features

# Cell 20 - Efficient ARIMA Model
def develop_efficient_seasonal_arima(train_data):
    """
    Develop ARIMA model with daily seasonality and external regressors for weekly patterns
    """
    print("=== Efficient Seasonal ARIMA Model Development ===")

    # Prepare the target variable
    y = train_data['X'].astype(float)
    y.index = pd.date_range(start=y.index[0], periods=len(y), freq='H')

    # Create efficient features
    exog_train = create_efficient_features(y.index)
    print(f"\nIncluded features: {exog_train.columns.tolist()}")

    try:
        # Fit model with daily seasonality and external regressors
        model = ARIMA(y,
                     order=(2, 1, 2),        # Regular ARIMA components
                     seasonal_order=(1, 1, 1, 24),  # Daily seasonality
                     exog=exog_train,        # Weekly patterns through cyclical features
                     enforce_stationarity=False,
                     enforce_invertibility=True)

        results = model.fit()
        print("\nModel Summary:")
        print(results.summary())

        return results

    except Exception as e:
        print(f"Error fitting model:")
        print(f"Error message: {str(e)}")
        raise

# Cell 21 - Efficient Forecast Generation
def generate_efficient_forecasts(model, train_data, forecast_horizon=744):
    """
    Generate forecasts efficiently
    """
    # Create forecast dates
    forecast_start = train_data.index[-1] + pd.Timedelta(hours=1)
    forecast_dates = pd.date_range(forecast_start, periods=forecast_horizon, freq='H')

    # Create features for forecast period
    exog_forecast = create_efficient_features(forecast_dates)

    # Generate forecasts
    forecasts = model.forecast(steps=forecast_horizon, exog=exog_forecast)
    forecasts = np.maximum(forecasts, 0)  # Ensure non-negative

    # Get confidence intervals
    conf_int = model.get_forecast(steps=forecast_horizon, exog=exog_forecast).conf_int()
    conf_int.iloc[:, 0] = np.maximum(conf_int.iloc[:, 0], 0)  # Ensure non-negative lower bound

    return forecasts, conf_int, forecast_dates

# Cell 22 - Run Efficient Pipeline
print("Starting Efficient ARIMA modeling pipeline...")

try:
    # Train the model
    arima_model = develop_efficient_seasonal_arima(train_set)

    # Generate forecasts
    forecast_mean, forecast_conf, forecast_dates = generate_efficient_forecasts(arima_model, train_set)

    # Create and save forecast DataFrame
    forecast_df = pd.DataFrame({
        'datetime': forecast_dates,
        'ARIMA_forecast': forecast_mean,
        'ARIMA_lower': forecast_conf.iloc[:, 0],
        'ARIMA_upper': forecast_conf.iloc[:, 1]
    })
    forecast_df.to_csv('arima_forecasts_efficient.csv', index=False)

    print("\nEfficient ARIMA model completed!")
    print("Forecasts saved to 'arima_forecasts_efficient.csv'")

except Exception as e:
    print(f"Error in pipeline: {str(e)}")
    raise

# Cell 23 - Plot Efficient Model Results
def plot_complete_comparison_efficient(df, forecast_csv, solution_path):
    """
    Plot November actuals, December efficient forecasts, and December real values
    Colors chosen for colorblind visibility:
    - November actuals: Dark Blue
    - December forecasts: Orange
    - December real values: Green
    - Confidence intervals: Light Orange shading
    """
    # Read data files
    forecasts = pd.read_csv(forecast_csv)
    forecasts['datetime'] = pd.to_datetime(forecasts['datetime'])

    solution = pd.read_csv(solution_path)
    solution.index = pd.date_range(start='2016-12-01',
                                 periods=len(solution),
                                 freq='H')

    # Get November data
    november_start = '2016-11-01'
    november_end = '2016-11-30 23:00:00'
    november_data = df.loc[november_start:november_end]

    # Create plot with proper size and dpi for better readability
    plt.figure(figsize=(15, 8), dpi=100)

    # Plot November actual values (Dark Blue)
    plt.plot(november_data.index, november_data['X'],
             label='November Actual Values',
             color='#000080',  # Dark Blue
             linewidth=2)

    # Plot December forecasts (Orange) - making sure it's in December
    plt.plot(pd.date_range(start='2016-12-01', periods=len(forecasts), freq='H'),
             forecasts['ARIMA_forecast'],
             label='December Forecasts (Efficient ARIMA)',
             color='#FFA500',  # Orange
             linewidth=2)

    # Plot December real values (Green)
    plt.plot(solution.index, solution['X'],
             label='December Real Values',
             color='#008000',  # Green
             linewidth=2,
             linestyle='--')

    # Add confidence intervals - making sure they're in December
    plt.fill_between(pd.date_range(start='2016-12-01', periods=len(forecasts), freq='H'),
                    forecasts['ARIMA_lower'],
                    forecasts['ARIMA_upper'],
                    color='#FFA500',  # Orange
                    alpha=0.2,
                    label='95% Confidence Interval')

    plt.title('November vs December 2016: Actual, Forecasts, and Real Values')
    plt.ylabel('Traffic Congestion')
    plt.xlabel('Date')
    plt.legend()
    plt.grid(True)

    # Adjust x-axis to show the entire period clearly
    plt.xlim(november_data.index[0], solution.index[-1])

    # Print comparison statistics
    print("\nForecast Error Metrics:")
    mae = mean_absolute_error(solution['X'], forecasts['ARIMA_forecast'])
    rmse = np.sqrt(mean_squared_error(solution['X'], forecasts['ARIMA_forecast']))
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")

    plt.show()

# Execute the plot
solution_path = os.path.join('solution', 't2_solution.csv')
if os.path.exists('arima_forecasts_efficient.csv') and os.path.exists(solution_path):
    print("Plotting complete comparison...")
    plot_complete_comparison_efficient(df, 'arima_forecasts_efficient.csv', solution_path)
else:
    print("Missing required files. Please check both forecast and solution files exist.")

# Cell 24 - Create separate plots for forecasts and real values
def plot_separate_comparisons(df, forecast_csv, solution_path):
    """
    Create two separate plots:
    1. November actuals + December forecasts
    2. November actuals + December real values

    Colors chosen for colorblind visibility:
    - November actuals: Dark Blue
    - December forecasts: Orange
    - December real values: Green
    - Confidence intervals: Light Orange shading
    """
    # Read data files
    forecasts = pd.read_csv(forecast_csv)
    forecasts['datetime'] = pd.to_datetime(forecasts['datetime'])

    solution = pd.read_csv(solution_path)
    solution.index = pd.date_range(start='2016-12-01',
                                 periods=len(solution),
                                 freq='H')

    # Get November data
    november_start = '2016-11-01'
    november_end = '2016-11-30 23:00:00'
    november_data = df.loc[november_start:november_end]

    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), dpi=100)

    # Plot 1: November + December Forecasts
    ax1.plot(november_data.index, november_data['X'],
            label='November Actual Values',
            color='#000080',  # Dark Blue
            linewidth=2)

    december_dates = pd.date_range(start='2016-12-01', periods=len(forecasts), freq='H')
    ax1.plot(december_dates, forecasts['ARIMA_forecast'],
            label='December Forecasts',
            color='#FFA500',  # Orange
            linewidth=2)

    ax1.fill_between(december_dates,
                    forecasts['ARIMA_lower'],
                    forecasts['ARIMA_upper'],
                    color='#FFA500',  # Orange
                    alpha=0.2,
                    label='95% Confidence Interval')

    ax1.set_title('November 2016 Actual Values + December 2016 Forecasts')
    ax1.set_ylabel('Traffic Congestion')
    ax1.grid(True)
    ax1.legend()

    # Plot 2: November + December Real Values
    ax2.plot(november_data.index, november_data['X'],
            label='November Actual Values',
            color='#000080',  # Dark Blue
            linewidth=2)

    ax2.plot(solution.index, solution['X'],
            label='December Real Values',
            color='#008000',  # Green
            linewidth=2)

    ax2.set_title('November 2016 Actual Values + December 2016 Real Values')
    ax2.set_ylabel('Traffic Congestion')
    ax2.set_xlabel('Date')
    ax2.grid(True)
    ax2.legend()

    # Adjust layout and display
    plt.tight_layout()

    # Print comparison statistics
    print("\nForecast Error Metrics:")
    mae = mean_absolute_error(solution['X'], forecasts['ARIMA_forecast'])
    rmse = np.sqrt(mean_squared_error(solution['X'], forecasts['ARIMA_forecast']))
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")

    plt.show()

# Execute the plot
solution_path = os.path.join('solution', 't2_solution.csv')
if os.path.exists('arima_forecasts_efficient.csv') and os.path.exists(solution_path):
    print("Plotting separate comparisons...")
    plot_separate_comparisons(df, 'arima_forecasts_efficient.csv', solution_path)
else:
    print("Missing required files. Please check both forecast and solution files exist.")